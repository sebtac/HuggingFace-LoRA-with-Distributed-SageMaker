{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning LLMs with LoRA and QLoRA in SM DDP and MDP Modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ST NOTES:\n",
    "\n",
    "\"\"\"\n",
    "MAIN OBJECTIVE: \n",
    "Implement and test efficiency of LoRA and QLoRA for fine-tuning of LLM models in DDP and MDP distributed modes in SageMaker\n",
    "           \n",
    "SECONDARY OBJECTIVE(s): \n",
    "- Test relative performance and applicability of p3 and g5 EC2 instances to LLM fine-tuning\n",
    "- Test the impact of LEARNING RATE on training performance in distributed modes\n",
    "- Test the ability to increase the Batch Size with LoRA and QLoRA in distributed modes\n",
    "\n",
    "\n",
    "MOTIVATION:\n",
    "The success of LLM models is rooted in HUGE sizes of the models trained on HUGE number of examples on MANY GPUs over LONG period of times resulting in HUGE TRAINIG COST\n",
    "Thus, LLMs are beyond reach to individual businesses due to technical and cost restrictions.\n",
    "Recently, research community came up with (x)LoRA approaches to train LLM with reduced hardware requirements.\n",
    "Today, a 65B parameter model can be trained on a single GPU with those techniques.\n",
    "While we welcome such development, we see a huge potential in combining (x)LoRA training with Data Parallelism for even faster training. \n",
    "In such scenario, LoRA is the enabler which makes feasible an industrial application of LLMs (reasonable hardware requirements and budget sizes) \n",
    "and Data Parallelism provides training speedups that make fine-tuning of LLMs feasible for wide use by industry (reasonable time scales)\n",
    "Moreover, we explore feasibility of combining (x)LoRA approaches with Model Parallelism to enable efficient training of bigger than 65B models - which we expect to be of huge interest to the industry in the coming years.\n",
    "\n",
    "\n",
    "KEY FINDINGs:\n",
    "- Both LoRA and QLoRA can be implemented in DDP distributed mode in SageMaker with expected benefits in:\n",
    "    - resource requirements\n",
    "    - training speed\n",
    "- QLoRA cannot be implemented in MDP distributed mode. Training freezes for unknown reason as no error message is generated and deeper troubleshooting is needed;\n",
    "- In Single GPU Mode, P3 Instances offer better performance (for smaller models) despite lower GPU memory per instance (P3:16GB vs. G5:24GB)\n",
    "- G5 instances are not supported in Distributed SageMaker modes\n",
    "\n",
    "\n",
    "FINDINGs for SINGLE GPU:\n",
    "\n",
    "- Except for (Q)LoRA G5 instances offer better performance than P3 instances in terms of training speed (93%) and comparable fit measures\n",
    "- The effect of LoRA and QLoRA implementations differs between the EC2 instance types\n",
    "    - For P3 instances:\n",
    "        - LoRA offers higher (80%) training speed than NO-LoRA training with small reduction (97%) in the fit measures (after 1 epoch!)\n",
    "        - QLoRA offers even higher (67%) training speed than NO-LoRA training but it also results in further fit measure reduction (95%) (after 1 epoch!)\n",
    "        - The above results meet expectations as:\n",
    "            - It is expected that LoRA offers faster training due to the reduction in the number of trainable parameters\n",
    "            - It is expected that QLoRA offers even faster training due to additional reduction in the memory footprint of the parameters\n",
    "            - It is expected that LoRA offers lower fit measure per given amount of training as information is encoded into fewer parameters\n",
    "            - It is expected that QLoRA offers even lower fit measure as 4-bit parameter encoding offers lower information capacity than do higher bitrates\n",
    "        - But the LoRA and QLoRA papers indicate that both methods should provide fit measures comparable to those offered by full training:\n",
    "            - We assume that training over more epochs would provide comparable model performance to the fully trained models\n",
    "            - Possibly such training might require more epochs than full training but given speed up efficiencies the overall cost might be still beneficial      \n",
    "    - For G5 Instances:\n",
    "        - The effects do not materialize so clearly and some are even reversed for QLoRA!\n",
    "        - Training Speed:\n",
    "            - LoRA offers higher (90%) training speed but QLoRA offers lower (102%) training speed (it is 113% of the LoRA training speed!)\n",
    "                - We suspect that it is due to presence of hardware optimizations for P3 instances that allows efficient handling of parameters represented in 4-bits on NVDIA GPUs.\n",
    "                - The issue might be related to the lack of proper libraries that handle communications with GPU or with the design differences between the GPU types - TBC...\n",
    "                - Alternatively, SMDDP API might have some G5 specific parameters that are not utilized in the code; TBC... \n",
    "        - Training Fit Measures:\n",
    "            - G5 Instances offer comparable performance to that of the P3 instances\n",
    "\n",
    "\n",
    "FINDINGs for 8xGPU Data Parallel Mode:\n",
    "\n",
    "- Only P3 instances available due to the SageMaker Environment limitations\n",
    "- Training Speed:\n",
    "    - Almost Linear training time speed up for all three (x)LoRA scenarios over Single GPU instances!!!\n",
    "- Training Fit Measures:\n",
    "    - Comparable to those achieved by NO-LoRA on a single GPU!!!\n",
    "        - But LoRA and QLoRA needed bump in the LR equal to Num-of-GPSs multiplied by:\n",
    "            - LoRA: 8\n",
    "            - QLoRA: 16\n",
    "        - Further exploration of those effects is needed but still it indicates that (x)LoRA approaches can be successfully used in the distributed settings.\n",
    "- Impact of Batch Size:\n",
    "    - NO LoRA works with Batch Size of only 32\n",
    "        - this is odd as it is the same batch size as the one used in the Single GPU mode\n",
    "        - the parameter that controls the batch size is \"per_device_train_batch_size\". \n",
    "            - Despite the name, the on-line documentation states that it is global batch size. \n",
    "            - Thus, it is expected that the distributed batch size should be Num_GPUs * SingleGPU Batch Size... TBC...\n",
    "    - LoRA -- not tested\n",
    "    - QLoRA works with Batch Size up to 128\n",
    "        - It constitutes 4-fold increase over NO-LoRA mode!\n",
    "        - Fit is lower than desired but this should be addressable with fine-tuning of LR and more training epochs.\n",
    "\n",
    "\n",
    "FINDINGs for 8xGPU Model Parallel Mode:\n",
    "\n",
    "- Only P3 instances available due to the SageMaker Environment limitations\n",
    "- QLoRA training failed!!!\n",
    "    - Training \"freezes\" for unknown reason\n",
    "        - some additional tests were performed without indication of the cause. TBC...\n",
    "    - We expect that combination of LoRA and MDP training will enable training of big-enough models for multiple applications before the need for QLoRA appears...\n",
    "- Training Speed:\n",
    "    - With given model size, MDP is an OVERKILL... thus we cannot properly test it.\n",
    "    - But we notice that LoRA offers speed ups (90%) over No-LoRA training similar to that on Single GPU\n",
    "    - The main consideration becomes number of partitions and thus the level of data parallelism.\n",
    "        - Reducing partitions from 4 to 2 results in almost 4-fold speed-up! which is attributable to\n",
    "            - Significant reduction in inter-GPU communications.\n",
    "            - 2-fold increase in data parallelism\n",
    "- Training Fit Measures:\n",
    "    - We can achieve fit measures comparable to those achieved by SINGLE GPU training\n",
    "    - But it is important to control the LEARNING RATE which should be relative to the Data Parallelism level\n",
    "- Impact of Batch Size:\n",
    "    - Both NO LoRA and LoRA works with Batch Size of 32 and 64 with 2 Partitions and they break with Batch Size \n",
    "        - For NO LoRA, it makes sense as partition 2 means that each batch is spread across 2 GPUs and we have can work with 4 unique batches in parallel\n",
    "        - For LoRA, this is odd as the test of DDP shows that it is possible for it to work with Batch size of 128. One would expect similar performance in this scenario as well\n",
    "\n",
    "CAVIATS:\n",
    "- testing training speed with only one epoch provides not the best performance estimate:\n",
    "    - This is so as bulk of the time is spent on initialization of training and evaluation. \n",
    "    - It would be best to test the time of training individual epoch and preferably not the first one and even a series of epochs.\n",
    "    - Still the results above are valid directionally...  \n",
    "- Analyzing of \"freezing\" of QLoRA in MDP requires deeper review beyond standard error messages generated by python. We will come back to it.\n",
    "- Subsequent tests will be performed on larger models and more epochs to get better estimates of the effects of interest.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mName: datasets\n",
      "Version: 2.10.1\n",
      "Summary: HuggingFace community-driven open-source library of datasets\n",
      "Home-page: https://github.com/huggingface/datasets\n",
      "Author: HuggingFace Inc.\n",
      "Author-email: thomas@huggingface.co\n",
      "License: Apache 2.0\n",
      "Location: /opt/conda/lib/python3.7/site-packages\n",
      "Requires: aiohttp, dill, fsspec, huggingface-hub, importlib-metadata, multiprocess, numpy, packaging, pandas, pyarrow, pyyaml, requests, responses, tqdm, xxhash\n",
      "Required-by: \n",
      "3.7.10 (default, Jun  4 2021, 14:48:32) \n",
      "[GCC 7.5.0]\n",
      "Name: datasets\n",
      "Version: 2.10.1\n",
      "Summary: HuggingFace community-driven open-source library of datasets\n",
      "Home-page: https://github.com/huggingface/datasets\n",
      "Author: HuggingFace Inc.\n",
      "Author-email: thomas@huggingface.co\n",
      "License: Apache 2.0\n",
      "Location: /opt/conda/lib/python3.7/site-packages\n",
      "Requires: aiohttp, dill, fsspec, huggingface-hub, importlib-metadata, multiprocess, numpy, packaging, pandas, pyarrow, pyyaml, requests, responses, tqdm, xxhash\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "### REQUIRES INSTANCE WITH AT LEAST 8GB OF MEMORY\n",
    "### You might need to restart the notebook!!!\n",
    "!pip -q install \"sagemaker>=2.140.0\" \"transformers==4.26.1\" \"datasets[s3]==2.10.1\" \"torch\" --upgrade\n",
    "!pip -q install accelerate\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments, AutoTokenizer, AutoModelForCausalLM #, BitsAndBytesConfig\n",
    "\n",
    "#import sys\n",
    "#print(sys.version)\n",
    "#!pip show datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::578864530451:role/service-role/AmazonSageMaker-ExecutionRole-20210306T201609\n",
      "sagemaker bucket: sagemaker-us-east-1-578864530451\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "We are using the `datasets` library to download and preprocess the `imdb` dataset. After preprocessing, the dataset will be uploaded to our `sagemaker_session_bucket` to be used within our training job. The [imdb](http://ai.stanford.edu/~amaas/data/sentiment/) dataset consists of 25000 training and 25000 testing highly polar movie reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a75320403ce4aa3b0b432eadb5d4c3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92004c58797f46649d0fccaa0c920ccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-d16bb3b5a60af224.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['text', 'label'],\n",
       "         num_rows: 25000\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['text', 'label'],\n",
       "         num_rows: 25000\n",
       "     })\n",
       "     unsupervised: Dataset({\n",
       "         features: ['text', 'label'],\n",
       "         num_rows: 50000\n",
       "     })\n",
       " }),)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DATASET USED\n",
    "dataset_name = 'imdb'\n",
    "\n",
    "if dataset_name == 'imdb':\n",
    "    # tokenizer used in preprocessing\n",
    "    tokenizer_name = 'distilbert-base-uncased'\n",
    "\n",
    "    # s3 key prefix for the data\n",
    "    s3_prefix = 'samples/datasets/imdb'\n",
    "\n",
    "    # load dataset\n",
    "    dataset = load_dataset(dataset_name)\n",
    "\n",
    "    # download tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "    # tokenizer helper function\n",
    "    def tokenize(batch):\n",
    "        return tokenizer(batch['text'], padding='max_length', truncation=True)\n",
    "\n",
    "    # load dataset\n",
    "    train_dataset, test_dataset = load_dataset('imdb', split=['train', 'test'])\n",
    "    test_dataset = test_dataset.shuffle().select(range(10000)) # smaller the size for test dataset to 10k \n",
    "\n",
    "    # tokenize dataset\n",
    "    train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "    test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "    # set format for pytorch\n",
    "    train_dataset =  train_dataset.rename_column(\"label\", \"labels\")\n",
    "    train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    test_dataset = test_dataset.rename_column(\"label\", \"labels\")\n",
    "    test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "    #print(\"test_dataset[0]\\n\\n\", test_dataset[0])\n",
    "\n",
    "    # save train_dataset to s3\n",
    "    training_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/train'\n",
    "    train_dataset.save_to_disk(training_input_path)\n",
    "\n",
    "    # save test_dataset to s3\n",
    "    test_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/test'\n",
    "    test_dataset.save_to_disk(test_input_path)\n",
    "\n",
    "dataset, #dataset[\"train\"]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SINGLE GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-pytorch-training-2023-09-22-12-36-20-432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-22 12:37:01 Starting - Starting the training job...\n",
      "2023-09-22 12:37:17 Starting - Preparing the instances for training......\n",
      "2023-09-22 12:38:27 Downloading - Downloading input data...\n",
      "2023-09-22 12:38:53 Training - Downloading the training image...............\n",
      "2023-09-22 12:41:24 Training - Training image download completed. Training in progress......\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-09-22 12:42:10,410 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-09-22 12:42:10,427 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-22 12:42:10,437 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-09-22 12:42:10,439 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-09-22 12:42:15,941 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-22 12:42:15,971 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-22 12:42:16,001 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-22 12:42:16,012 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.4xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 1,\n",
      "        \"eval_batch_size\": 256,\n",
      "        \"learning_rate\": 5e-05,\n",
      "        \"model_name\": \"distilbert-base-uncased\",\n",
      "        \"train_batch_size\": 32\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.4xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2023-09-22-12-36-20-432\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-578864530451/huggingface-pytorch-training-2023-09-22-12-36-20-432/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train-QLORA\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.4xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.4xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train-QLORA.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":1,\"eval_batch_size\":256,\"learning_rate\":5e-05,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":32}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train-QLORA.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.4xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.4xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train-QLORA\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=16\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-578864530451/huggingface-pytorch-training-2023-09-22-12-36-20-432/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.4xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":1,\"eval_batch_size\":256,\"learning_rate\":5e-05,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":32},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-pytorch-training-2023-09-22-12-36-20-432\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-578864530451/huggingface-pytorch-training-2023-09-22-12-36-20-432/source/sourcedir.tar.gz\",\"module_name\":\"train-QLORA\",\"network_interface_name\":\"eth0\",\"num_cpus\":16,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.4xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train-QLORA.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"1\",\"--eval_batch_size\",\"256\",\"--learning_rate\",\"5e-05\",\"--model_name\",\"distilbert-base-uncased\",\"--train_batch_size\",\"32\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_BATCH_SIZE=256\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=5e-05\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=distilbert-base-uncased\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=32\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 train-QLORA.py --epochs 1 --eval_batch_size 256 --learning_rate 5e-05 --model_name distilbert-base-uncased --train_batch_size 32\u001b[0m\n",
      "\u001b[34m[2023-09-22 12:42:17.538: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m2023-09-22 12:42:17,541 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m2023-09-22 12:42:17,562 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34msh: 1: cannot open 0.14.0: No such file\u001b[0m\n",
      "\u001b[34m2023-09-22 12:43:00,622 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m2023-09-22 12:43:00,622 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34mcompute_dtype torch.float16\u001b[0m\n",
      "\u001b[34mRANK None\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json: 100%|██████████| 483/483 [00:00<00:00, 169kB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:   8%|▊         | 21.0M/268M [00:00<00:01, 157MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  23%|██▎       | 62.9M/268M [00:00<00:00, 266MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  47%|████▋     | 126M/268M [00:00<00:00, 391MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  70%|███████   | 189M/268M [00:00<00:00, 466MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  94%|█████████▍| 252M/268M [00:00<00:00, 510MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors: 100%|██████████| 268M/268M [00:00<00:00, 442MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'classifier.bias', 'pre_classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'classifier.bias', 'pre_classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mAFTER prepare_model_for_kbit_training trainable params: 0 || all params: 45721346 || trainable%: 0.00\u001b[0m\n",
      "\u001b[34mname base_model.model.distilbert.transformer.layer.0.attention.q_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34mname base_model.model.distilbert.transformer.layer.0.attention.q_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34mname base_model.model.distilbert.transformer.layer.0.attention.v_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34mname base_model.model.distilbert.transformer.layer.0.attention.v_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34mname base_model.model.distilbert.transformer.layer.1.attention.q_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34mname base_model.model.distilbert.transformer.layer.1.attention.q_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34mname base_model.model.distilbert.transformer.layer.1.attention.v_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34mname base_model.model.distilbert.transformer.layer.1.attention.v_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34mname base_model.model.distilbert.transformer.layer.2.attention.q_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34mname base_model.model.distilbert.transformer.layer.2.attention.q_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34mname base_model.model.distilbert.transformer.layer.2.attention.v_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34mname base_model.model.distilbert.transformer.layer.2.attention.v_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34mname base_model.model.distilbert.transformer.layer.3.attention.q_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34mname base_model.model.distilbert.transformer.layer.3.attention.q_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34mname base_model.model.distilbert.transformer.layer.3.attention.v_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34mname base_model.model.distilbert.transformer.layer.3.attention.v_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34mname base_model.model.distilbert.transformer.layer.4.attention.q_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34mname base_model.model.distilbert.transformer.layer.4.attention.q_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34mname base_model.model.distilbert.transformer.layer.4.attention.v_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34mname base_model.model.distilbert.transformer.layer.4.attention.v_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34mname base_model.model.distilbert.transformer.layer.5.attention.q_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34mname base_model.model.distilbert.transformer.layer.5.attention.q_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34mname base_model.model.distilbert.transformer.layer.5.attention.v_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34mname base_model.model.distilbert.transformer.layer.5.attention.v_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34mQLoRA Model trainable params: 589824 || all params: 46903300 || trainable%: 1.26\u001b[0m\n",
      "\u001b[34mDownloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 9.45kB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 49.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 82.0MB/s]\u001b[0m\n",
      "\u001b[34mLoading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\u001b[0m\n",
      "\u001b[34mLoading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\u001b[0m\n",
      "\u001b[34mSTART TIME: 1695386582.6245904\u001b[0m\n",
      "\u001b[34m0%|          | 0/782 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2023-09-22 12:43:02.797: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.34.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m2023-09-22 12:43:02,801 - root - INFO - Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[2023-09-22 12:43:02.825 algo-1:67 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-09-22 12:43:02.851 algo-1:67 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-09-22 12:43:02.852 algo-1:67 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-09-22 12:43:02.852 algo-1:67 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-09-22 12:43:02.853 algo-1:67 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-09-22 12:43:02.853 algo-1:67 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34mYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m0%|          | 1/782 [00:01<19:13,  1.48s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 2/782 [00:01<11:22,  1.14it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 3/782 [00:02<08:51,  1.47it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 4/782 [00:02<07:40,  1.69it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 5/782 [00:03<07:00,  1.85it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 6/782 [00:03<06:36,  1.96it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 7/782 [00:04<06:20,  2.04it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 8/782 [00:04<06:10,  2.09it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 9/782 [00:05<06:03,  2.13it/s]\u001b[0m\n",
      "\u001b[34m1%|▏         | 10/782 [00:05<05:58,  2.16it/s]\u001b[0m\n",
      "\u001b[34m1%|▏         | 11/782 [00:05<05:54,  2.17it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 12/782 [00:06<05:51,  2.19it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 13/782 [00:06<05:50,  2.20it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 14/782 [00:07<05:48,  2.20it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 15/782 [00:07<05:47,  2.21it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 16/782 [00:08<05:46,  2.21it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 17/782 [00:08<05:45,  2.21it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 18/782 [00:09<05:45,  2.21it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 19/782 [00:09<05:44,  2.21it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 20/782 [00:10<05:43,  2.22it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 21/782 [00:10<05:43,  2.22it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 22/782 [00:10<05:42,  2.22it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 23/782 [00:11<05:42,  2.22it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 24/782 [00:11<05:41,  2.22it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 25/782 [00:12<05:41,  2.22it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 26/782 [00:12<05:40,  2.22it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 27/782 [00:13<05:40,  2.22it/s]\u001b[0m\n",
      "\u001b[34m4%|▎         | 28/782 [00:13<05:39,  2.22it/s]\u001b[0m\n",
      "\u001b[34m4%|▎         | 29/782 [00:14<05:39,  2.22it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 30/782 [00:14<05:38,  2.22it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 31/782 [00:15<05:38,  2.22it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 32/782 [00:15<05:37,  2.22it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 33/782 [00:15<05:37,  2.22it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 34/782 [00:16<05:37,  2.22it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 35/782 [00:16<05:36,  2.22it/s]\u001b[0m\n",
      "\u001b[34m5%|▍         | 36/782 [00:17<05:36,  2.22it/s]\u001b[0m\n",
      "\u001b[34m5%|▍         | 37/782 [00:17<05:35,  2.22it/s]\u001b[0m\n",
      "\u001b[34m5%|▍         | 38/782 [00:18<05:35,  2.22it/s]\u001b[0m\n",
      "\u001b[34m5%|▍         | 39/782 [00:18<05:34,  2.22it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 40/782 [00:19<05:34,  2.22it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 41/782 [00:19<05:33,  2.22it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 42/782 [00:19<05:33,  2.22it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 43/782 [00:20<05:32,  2.22it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 44/782 [00:20<05:32,  2.22it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 45/782 [00:21<05:31,  2.22it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 46/782 [00:21<05:31,  2.22it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 47/782 [00:22<05:31,  2.22it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 48/782 [00:22<05:30,  2.22it/s]\u001b[0m\n",
      "\u001b[34m6%|▋         | 49/782 [00:23<05:30,  2.22it/s]\u001b[0m\n",
      "\u001b[34m6%|▋         | 50/782 [00:23<05:29,  2.22it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 51/782 [00:24<05:29,  2.22it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 52/782 [00:24<05:28,  2.22it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 53/782 [00:24<05:28,  2.22it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 54/782 [00:25<05:28,  2.22it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 55/782 [00:25<05:27,  2.22it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 56/782 [00:26<05:27,  2.22it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 57/782 [00:26<05:26,  2.22it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 58/782 [00:27<05:26,  2.22it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 59/782 [00:27<05:25,  2.22it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 60/782 [00:28<05:25,  2.22it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 61/782 [00:28<05:24,  2.22it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 62/782 [00:28<05:24,  2.22it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 63/782 [00:29<05:24,  2.22it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 64/782 [00:29<05:23,  2.22it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 65/782 [00:30<05:23,  2.22it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 66/782 [00:30<05:22,  2.22it/s]\u001b[0m\n",
      "\u001b[34m9%|▊         | 67/782 [00:31<05:22,  2.22it/s]\u001b[0m\n",
      "\u001b[34m9%|▊         | 68/782 [00:31<05:21,  2.22it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 69/782 [00:32<05:21,  2.22it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 70/782 [00:32<05:20,  2.22it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 71/782 [00:33<05:20,  2.22it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 72/782 [00:33<05:20,  2.22it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 73/782 [00:33<05:19,  2.22it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 74/782 [00:34<05:18,  2.22it/s]\u001b[0m\n",
      "\u001b[34m10%|▉         | 75/782 [00:34<05:18,  2.22it/s]\u001b[0m\n",
      "\u001b[34m10%|▉         | 76/782 [00:35<05:18,  2.22it/s]\u001b[0m\n",
      "\u001b[34m10%|▉         | 77/782 [00:35<05:17,  2.22it/s]\u001b[0m\n",
      "\u001b[34m10%|▉         | 78/782 [00:36<05:17,  2.22it/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 79/782 [00:36<05:16,  2.22it/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 80/782 [00:37<05:16,  2.22it/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 81/782 [00:37<05:15,  2.22it/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 82/782 [00:37<05:15,  2.22it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 83/782 [00:38<05:15,  2.22it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 84/782 [00:38<05:14,  2.22it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 85/782 [00:39<05:14,  2.22it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 86/782 [00:39<05:13,  2.22it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 87/782 [00:40<05:13,  2.22it/s]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 88/782 [00:40<05:12,  2.22it/s]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 89/782 [00:41<05:12,  2.22it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 90/782 [00:41<05:11,  2.22it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 91/782 [00:42<05:11,  2.22it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 92/782 [00:42<05:11,  2.22it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 93/782 [00:42<05:10,  2.22it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 94/782 [00:43<05:10,  2.22it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 95/782 [00:43<05:09,  2.22it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 96/782 [00:44<05:09,  2.22it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 97/782 [00:44<05:08,  2.22it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 98/782 [00:45<05:08,  2.22it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 99/782 [00:45<05:07,  2.22it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 100/782 [00:46<05:07,  2.22it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 101/782 [00:46<05:06,  2.22it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 102/782 [00:47<05:06,  2.22it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 103/782 [00:47<05:06,  2.22it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 104/782 [00:47<05:05,  2.22it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 105/782 [00:48<05:05,  2.22it/s]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 106/782 [00:48<05:04,  2.22it/s]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 107/782 [00:49<05:04,  2.22it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 108/782 [00:49<05:03,  2.22it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 109/782 [00:50<05:03,  2.22it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 110/782 [00:50<05:02,  2.22it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 111/782 [00:51<05:02,  2.22it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 112/782 [00:51<05:01,  2.22it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 113/782 [00:51<05:01,  2.22it/s]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 114/782 [00:52<05:01,  2.22it/s]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 115/782 [00:52<05:00,  2.22it/s]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 116/782 [00:53<05:00,  2.22it/s]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 117/782 [00:53<04:59,  2.22it/s]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 118/782 [00:54<04:59,  2.22it/s]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 119/782 [00:54<04:58,  2.22it/s]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 120/782 [00:55<04:58,  2.22it/s]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 121/782 [00:55<04:57,  2.22it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 122/782 [00:56<04:57,  2.22it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 123/782 [00:56<04:56,  2.22it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 124/782 [00:56<04:56,  2.22it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 125/782 [00:57<04:56,  2.22it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 126/782 [00:57<04:55,  2.22it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 127/782 [00:58<04:55,  2.22it/s]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 128/782 [00:58<04:54,  2.22it/s]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 129/782 [00:59<04:54,  2.22it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 130/782 [00:59<04:53,  2.22it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 131/782 [01:00<04:53,  2.22it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 132/782 [01:00<04:53,  2.22it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 133/782 [01:00<04:52,  2.22it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 134/782 [01:01<04:52,  2.22it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 135/782 [01:01<04:51,  2.22it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 136/782 [01:02<04:51,  2.22it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 137/782 [01:02<04:50,  2.22it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 138/782 [01:03<04:50,  2.22it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 139/782 [01:03<04:49,  2.22it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 140/782 [01:04<04:49,  2.22it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 141/782 [01:04<04:48,  2.22it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 142/782 [01:05<04:48,  2.22it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 143/782 [01:05<04:48,  2.22it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 144/782 [01:05<04:47,  2.22it/s]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 145/782 [01:06<04:47,  2.22it/s]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 146/782 [01:06<04:46,  2.22it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 147/782 [01:07<04:46,  2.22it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 148/782 [01:07<04:45,  2.22it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 149/782 [01:08<04:45,  2.22it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 150/782 [01:08<04:44,  2.22it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 151/782 [01:09<04:44,  2.22it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 152/782 [01:09<04:43,  2.22it/s]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 153/782 [01:09<04:43,  2.22it/s]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 154/782 [01:10<04:43,  2.22it/s]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 155/782 [01:10<04:42,  2.22it/s]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 156/782 [01:11<04:42,  2.22it/s]\u001b[0m\n",
      "\u001b[34m20%|██        | 157/782 [01:11<04:41,  2.22it/s]\u001b[0m\n",
      "\u001b[34m20%|██        | 158/782 [01:12<04:41,  2.22it/s]\u001b[0m\n",
      "\u001b[34m20%|██        | 159/782 [01:12<04:40,  2.22it/s]\u001b[0m\n",
      "\u001b[34m20%|██        | 160/782 [01:13<04:40,  2.22it/s]\u001b[0m\n",
      "\u001b[34m21%|██        | 161/782 [01:13<04:39,  2.22it/s]\u001b[0m\n",
      "\u001b[34m21%|██        | 162/782 [01:14<04:39,  2.22it/s]\u001b[0m\n",
      "\u001b[34m21%|██        | 163/782 [01:14<04:39,  2.22it/s]\u001b[0m\n",
      "\u001b[34m21%|██        | 164/782 [01:14<04:38,  2.22it/s]\u001b[0m\n",
      "\u001b[34m21%|██        | 165/782 [01:15<04:38,  2.22it/s]\u001b[0m\n",
      "\u001b[34m21%|██        | 166/782 [01:15<04:37,  2.22it/s]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 167/782 [01:16<04:37,  2.22it/s]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 168/782 [01:16<04:36,  2.22it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 169/782 [01:17<04:36,  2.22it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 170/782 [01:17<04:35,  2.22it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 171/782 [01:18<04:35,  2.22it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 172/782 [01:18<04:34,  2.22it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 173/782 [01:19<04:34,  2.22it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 174/782 [01:19<04:33,  2.22it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 175/782 [01:19<04:33,  2.22it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 176/782 [01:20<04:33,  2.22it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 177/782 [01:20<04:32,  2.22it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 178/782 [01:21<04:32,  2.22it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 179/782 [01:21<04:31,  2.22it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 180/782 [01:22<04:31,  2.22it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 181/782 [01:22<04:30,  2.22it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 182/782 [01:23<04:30,  2.22it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 183/782 [01:23<04:29,  2.22it/s]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 184/782 [01:23<04:29,  2.22it/s]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 185/782 [01:24<04:29,  2.22it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 186/782 [01:24<04:28,  2.22it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 187/782 [01:25<04:28,  2.22it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 188/782 [01:25<04:27,  2.22it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 189/782 [01:26<04:27,  2.22it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 190/782 [01:26<04:26,  2.22it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 191/782 [01:27<04:26,  2.22it/s]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 192/782 [01:27<04:25,  2.22it/s]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 193/782 [01:28<04:25,  2.22it/s]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 194/782 [01:28<04:25,  2.22it/s]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 195/782 [01:28<04:24,  2.22it/s]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 196/782 [01:29<04:24,  2.22it/s]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 197/782 [01:29<04:23,  2.22it/s]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 198/782 [01:30<04:23,  2.22it/s]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 199/782 [01:30<04:22,  2.22it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 200/782 [01:31<04:22,  2.22it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 201/782 [01:31<04:21,  2.22it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 202/782 [01:32<04:21,  2.22it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 203/782 [01:32<04:20,  2.22it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 204/782 [01:32<04:20,  2.22it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 205/782 [01:33<04:20,  2.22it/s]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 206/782 [01:33<04:19,  2.22it/s]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 207/782 [01:34<04:19,  2.22it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 208/782 [01:34<04:18,  2.22it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 209/782 [01:35<04:18,  2.22it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 210/782 [01:35<04:17,  2.22it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 211/782 [01:36<04:17,  2.22it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 212/782 [01:36<04:16,  2.22it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 213/782 [01:37<04:16,  2.22it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 214/782 [01:37<04:16,  2.22it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 215/782 [01:37<04:15,  2.22it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 216/782 [01:38<04:15,  2.22it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 217/782 [01:38<04:14,  2.22it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 218/782 [01:39<04:14,  2.22it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 219/782 [01:39<04:13,  2.22it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 220/782 [01:40<04:13,  2.22it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 221/782 [01:40<04:12,  2.22it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 222/782 [01:41<04:12,  2.22it/s]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 223/782 [01:41<04:11,  2.22it/s]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 224/782 [01:41<04:11,  2.22it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 225/782 [01:42<04:11,  2.22it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 226/782 [01:42<04:10,  2.22it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 227/782 [01:43<04:10,  2.22it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 228/782 [01:43<04:09,  2.22it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 229/782 [01:44<04:09,  2.22it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 230/782 [01:44<04:08,  2.22it/s]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 231/782 [01:45<04:08,  2.22it/s]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 232/782 [01:45<04:07,  2.22it/s]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 233/782 [01:46<04:07,  2.22it/s]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 234/782 [01:46<04:06,  2.22it/s]\u001b[0m\n",
      "\u001b[34m30%|███       | 235/782 [01:46<04:06,  2.22it/s]\u001b[0m\n",
      "\u001b[34m30%|███       | 236/782 [01:47<04:06,  2.22it/s]\u001b[0m\n",
      "\u001b[34m30%|███       | 237/782 [01:47<04:05,  2.22it/s]\u001b[0m\n",
      "\u001b[34m30%|███       | 238/782 [01:48<04:05,  2.22it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 239/782 [01:48<04:04,  2.22it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 240/782 [01:49<04:04,  2.22it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 241/782 [01:49<04:03,  2.22it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 242/782 [01:50<04:03,  2.22it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 243/782 [01:50<04:02,  2.22it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 244/782 [01:51<04:02,  2.22it/s]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 245/782 [01:51<04:02,  2.22it/s]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 246/782 [01:51<04:01,  2.22it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 247/782 [01:52<04:01,  2.22it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 248/782 [01:52<04:00,  2.22it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 249/782 [01:53<04:00,  2.22it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 250/782 [01:53<03:59,  2.22it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 251/782 [01:54<03:59,  2.22it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 252/782 [01:54<03:58,  2.22it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 253/782 [01:55<03:58,  2.22it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 254/782 [01:55<03:57,  2.22it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 255/782 [01:55<03:57,  2.22it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 256/782 [01:56<03:57,  2.22it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 257/782 [01:56<03:56,  2.22it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 258/782 [01:57<03:56,  2.22it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 259/782 [01:57<03:55,  2.22it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 260/782 [01:58<03:55,  2.22it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 261/782 [01:58<03:54,  2.22it/s]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 262/782 [01:59<03:54,  2.22it/s]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 263/782 [01:59<03:54,  2.22it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 264/782 [02:00<03:53,  2.22it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 265/782 [02:00<03:53,  2.22it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 266/782 [02:00<03:52,  2.22it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 267/782 [02:01<03:52,  2.22it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 268/782 [02:01<03:51,  2.22it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 269/782 [02:02<03:51,  2.22it/s]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 270/782 [02:02<03:50,  2.22it/s]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 271/782 [02:03<03:50,  2.22it/s]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 272/782 [02:03<03:49,  2.22it/s]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 273/782 [02:04<03:49,  2.22it/s]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 274/782 [02:04<03:49,  2.22it/s]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 275/782 [02:04<03:48,  2.22it/s]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 276/782 [02:05<03:48,  2.22it/s]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 277/782 [02:05<03:47,  2.22it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 278/782 [02:06<03:47,  2.22it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 279/782 [02:06<03:46,  2.22it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 280/782 [02:07<03:46,  2.22it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 281/782 [02:07<03:45,  2.22it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 282/782 [02:08<03:45,  2.22it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 283/782 [02:08<03:44,  2.22it/s]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 284/782 [02:09<03:44,  2.22it/s]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 285/782 [02:09<03:44,  2.22it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 286/782 [02:09<03:43,  2.22it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 287/782 [02:10<03:43,  2.22it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 288/782 [02:10<03:42,  2.22it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 289/782 [02:11<03:42,  2.22it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 290/782 [02:11<03:41,  2.22it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 291/782 [02:12<03:41,  2.22it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 292/782 [02:12<03:40,  2.22it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 293/782 [02:13<03:40,  2.22it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 294/782 [02:13<03:40,  2.22it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 295/782 [02:14<03:39,  2.22it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 296/782 [02:14<03:39,  2.22it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 297/782 [02:14<03:38,  2.22it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 298/782 [02:15<03:38,  2.22it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 299/782 [02:15<03:37,  2.22it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 300/782 [02:16<03:37,  2.22it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 301/782 [02:16<03:36,  2.22it/s]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 302/782 [02:17<03:36,  2.22it/s]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 303/782 [02:17<03:35,  2.22it/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 304/782 [02:18<03:35,  2.22it/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 305/782 [02:18<03:35,  2.22it/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 306/782 [02:18<03:34,  2.22it/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 307/782 [02:19<03:34,  2.22it/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 308/782 [02:19<03:33,  2.22it/s]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 309/782 [02:20<03:33,  2.22it/s]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 310/782 [02:20<03:32,  2.22it/s]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 311/782 [02:21<03:32,  2.22it/s]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 312/782 [02:21<03:31,  2.22it/s]\u001b[0m\n",
      "\u001b[34m40%|████      | 313/782 [02:22<03:31,  2.22it/s]\u001b[0m\n",
      "\u001b[34m40%|████      | 314/782 [02:22<03:30,  2.22it/s]\u001b[0m\n",
      "\u001b[34m40%|████      | 315/782 [02:23<03:30,  2.22it/s]\u001b[0m\n",
      "\u001b[34m40%|████      | 316/782 [02:23<03:30,  2.22it/s]\u001b[0m\n",
      "\u001b[34m41%|████      | 317/782 [02:23<03:29,  2.22it/s]\u001b[0m\n",
      "\u001b[34m41%|████      | 318/782 [02:24<03:29,  2.22it/s]\u001b[0m\n",
      "\u001b[34m41%|████      | 319/782 [02:24<03:28,  2.22it/s]\u001b[0m\n",
      "\u001b[34m41%|████      | 320/782 [02:25<03:28,  2.22it/s]\u001b[0m\n",
      "\u001b[34m41%|████      | 321/782 [02:25<03:27,  2.22it/s]\u001b[0m\n",
      "\u001b[34m41%|████      | 322/782 [02:26<03:27,  2.22it/s]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 323/782 [02:26<03:26,  2.22it/s]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 324/782 [02:27<03:26,  2.22it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 325/782 [02:27<03:26,  2.22it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 326/782 [02:27<03:25,  2.22it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 327/782 [02:28<03:25,  2.22it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 328/782 [02:28<03:24,  2.22it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 329/782 [02:29<03:24,  2.22it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 330/782 [02:29<03:23,  2.22it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 331/782 [02:30<03:23,  2.22it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 332/782 [02:30<03:22,  2.22it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 333/782 [02:31<03:22,  2.22it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 334/782 [02:31<03:22,  2.22it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 335/782 [02:32<03:21,  2.22it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 336/782 [02:32<03:21,  2.22it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 337/782 [02:32<03:20,  2.22it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 338/782 [02:33<03:20,  2.22it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 339/782 [02:33<03:19,  2.22it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 340/782 [02:34<03:19,  2.22it/s]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 341/782 [02:34<03:18,  2.22it/s]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 342/782 [02:35<03:18,  2.22it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 343/782 [02:35<03:17,  2.22it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 344/782 [02:36<03:17,  2.22it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 345/782 [02:36<03:17,  2.22it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 346/782 [02:36<03:16,  2.22it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 347/782 [02:37<03:16,  2.22it/s]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 348/782 [02:37<03:15,  2.22it/s]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 349/782 [02:38<03:15,  2.22it/s]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 350/782 [02:38<03:14,  2.22it/s]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 351/782 [02:39<03:14,  2.22it/s]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 352/782 [02:39<03:13,  2.22it/s]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 353/782 [02:40<03:13,  2.22it/s]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 354/782 [02:40<03:12,  2.22it/s]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 355/782 [02:41<03:12,  2.22it/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 356/782 [02:41<03:12,  2.22it/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 357/782 [02:41<03:11,  2.22it/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 358/782 [02:42<03:11,  2.22it/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 359/782 [02:42<03:10,  2.22it/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 360/782 [02:43<03:10,  2.22it/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 361/782 [02:43<03:09,  2.22it/s]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 362/782 [02:44<03:09,  2.22it/s]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 363/782 [02:44<03:08,  2.22it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 364/782 [02:45<03:08,  2.22it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 365/782 [02:45<03:07,  2.22it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 366/782 [02:46<03:07,  2.22it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 367/782 [02:46<03:07,  2.22it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 368/782 [02:46<03:06,  2.22it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 369/782 [02:47<03:06,  2.22it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 370/782 [02:47<03:05,  2.22it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 371/782 [02:48<03:05,  2.22it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 372/782 [02:48<03:04,  2.22it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 373/782 [02:49<03:04,  2.22it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 374/782 [02:49<03:03,  2.22it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 375/782 [02:50<03:03,  2.22it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 376/782 [02:50<03:02,  2.22it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 377/782 [02:50<03:02,  2.22it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 378/782 [02:51<03:02,  2.22it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 379/782 [02:51<03:01,  2.22it/s]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 380/782 [02:52<03:01,  2.22it/s]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 381/782 [02:52<03:00,  2.22it/s]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 382/782 [02:53<03:00,  2.22it/s]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 383/782 [02:53<02:59,  2.22it/s]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 384/782 [02:54<02:59,  2.22it/s]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 385/782 [02:54<02:58,  2.22it/s]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 386/782 [02:55<02:58,  2.22it/s]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 387/782 [02:55<02:57,  2.22it/s]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 388/782 [02:55<02:57,  2.22it/s]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 389/782 [02:56<02:57,  2.22it/s]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 390/782 [02:56<02:56,  2.22it/s]\u001b[0m\n",
      "\u001b[34m50%|█████     | 391/782 [02:57<02:56,  2.22it/s]\u001b[0m\n",
      "\u001b[34m50%|█████     | 392/782 [02:57<02:55,  2.22it/s]\u001b[0m\n",
      "\u001b[34m50%|█████     | 393/782 [02:58<02:55,  2.22it/s]\u001b[0m\n",
      "\u001b[34m50%|█████     | 394/782 [02:58<02:54,  2.22it/s]\u001b[0m\n",
      "\u001b[34m51%|█████     | 395/782 [02:59<02:54,  2.22it/s]\u001b[0m\n",
      "\u001b[34m51%|█████     | 396/782 [02:59<02:53,  2.22it/s]\u001b[0m\n",
      "\u001b[34m51%|█████     | 397/782 [02:59<02:53,  2.22it/s]\u001b[0m\n",
      "\u001b[34m51%|█████     | 398/782 [03:00<02:53,  2.22it/s]\u001b[0m\n",
      "\u001b[34m51%|█████     | 399/782 [03:00<02:52,  2.22it/s]\u001b[0m\n",
      "\u001b[34m51%|█████     | 400/782 [03:01<02:52,  2.22it/s]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 401/782 [03:01<02:51,  2.22it/s]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 402/782 [03:02<02:51,  2.22it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 403/782 [03:02<02:50,  2.22it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 404/782 [03:03<02:50,  2.21it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 405/782 [03:03<02:50,  2.21it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 406/782 [03:04<02:49,  2.21it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 407/782 [03:04<02:49,  2.21it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 408/782 [03:04<02:49,  2.21it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 409/782 [03:05<02:48,  2.21it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 410/782 [03:05<02:48,  2.21it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 411/782 [03:06<02:47,  2.21it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 412/782 [03:06<02:47,  2.21it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 413/782 [03:07<02:46,  2.21it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 414/782 [03:07<02:46,  2.21it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 415/782 [03:08<02:45,  2.21it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 416/782 [03:08<02:45,  2.22it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 417/782 [03:09<02:44,  2.22it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 418/782 [03:09<02:44,  2.22it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 419/782 [03:09<02:43,  2.22it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 420/782 [03:10<02:43,  2.22it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 421/782 [03:10<02:42,  2.22it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 422/782 [03:11<02:42,  2.22it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 423/782 [03:11<02:41,  2.22it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 424/782 [03:12<02:41,  2.22it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 425/782 [03:12<02:41,  2.22it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 426/782 [03:13<02:40,  2.22it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 427/782 [03:13<02:40,  2.22it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 428/782 [03:13<02:39,  2.22it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 429/782 [03:14<02:39,  2.22it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 430/782 [03:14<02:38,  2.22it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 431/782 [03:15<02:38,  2.22it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 432/782 [03:15<02:37,  2.22it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 433/782 [03:16<02:37,  2.22it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 434/782 [03:16<02:37,  2.22it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 435/782 [03:17<02:36,  2.21it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 436/782 [03:17<02:36,  2.21it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 437/782 [03:18<02:35,  2.21it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 438/782 [03:18<02:35,  2.22it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 439/782 [03:18<02:34,  2.22it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 440/782 [03:19<02:34,  2.22it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 441/782 [03:19<02:33,  2.22it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 442/782 [03:20<02:33,  2.22it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 443/782 [03:20<02:32,  2.22it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 444/782 [03:21<02:32,  2.22it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 445/782 [03:21<02:31,  2.22it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 446/782 [03:22<02:31,  2.22it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 447/782 [03:22<02:31,  2.22it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 448/782 [03:22<02:30,  2.22it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 449/782 [03:23<02:30,  2.22it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 450/782 [03:23<02:29,  2.22it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 451/782 [03:24<02:29,  2.22it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 452/782 [03:24<02:28,  2.22it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 453/782 [03:25<02:28,  2.22it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 454/782 [03:25<02:27,  2.22it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 455/782 [03:26<02:27,  2.22it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 456/782 [03:26<02:26,  2.22it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 457/782 [03:27<02:26,  2.22it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 458/782 [03:27<02:26,  2.22it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 459/782 [03:27<02:25,  2.22it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 460/782 [03:28<02:25,  2.22it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 461/782 [03:28<02:24,  2.22it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 462/782 [03:29<02:24,  2.22it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 463/782 [03:29<02:23,  2.22it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 464/782 [03:30<02:23,  2.22it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 465/782 [03:30<02:22,  2.22it/s]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 466/782 [03:31<02:22,  2.22it/s]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 467/782 [03:31<02:22,  2.22it/s]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 468/782 [03:32<02:21,  2.22it/s]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 469/782 [03:32<02:21,  2.21it/s]\u001b[0m\n",
      "\u001b[34m60%|██████    | 470/782 [03:32<02:20,  2.22it/s]\u001b[0m\n",
      "\u001b[34m60%|██████    | 471/782 [03:33<02:20,  2.21it/s]\u001b[0m\n",
      "\u001b[34m60%|██████    | 472/782 [03:33<02:20,  2.21it/s]\u001b[0m\n",
      "\u001b[34m60%|██████    | 473/782 [03:34<02:19,  2.21it/s]\u001b[0m\n",
      "\u001b[34m61%|██████    | 474/782 [03:34<02:19,  2.22it/s]\u001b[0m\n",
      "\u001b[34m61%|██████    | 475/782 [03:35<02:18,  2.22it/s]\u001b[0m\n",
      "\u001b[34m61%|██████    | 476/782 [03:35<02:18,  2.22it/s]\u001b[0m\n",
      "\u001b[34m61%|██████    | 477/782 [03:36<02:17,  2.22it/s]\u001b[0m\n",
      "\u001b[34m61%|██████    | 478/782 [03:36<02:17,  2.22it/s]\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 479/782 [03:36<02:16,  2.22it/s]\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 480/782 [03:37<02:16,  2.22it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 481/782 [03:37<02:15,  2.22it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 482/782 [03:38<02:15,  2.21it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 483/782 [03:38<02:15,  2.21it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 484/782 [03:39<02:14,  2.21it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 485/782 [03:39<02:14,  2.21it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 486/782 [03:40<02:13,  2.21it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 487/782 [03:40<02:13,  2.21it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 488/782 [03:41<02:12,  2.21it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 489/782 [03:41<02:12,  2.21it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 490/782 [03:41<02:11,  2.21it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 491/782 [03:42<02:11,  2.22it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 492/782 [03:42<02:10,  2.22it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 493/782 [03:43<02:10,  2.22it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 494/782 [03:43<02:09,  2.22it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 495/782 [03:44<02:09,  2.22it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 496/782 [03:44<02:08,  2.22it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 497/782 [03:45<02:08,  2.22it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 498/782 [03:45<02:07,  2.22it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 499/782 [03:46<02:07,  2.22it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 500/782 [03:46<02:07,  2.22it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 0.5968, 'learning_rate': 5e-05, 'epoch': 0.64}\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 500/782 [03:46<02:07,  2.22it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 501/782 [03:46<02:11,  2.14it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 502/782 [03:47<02:09,  2.16it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 503/782 [03:47<02:08,  2.18it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 504/782 [03:48<02:06,  2.19it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 505/782 [03:48<02:06,  2.20it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 506/782 [03:49<02:05,  2.20it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 507/782 [03:49<02:04,  2.21it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 508/782 [03:50<02:03,  2.21it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 509/782 [03:50<02:03,  2.21it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 510/782 [03:51<02:02,  2.22it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 511/782 [03:51<02:02,  2.22it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 512/782 [03:51<02:01,  2.22it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 513/782 [03:52<02:01,  2.22it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 514/782 [03:52<02:00,  2.22it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 515/782 [03:53<02:00,  2.22it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 516/782 [03:53<01:59,  2.22it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 517/782 [03:54<01:59,  2.22it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 518/782 [03:54<01:59,  2.22it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 519/782 [03:55<01:58,  2.22it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 520/782 [03:55<01:58,  2.22it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 521/782 [03:55<01:57,  2.22it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 522/782 [03:56<01:57,  2.22it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 523/782 [03:56<01:56,  2.22it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 524/782 [03:57<01:56,  2.22it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 525/782 [03:57<01:55,  2.22it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 526/782 [03:58<01:55,  2.22it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 527/782 [03:58<01:54,  2.22it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 528/782 [03:59<01:54,  2.22it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 529/782 [03:59<01:54,  2.22it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 530/782 [04:00<01:53,  2.22it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 531/782 [04:00<01:53,  2.21it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 532/782 [04:00<01:52,  2.22it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 533/782 [04:01<01:52,  2.22it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 534/782 [04:01<01:51,  2.22it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 535/782 [04:02<01:51,  2.22it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 536/782 [04:02<01:50,  2.22it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 537/782 [04:03<01:50,  2.22it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 538/782 [04:03<01:50,  2.22it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 539/782 [04:04<01:49,  2.22it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 540/782 [04:04<01:49,  2.22it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 541/782 [04:05<01:48,  2.21it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 542/782 [04:05<01:48,  2.21it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 543/782 [04:05<01:47,  2.22it/s]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 544/782 [04:06<01:47,  2.22it/s]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 545/782 [04:06<01:46,  2.22it/s]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 546/782 [04:07<01:46,  2.22it/s]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 547/782 [04:07<01:45,  2.22it/s]\u001b[0m\n",
      "\u001b[34m70%|███████   | 548/782 [04:08<01:45,  2.22it/s]\u001b[0m\n",
      "\u001b[34m70%|███████   | 549/782 [04:08<01:45,  2.22it/s]\u001b[0m\n",
      "\u001b[34m70%|███████   | 550/782 [04:09<01:44,  2.22it/s]\u001b[0m\n",
      "\u001b[34m70%|███████   | 551/782 [04:09<01:44,  2.22it/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 552/782 [04:09<01:43,  2.22it/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 553/782 [04:10<01:43,  2.22it/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 554/782 [04:10<01:42,  2.22it/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 555/782 [04:11<01:42,  2.22it/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 556/782 [04:11<01:41,  2.22it/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 557/782 [04:12<01:41,  2.22it/s]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 558/782 [04:12<01:40,  2.22it/s]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 559/782 [04:13<01:40,  2.22it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 560/782 [04:13<01:40,  2.22it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 561/782 [04:14<01:39,  2.22it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 562/782 [04:14<01:39,  2.22it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 563/782 [04:14<01:38,  2.22it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 564/782 [04:15<01:38,  2.22it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 565/782 [04:15<01:37,  2.22it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 566/782 [04:16<01:37,  2.22it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 567/782 [04:16<01:36,  2.22it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 568/782 [04:17<01:36,  2.22it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 569/782 [04:17<01:35,  2.22it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 570/782 [04:18<01:35,  2.22it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 571/782 [04:18<01:35,  2.22it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 572/782 [04:18<01:34,  2.22it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 573/782 [04:19<01:34,  2.22it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 574/782 [04:19<01:33,  2.22it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 575/782 [04:20<01:33,  2.22it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 576/782 [04:20<01:32,  2.22it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 577/782 [04:21<01:32,  2.22it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 578/782 [04:21<01:31,  2.22it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 579/782 [04:22<01:31,  2.22it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 580/782 [04:22<01:31,  2.22it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 581/782 [04:23<01:30,  2.22it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 582/782 [04:23<01:30,  2.22it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 583/782 [04:23<01:29,  2.22it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 584/782 [04:24<01:29,  2.22it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 585/782 [04:24<01:28,  2.22it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 586/782 [04:25<01:28,  2.22it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 587/782 [04:25<01:27,  2.22it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 588/782 [04:26<01:27,  2.22it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 589/782 [04:26<01:26,  2.22it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 590/782 [04:27<01:26,  2.22it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 591/782 [04:27<01:26,  2.22it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 592/782 [04:27<01:25,  2.22it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 593/782 [04:28<01:25,  2.22it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 594/782 [04:28<01:24,  2.22it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 595/782 [04:29<01:24,  2.22it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 596/782 [04:29<01:23,  2.22it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 597/782 [04:30<01:23,  2.22it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 598/782 [04:30<01:22,  2.22it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 599/782 [04:31<01:22,  2.22it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 600/782 [04:31<01:21,  2.22it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 601/782 [04:32<01:21,  2.22it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 602/782 [04:32<01:21,  2.22it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 603/782 [04:32<01:20,  2.22it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 604/782 [04:33<01:20,  2.22it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 605/782 [04:33<01:19,  2.22it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 606/782 [04:34<01:19,  2.22it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 607/782 [04:34<01:18,  2.22it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 608/782 [04:35<01:18,  2.22it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 609/782 [04:35<01:17,  2.22it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 610/782 [04:36<01:17,  2.22it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 611/782 [04:36<01:17,  2.22it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 612/782 [04:36<01:16,  2.22it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 613/782 [04:37<01:16,  2.22it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 614/782 [04:37<01:15,  2.22it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 615/782 [04:38<01:15,  2.22it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 616/782 [04:38<01:14,  2.22it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 617/782 [04:39<01:14,  2.22it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 618/782 [04:39<01:13,  2.22it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 619/782 [04:40<01:13,  2.22it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 620/782 [04:40<01:12,  2.22it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 621/782 [04:41<01:12,  2.22it/s]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 622/782 [04:41<01:12,  2.22it/s]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 623/782 [04:41<01:11,  2.22it/s]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 624/782 [04:42<01:11,  2.22it/s]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 625/782 [04:42<01:10,  2.22it/s]\u001b[0m\n",
      "\u001b[34m80%|████████  | 626/782 [04:43<01:10,  2.22it/s]\u001b[0m\n",
      "\u001b[34m80%|████████  | 627/782 [04:43<01:09,  2.22it/s]\u001b[0m\n",
      "\u001b[34m80%|████████  | 628/782 [04:44<01:09,  2.22it/s]\u001b[0m\n",
      "\u001b[34m80%|████████  | 629/782 [04:44<01:08,  2.22it/s]\u001b[0m\n",
      "\u001b[34m81%|████████  | 630/782 [04:45<01:08,  2.22it/s]\u001b[0m\n",
      "\u001b[34m81%|████████  | 631/782 [04:45<01:08,  2.22it/s]\u001b[0m\n",
      "\u001b[34m81%|████████  | 632/782 [04:46<01:07,  2.22it/s]\u001b[0m\n",
      "\u001b[34m81%|████████  | 633/782 [04:46<01:07,  2.22it/s]\u001b[0m\n",
      "\u001b[34m81%|████████  | 634/782 [04:46<01:06,  2.22it/s]\u001b[0m\n",
      "\u001b[34m81%|████████  | 635/782 [04:47<01:06,  2.22it/s]\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 636/782 [04:47<01:05,  2.22it/s]\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 637/782 [04:48<01:05,  2.22it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 638/782 [04:48<01:04,  2.22it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 639/782 [04:49<01:04,  2.22it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 640/782 [04:49<01:03,  2.22it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 641/782 [04:50<01:03,  2.22it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 642/782 [04:50<01:03,  2.22it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 643/782 [04:50<01:02,  2.22it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 644/782 [04:51<01:02,  2.22it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 645/782 [04:51<01:01,  2.22it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 646/782 [04:52<01:01,  2.22it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 647/782 [04:52<01:00,  2.22it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 648/782 [04:53<01:00,  2.22it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 649/782 [04:53<00:59,  2.22it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 650/782 [04:54<00:59,  2.22it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 651/782 [04:54<00:59,  2.22it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 652/782 [04:55<00:58,  2.22it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▎ | 653/782 [04:55<00:58,  2.22it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▎ | 654/782 [04:55<00:57,  2.22it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 655/782 [04:56<00:57,  2.22it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 656/782 [04:56<00:56,  2.22it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 657/782 [04:57<00:56,  2.22it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 658/782 [04:57<00:55,  2.22it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 659/782 [04:58<00:55,  2.22it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 660/782 [04:58<00:55,  2.22it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 661/782 [04:59<00:54,  2.22it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 662/782 [04:59<00:54,  2.22it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 663/782 [04:59<00:53,  2.22it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 664/782 [05:00<00:53,  2.22it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 665/782 [05:00<00:52,  2.22it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 666/782 [05:01<00:52,  2.22it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 667/782 [05:01<00:51,  2.22it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 668/782 [05:02<00:51,  2.22it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 669/782 [05:02<00:50,  2.22it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 670/782 [05:03<00:50,  2.22it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 671/782 [05:03<00:50,  2.22it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 672/782 [05:04<00:49,  2.22it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 673/782 [05:04<00:49,  2.22it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 674/782 [05:04<00:48,  2.22it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 675/782 [05:05<00:48,  2.22it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 676/782 [05:05<00:47,  2.22it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 677/782 [05:06<00:47,  2.22it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 678/782 [05:06<00:46,  2.22it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 679/782 [05:07<00:46,  2.22it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 680/782 [05:07<00:46,  2.22it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 681/782 [05:08<00:45,  2.22it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 682/782 [05:08<00:45,  2.22it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 683/782 [05:08<00:44,  2.22it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 684/782 [05:09<00:44,  2.22it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 685/782 [05:09<00:43,  2.22it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 686/782 [05:10<00:43,  2.21it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 687/782 [05:10<00:42,  2.21it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 688/782 [05:11<00:42,  2.21it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 689/782 [05:11<00:42,  2.21it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 690/782 [05:12<00:41,  2.21it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 691/782 [05:12<00:41,  2.21it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 692/782 [05:13<00:40,  2.21it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 693/782 [05:13<00:40,  2.21it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 694/782 [05:13<00:39,  2.21it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 695/782 [05:14<00:39,  2.21it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 696/782 [05:14<00:38,  2.21it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 697/782 [05:15<00:38,  2.21it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 698/782 [05:15<00:37,  2.21it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 699/782 [05:16<00:37,  2.21it/s]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 700/782 [05:16<00:37,  2.21it/s]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 701/782 [05:17<00:36,  2.21it/s]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 702/782 [05:17<00:36,  2.21it/s]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 703/782 [05:18<00:35,  2.22it/s]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 704/782 [05:18<00:35,  2.22it/s]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 705/782 [05:18<00:34,  2.22it/s]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 706/782 [05:19<00:34,  2.22it/s]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 707/782 [05:19<00:33,  2.22it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 708/782 [05:20<00:33,  2.22it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 709/782 [05:20<00:32,  2.22it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 710/782 [05:21<00:32,  2.22it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 711/782 [05:21<00:31,  2.22it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 712/782 [05:22<00:31,  2.22it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 713/782 [05:22<00:31,  2.22it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 714/782 [05:22<00:30,  2.22it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 715/782 [05:23<00:30,  2.22it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 716/782 [05:23<00:29,  2.22it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 717/782 [05:24<00:29,  2.22it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 718/782 [05:24<00:28,  2.22it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 719/782 [05:25<00:28,  2.22it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 720/782 [05:25<00:27,  2.22it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 721/782 [05:26<00:27,  2.22it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 722/782 [05:26<00:27,  2.22it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 723/782 [05:27<00:26,  2.22it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 724/782 [05:27<00:26,  2.22it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 725/782 [05:27<00:25,  2.22it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 726/782 [05:28<00:25,  2.22it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 727/782 [05:28<00:24,  2.22it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 728/782 [05:29<00:24,  2.22it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 729/782 [05:29<00:23,  2.22it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 730/782 [05:30<00:23,  2.22it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 731/782 [05:30<00:22,  2.22it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 732/782 [05:31<00:22,  2.22it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 733/782 [05:31<00:22,  2.22it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 734/782 [05:32<00:21,  2.22it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 735/782 [05:32<00:21,  2.22it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 736/782 [05:32<00:20,  2.22it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 737/782 [05:33<00:20,  2.22it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 738/782 [05:33<00:19,  2.22it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 739/782 [05:34<00:19,  2.22it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 740/782 [05:34<00:18,  2.22it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 741/782 [05:35<00:18,  2.22it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 742/782 [05:35<00:18,  2.22it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 743/782 [05:36<00:17,  2.22it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 744/782 [05:36<00:17,  2.22it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 745/782 [05:36<00:16,  2.22it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 746/782 [05:37<00:16,  2.22it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 747/782 [05:37<00:15,  2.22it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 748/782 [05:38<00:15,  2.21it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 749/782 [05:38<00:14,  2.21it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 750/782 [05:39<00:14,  2.21it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 751/782 [05:39<00:13,  2.21it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 752/782 [05:40<00:13,  2.21it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 753/782 [05:40<00:13,  2.21it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 754/782 [05:41<00:12,  2.22it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 755/782 [05:41<00:12,  2.21it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 756/782 [05:41<00:11,  2.21it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 757/782 [05:42<00:11,  2.21it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 758/782 [05:42<00:10,  2.21it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 759/782 [05:43<00:10,  2.21it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 760/782 [05:43<00:09,  2.21it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 761/782 [05:44<00:09,  2.21it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 762/782 [05:44<00:09,  2.21it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 763/782 [05:45<00:08,  2.21it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 764/782 [05:45<00:08,  2.21it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 765/782 [05:46<00:07,  2.22it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 766/782 [05:46<00:07,  2.22it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 767/782 [05:46<00:06,  2.22it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 768/782 [05:47<00:06,  2.22it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 769/782 [05:47<00:05,  2.22it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 770/782 [05:48<00:05,  2.22it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 771/782 [05:48<00:04,  2.22it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 772/782 [05:49<00:04,  2.22it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 773/782 [05:49<00:04,  2.22it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 774/782 [05:50<00:03,  2.22it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 775/782 [05:50<00:03,  2.22it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 776/782 [05:50<00:02,  2.22it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 777/782 [05:51<00:02,  2.22it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 778/782 [05:51<00:01,  2.22it/s]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 779/782 [05:52<00:01,  2.22it/s]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 780/782 [05:52<00:00,  2.22it/s]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 781/782 [05:53<00:00,  2.23it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 782/782 [05:53<00:00,  2.85it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 0/40 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m5%|▌         | 2/40 [00:00<00:18,  2.03it/s]#033[A\u001b[0m\n",
      "\u001b[34m8%|▊         | 3/40 [00:01<00:25,  1.43it/s]#033[A\u001b[0m\n",
      "\u001b[34m10%|█         | 4/40 [00:02<00:28,  1.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m12%|█▎        | 5/40 [00:03<00:30,  1.15it/s]#033[A\u001b[0m\n",
      "\u001b[34m15%|█▌        | 6/40 [00:04<00:30,  1.10it/s]#033[A\u001b[0m\n",
      "\u001b[34m18%|█▊        | 7/40 [00:05<00:30,  1.07it/s]#033[A\u001b[0m\n",
      "\u001b[34m20%|██        | 8/40 [00:06<00:30,  1.05it/s]#033[A\u001b[0m\n",
      "\u001b[34m22%|██▎       | 9/40 [00:07<00:29,  1.04it/s]#033[A\u001b[0m\n",
      "\u001b[34m25%|██▌       | 10/40 [00:08<00:29,  1.03it/s]#033[A\u001b[0m\n",
      "\u001b[34m28%|██▊       | 11/40 [00:09<00:28,  1.02it/s]#033[A\u001b[0m\n",
      "\u001b[34m30%|███       | 12/40 [00:10<00:27,  1.02it/s]#033[A\u001b[0m\n",
      "\u001b[34m32%|███▎      | 13/40 [00:11<00:26,  1.02it/s]#033[A\u001b[0m\n",
      "\u001b[34m35%|███▌      | 14/40 [00:12<00:25,  1.01it/s]#033[A\u001b[0m\n",
      "\u001b[34m38%|███▊      | 15/40 [00:13<00:24,  1.01it/s]#033[A\u001b[0m\n",
      "\u001b[34m40%|████      | 16/40 [00:14<00:23,  1.01it/s]#033[A\u001b[0m\n",
      "\u001b[34m42%|████▎     | 17/40 [00:15<00:22,  1.01it/s]#033[A\u001b[0m\n",
      "\u001b[34m45%|████▌     | 18/40 [00:16<00:21,  1.01it/s]#033[A\u001b[0m\n",
      "\u001b[34m48%|████▊     | 19/40 [00:17<00:20,  1.01it/s]#033[A\u001b[0m\n",
      "\u001b[34m50%|█████     | 20/40 [00:18<00:19,  1.01it/s]#033[A\u001b[0m\n",
      "\u001b[34m52%|█████▎    | 21/40 [00:19<00:18,  1.01it/s]#033[A\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 22/40 [00:20<00:17,  1.01it/s]#033[A\u001b[0m\n",
      "\u001b[34m57%|█████▊    | 23/40 [00:21<00:16,  1.01it/s]#033[A\u001b[0m\n",
      "\u001b[34m60%|██████    | 24/40 [00:22<00:15,  1.01it/s]#033[A\u001b[0m\n",
      "\u001b[34m62%|██████▎   | 25/40 [00:23<00:14,  1.01it/s]#033[A\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 26/40 [00:24<00:13,  1.01it/s]#033[A\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 27/40 [00:25<00:12,  1.01it/s]#033[A\u001b[0m\n",
      "\u001b[34m70%|███████   | 28/40 [00:26<00:11,  1.01it/s]#033[A\u001b[0m\n",
      "\u001b[34m72%|███████▎  | 29/40 [00:27<00:10,  1.01it/s]#033[A\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 30/40 [00:28<00:09,  1.01it/s]#033[A\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 31/40 [00:29<00:08,  1.01it/s]#033[A\u001b[0m\n",
      "\u001b[34m80%|████████  | 32/40 [00:30<00:07,  1.01it/s]#033[A\u001b[0m\n",
      "\u001b[34m82%|████████▎ | 33/40 [00:31<00:06,  1.01it/s]#033[A\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 34/40 [00:32<00:05,  1.01it/s]#033[A\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 35/40 [00:33<00:04,  1.01it/s]#033[A\u001b[0m\n",
      "\u001b[34m90%|█████████ | 36/40 [00:34<00:03,  1.01it/s]#033[A\u001b[0m\n",
      "\u001b[34m92%|█████████▎| 37/40 [00:35<00:02,  1.01it/s]#033[A\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 38/40 [00:36<00:01,  1.01it/s]#033[A\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 39/40 [00:37<00:00,  1.04it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2878725528717041, 'eval_accuracy': 0.8836, 'eval_f1': 0.88459250446163, 'eval_precision': 0.8784954706577393, 'eval_recall': 0.8907747603833865, 'eval_runtime': 38.5864, 'eval_samples_per_second': 259.159, 'eval_steps_per_second': 1.037, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 782/782 [06:31<00:00,  2.85it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 40/40 [00:37<00:00,  1.04it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'train_runtime': 391.9191, 'train_samples_per_second': 63.789, 'train_steps_per_second': 1.995, 'train_loss': 0.4953826943321911, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 782/782 [06:31<00:00,  2.85it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 782/782 [06:31<00:00,  2.00it/s]\u001b[0m\n",
      "\u001b[34mEXEC TIME: 392.0092279911041\u001b[0m\n",
      "\u001b[34m0%|          | 0/40 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 2/40 [00:00<00:18,  2.03it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 3/40 [00:01<00:25,  1.43it/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 4/40 [00:02<00:28,  1.24it/s]\u001b[0m\n",
      "\u001b[34m12%|█▎        | 5/40 [00:03<00:30,  1.15it/s]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 6/40 [00:04<00:30,  1.10it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 7/40 [00:05<00:30,  1.07it/s]\u001b[0m\n",
      "\u001b[34m20%|██        | 8/40 [00:06<00:30,  1.05it/s]\u001b[0m\n",
      "\u001b[34m22%|██▎       | 9/40 [00:07<00:29,  1.04it/s]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 10/40 [00:08<00:29,  1.03it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 11/40 [00:09<00:28,  1.02it/s]\u001b[0m\n",
      "\u001b[34m30%|███       | 12/40 [00:10<00:27,  1.02it/s]\u001b[0m\n",
      "\u001b[34m32%|███▎      | 13/40 [00:11<00:26,  1.02it/s]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 14/40 [00:12<00:25,  1.02it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 15/40 [00:13<00:24,  1.02it/s]\u001b[0m\n",
      "\u001b[34m40%|████      | 16/40 [00:14<00:23,  1.02it/s]\u001b[0m\n",
      "\u001b[34m42%|████▎     | 17/40 [00:15<00:22,  1.02it/s]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 18/40 [00:16<00:21,  1.02it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 19/40 [00:17<00:20,  1.01it/s]\u001b[0m\n",
      "\u001b[34m50%|█████     | 20/40 [00:18<00:19,  1.01it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▎    | 21/40 [00:19<00:18,  1.01it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 22/40 [00:20<00:17,  1.02it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▊    | 23/40 [00:21<00:16,  1.02it/s]\u001b[0m\n",
      "\u001b[34m60%|██████    | 24/40 [00:22<00:15,  1.01it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▎   | 25/40 [00:23<00:14,  1.01it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 26/40 [00:24<00:13,  1.01it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 27/40 [00:25<00:12,  1.01it/s]\u001b[0m\n",
      "\u001b[34m70%|███████   | 28/40 [00:26<00:11,  1.01it/s]\u001b[0m\n",
      "\n",
      "2023-09-22 12:50:18 Uploading - Uploading generated training model\u001b[34m72%|███████▎  | 29/40 [00:27<00:10,  1.01it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 30/40 [00:28<00:09,  1.02it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 31/40 [00:29<00:08,  1.02it/s]\u001b[0m\n",
      "\u001b[34m80%|████████  | 32/40 [00:30<00:07,  1.01it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▎ | 33/40 [00:31<00:06,  1.02it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 34/40 [00:32<00:05,  1.02it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 35/40 [00:33<00:04,  1.02it/s]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 36/40 [00:34<00:03,  1.02it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▎| 37/40 [00:35<00:02,  1.02it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 38/40 [00:36<00:01,  1.02it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 39/40 [00:37<00:00,  1.04it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 40/40 [00:37<00:00,  1.07it/s]\u001b[0m\n",
      "\u001b[34m***** Eval results *****\u001b[0m\n",
      "\u001b[34m2023-09-22 12:50:13,849 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-09-22 12:50:13,849 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-09-22 12:50:13,850 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-09-22 12:50:28 Completed - Training job completed\n",
      "Training seconds: 725\n",
      "Billable seconds: 725\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" 2ND RUN\\n{'eval_loss': 0.30123773217201233, 'eval_accuracy': 0.883, 'eval_f1': 0.884318766066838, 'eval_precision': 0.8758323540932237, 'eval_recall': 0.8929712460063898, 'eval_runtime': 38.5191, 'eval_samples_per_second': 259.612, 'eval_steps_per_second': 1.038, 'epoch': 1.0}\\n100%|██████████| 782/782 [06:31<00:00,  2.85it/s]\\n#015100%|██████████| 40/40 [00:37<00:00,  1.04it/s]#033[A\\n#033[A\\n{'train_runtime': 391.7503, 'train_samples_per_second': 63.816, 'train_steps_per_second': 1.996, 'train_loss': 0.5238332626459848, 'epoch': 1.0}\\n100%|██████████| 782/782 [06:31<00:00,  2.85it/s]\\n100%|██████████| 782/782 [06:31<00:00,  2.00it/s]\\nEXEC TIME: 391.8400411605835\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "# hyperparameters, which are passed into the training job\n",
    "\n",
    "if dataset_name == 'imdb':\n",
    "    hyperparameters={'epochs': 1,\n",
    "                     'train_batch_size': 32,\n",
    "                     'model_name':'distilbert-base-uncased',\n",
    "                     \"eval_batch_size\": 256, # InExample: 64\n",
    "                     \"learning_rate\": 5e-5 * 1 # InExample: 5e-5\n",
    "                    }\n",
    "\n",
    "#entry_point = 'train.py' # NO LORA\n",
    "#entry_point = 'train-LORA.py'\n",
    "entry_point = \"train-QLORA.py\"\n",
    "\n",
    "huggingface_estimator = HuggingFace(entry_point=entry_point,\n",
    "                                    source_dir='./',\n",
    "                                    instance_type='ml.g5.4xlarge', #'ml.p3.2xlarge', 'ml.g5.4xlarge'-- for 64GB of Memory\n",
    "                                    instance_count=1,\n",
    "                                    role=role,\n",
    "                                    transformers_version='4.26',\n",
    "                                    pytorch_version='1.13',\n",
    "                                    py_version='py39', \n",
    "                                    hyperparameters = hyperparameters)\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit({'train': training_input_path, \n",
    "                           'test': test_input_path})\n",
    "\n",
    "### SINGE GPU RESULTS ANALYSIS\n",
    "\n",
    "# MODEL: distilbert-base-uncased\n",
    "# DATASET: imdb\n",
    "# TASK: BinarySequenceClassification\n",
    "\n",
    "\n",
    "\n",
    "### NO LORA -- BENCHMARK\n",
    "\n",
    "## INSTANCE_TYPE: ml.p3.2xlarge\n",
    "\n",
    "# EXEC TIME: 411.8172905445099 -- train & first evaluation time\n",
    "\n",
    "# 'eval_accuracy': 0.928, \n",
    "# 'eval_f1': 0.9274924471299093, \n",
    "# 'eval_precision': 0.9373091797272542, \n",
    "# 'eval_recall': 0.9178792106836755,\n",
    "\n",
    "## INSTANCE_TYPE: ml.g5.4xlarge\n",
    "\n",
    "## EXEC TIME: 381.3633711338043\n",
    "\n",
    "# 'eval_loss': 0.18705244362354279, \n",
    "# 'eval_accuracy': 0.9284, \n",
    "# 'eval_f1': 0.9277205733898648, \n",
    "# 'eval_precision': 0.9381380155165374, \n",
    "# 'eval_recall': 0.9175319488817891, \n",
    "\n",
    "\n",
    "\n",
    "### WITH LoRA -- as expected: FASTER ALTHOUGH SLIGHTLY WEAKER FIT\n",
    "\n",
    "## INSTANCE_TYPE: ml.p3.2xlarge\n",
    "\n",
    "# EXEC TIME: 331.6438286304474\n",
    "\n",
    "# 'eval_accuracy': 0.9055, \n",
    "# 'eval_f1': 0.9063893016344725, \n",
    "# 'eval_precision': 0.9009452540370224, \n",
    "# 'eval_recall': 0.9118995415587005\n",
    "\n",
    "## INSTANCE_TYPE: ml.g5.4xlarge\n",
    "\n",
    "# EXEC TIME: 346.6780774593353\n",
    "\n",
    "# 'eval_loss': 0.27975377440452576, \n",
    "# 'eval_accuracy': 0.8929, \n",
    "# 'eval_f1': 0.893294809205938, \n",
    "# 'eval_precision': 0.8914297076953669, \n",
    "# 'eval_recall': 0.895167731629393,\n",
    "\n",
    "\n",
    "\n",
    "### WITH QLoRA -- as expected: EVEN WEAKER, BUT SLIGHTLY SLOWER!?!?!?!\n",
    "\n",
    "## INSTANCE_TYPE: ml.p3.2xlarge\n",
    "\n",
    "# EXEC TIME: 275.81963658332825 -- MUCH FASTER?!?!?!?\n",
    "\n",
    "# 'eval_loss': 0.3110389709472656, \n",
    "# 'eval_accuracy': 0.8835, \n",
    "# 'eval_f1': 0.8835582208895553, \n",
    "# 'eval_precision': 0.8845307184310587, \n",
    "# 'eval_recall': 0.8825878594249201\n",
    "\n",
    "## INSTANCE_TYPE: ml.g5.4xlarge\n",
    "\n",
    "# EXEC TIME: 392.0947148799896\n",
    "\n",
    "# 'eval_loss': 0.29552170634269714, \n",
    "# 'eval_accuracy': 0.8848, \n",
    "# 'eval_f1': 0.8866141732283463, \n",
    "# 'eval_precision': 0.8742236024844721, \n",
    "# 'eval_recall': 0.8993610223642172,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-pytorch-training-2023-09-22-19-33-44-463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-22 19:34:26 Starting - Starting the training job......\n",
      "2023-09-22 19:35:12 Starting - Preparing the instances for training.........\n",
      "2023-09-22 19:36:39 Downloading - Downloading input data...\n",
      "2023-09-22 19:37:04 Training - Downloading the training image..................\n",
      "2023-09-22 19:40:00 Training - Training image download completed. Training in progress...\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-09-22 19:40:31,921 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-09-22 19:40:31,982 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-22 19:40:31,994 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-09-22 19:40:31,996 sagemaker_pytorch_container.training INFO     Invoking SMDataParallel\u001b[0m\n",
      "\u001b[34m2023-09-22 19:40:31,997 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-09-22 19:40:39,201 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-22 19:40:39,277 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-22 19:40:39,291 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2023-09-22 19:40:39,291 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2023-09-22 19:40:39,293 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2023-09-22 19:40:39,294 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[34m2023-09-22 19:40:39,294 sagemaker-training-toolkit INFO     Host: ['algo-1']\u001b[0m\n",
      "\u001b[34m2023-09-22 19:40:39,295 sagemaker-training-toolkit INFO     sagemaker_communication_backend: None\u001b[0m\n",
      "\u001b[34m2023-09-22 19:40:39,295 sagemaker-training-toolkit WARNING  Missing library /opt/conda/lib/libsmddp.so for SMDDP collective\u001b[0m\n",
      "\u001b[34m2023-09-22 19:40:39,295 sagemaker-training-toolkit WARNING  The system is not configured to run SMDDP collectives optimizedfor AWS infrastructure.Please use the latest SageMaker Deep Learning Container (DLC) to enable SMDDP Collectives support.\u001b[0m\n",
      "\u001b[34mContinuing model training with default NCCL communication backend.\u001b[0m\n",
      "\u001b[34m2023-09-22 19:40:39,296 sagemaker-training-toolkit INFO     instance type: ml.p3.16xlarge\u001b[0m\n",
      "\u001b[34m2023-09-22 19:40:39,296 sagemaker-training-toolkit INFO     Env Hosts: ['algo-1'] Hosts: ['algo-1'] process_per_hosts: 8 num_processes: 8\u001b[0m\n",
      "\u001b[34m2023-09-22 19:40:39,359 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-22 19:40:39,373 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_distributed_dataparallel_custom_mpi_options\": \"-verbose -x NCCL_DEBUG=VERSION\",\n",
      "        \"sagemaker_distributed_dataparallel_enabled\": true,\n",
      "        \"sagemaker_instance_type\": \"ml.p3.16xlarge\"\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p3.16xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 1,\n",
      "        \"eval_batch_size\": 256,\n",
      "        \"learning_rate\": 0.0128,\n",
      "        \"model_name\": \"distilbert-base-uncased\",\n",
      "        \"train_batch_size\": 256\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p3.16xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2023-09-22-19-33-44-463\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-578864530451/huggingface-pytorch-training-2023-09-22-19-33-44-463/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train-QLORA\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.16xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.16xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train-QLORA.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":1,\"eval_batch_size\":256,\"learning_rate\":0.0128,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":256}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train-QLORA.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_distributed_dataparallel_custom_mpi_options\":\"-verbose -x NCCL_DEBUG=VERSION\",\"sagemaker_distributed_dataparallel_enabled\":true,\"sagemaker_instance_type\":\"ml.p3.16xlarge\"}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.16xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p3.16xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train-QLORA\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-578864530451/huggingface-pytorch-training-2023-09-22-19-33-44-463/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_custom_mpi_options\":\"-verbose -x NCCL_DEBUG=VERSION\",\"sagemaker_distributed_dataparallel_enabled\":true,\"sagemaker_instance_type\":\"ml.p3.16xlarge\"},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p3.16xlarge\",\"distribution_hosts\":[\"algo-1\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":1,\"eval_batch_size\":256,\"learning_rate\":0.0128,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":256},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-pytorch-training-2023-09-22-19-33-44-463\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-578864530451/huggingface-pytorch-training-2023-09-22-19-33-44-463/source/sourcedir.tar.gz\",\"module_name\":\"train-QLORA\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.16xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train-QLORA.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"1\",\"--eval_batch_size\",\"256\",\"--learning_rate\",\"0.0128\",\"--model_name\",\"distilbert-base-uncased\",\"--train_batch_size\",\"256\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_BATCH_SIZE=256\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0128\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=distilbert-base-uncased\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=256\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1 -np 8 --allow-run-as-root --tag-output --oversubscribe -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -mca plm_rsh_num_concurrent 1 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x SMDATAPARALLEL_USE_SINGLENODE=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1 -x LD_PRELOAD=/opt/conda/lib/python3.9/site-packages/gethostname.cpython-39-x86_64-linux-gnu.so -verbose -x NCCL_DEBUG=VERSION -x USE_SMDDP_COLLECTIVES=0 smddprun /opt/conda/bin/python3.9 -m mpi4py train-QLORA.py --epochs 1 --eval_batch_size 256 --learning_rate 0.0128 --model_name distilbert-base-uncased --train_batch_size 256\u001b[0m\n",
      "\u001b[34m[2023-09-22 19:40:41.530: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m2023-09-22 19:40:41,536 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:ERROR: Exception:\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/cli/base_command.py\", line 160, in exc_logging_wrapper\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:    status = run_func(*args)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/cli/req_command.py\", line 247, in wrapper\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:    return func(self, options, args)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/commands/install.py\", line 503, in run\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:    installed = install_given_reqs(\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/req/__init__.py\", line 73, in install_given_reqs\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:    requirement.install(\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/req/req_install.py\", line 796, in install\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:    install_wheel(\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/operations/install/wheel.py\", line 729, in install_wheel\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:    _install_wheel(\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/operations/install/wheel.py\", line 618, in _install_wheel\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:    assert os.path.exists(pyc_path)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:AssertionError\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:ERROR: Wheel 'rouge-score' located at /root/.cache/pip/wheels/9b/3d/39/09558097d3119ca0a4d462df68f22c6f3c1b345ac63a09b86e/rouge_score-0.1.2-py3-none-any.whl is invalid.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:WARNING: Error parsing requirements for tokenizers: [Errno 2] No such file or directory: '/opt/conda/lib/python3.9/site-packages/tokenizers-0.13.2.dist-info/METADATA'\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:WARNING: Error parsing requirements for tokenizers: [Errno 2] No such file or directory: '/opt/conda/lib/python3.9/site-packages/tokenizers-0.13.2.dist-info/METADATA'\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:WARNING: Error parsing requirements for tokenizers: [Errno 2] No such file or directory: '/opt/conda/lib/python3.9/site-packages/tokenizers-0.13.2.dist-info/METADATA'\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:WARNING: Error parsing requirements for tokenizers: [Errno 2] No such file or directory: '/opt/conda/lib/python3.9/site-packages/tokenizers-0.13.2.dist-info/METADATA'\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:WARNING: Error parsing requirements for tokenizers: [Errno 2] No such file or directory: '/opt/conda/lib/python3.9/site-packages/tokenizers-0.13.2.dist-info/METADATA'\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:WARNING: Error parsing requirements for tokenizers: [Errno 2] No such file or directory: '/opt/conda/lib/python3.9/site-packages/tokenizers-0.13.2.dist-info/METADATA'\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:WARNING: Error parsing requirements for huggingface-hub: [Errno 2] No such file or directory: '/opt/conda/lib/python3.9/site-packages/huggingface_hub-0.12.0.dist-info/METADATA'\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:WARNING: Error parsing requirements for huggingface-hub: [Errno 2] No such file or directory: '/opt/conda/lib/python3.9/site-packages/huggingface_hub-0.12.0.dist-info/METADATA'\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:WARNING: Error parsing requirements for huggingface-hub: [Errno 2] No such file or directory: '/opt/conda/lib/python3.9/site-packages/huggingface_hub-0.12.0.dist-info/METADATA'\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:WARNING: Error parsing requirements for huggingface-hub: [Errno 2] No such file or directory: '/opt/conda/lib/python3.9/site-packages/huggingface_hub-0.12.0.dist-info/METADATA'\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:WARNING: Error parsing requirements for huggingface-hub: [Errno 2] No such file or directory: '/opt/conda/lib/python3.9/site-packages/huggingface_hub-0.12.0.dist-info/METADATA'\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:WARNING: Error parsing requirements for huggingface-hub: [Errno 2] No such file or directory: '/opt/conda/lib/python3.9/site-packages/huggingface_hub-0.12.0.dist-info/METADATA'\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:WARNING: Error parsing requirements for huggingface-hub: [Errno 2] No such file or directory: '/opt/conda/lib/python3.9/site-packages/huggingface_hub-0.12.0.dist-info/METADATA'\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:    WARNING: No metadata found in /opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:    WARNING: No metadata found in /opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:    WARNING: No metadata found in /opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    WARNING: No metadata found in /opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:    WARNING: No metadata found in /opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:  ERROR: Can't roll back huggingface-hub; was not uninstalled\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/opt/conda/lib/python3.9/site-packages/huggingface_hub/_snapshot_download.py'\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:    WARNING: No metadata found in /opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:    WARNING: No metadata found in /opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    WARNING: No metadata found in /opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:    WARNING: No metadata found in /opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    WARNING: No metadata found in /opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:    WARNING: No metadata found in /opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:    WARNING: No metadata found in /opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:    WARNING: No metadata found in /opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:    WARNING: No metadata found in /opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:    WARNING: No metadata found in /opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:    WARNING: No metadata found in /opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:    WARNING: No metadata found in /opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:    WARNING: No metadata found in /opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:    WARNING: No metadata found in /opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:  ERROR: Can't roll back transformers; was not uninstalled\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/opt/conda/lib/python3.9/site-packages/transformers/trainer_seq2seq.py'\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:WARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:WARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:WARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:WARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:WARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:WARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:    WARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:WARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:WARNING: Error parsing requirements for accelerate: [Errno 2] No such file or directory: '/opt/conda/lib/python3.9/site-packages/accelerate-0.16.0.dist-info/METADATA'\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:WARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:WARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:    WARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:    WARNING: No metadata found in /opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:WARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:WARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:WARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:sh: 1: cannot open 0.14.0: No such file\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:sh: 1: cannot open 0.14.0: No such file\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:sh: 1: cannot open 0.14.0: No such file\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:sh: 1: cannot open 0.14.0: No such file\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:sh: 1: cannot open 0.14.0: No such file\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:sh: 1: cannot open 0.14.0: No such file\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:sh: 1: cannot open 0.14.0: No such file\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:sh: 1: cannot open 0.14.0: No such file\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:2023-09-22 19:42:01,665 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:2023-09-22 19:42:01,665 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:compute_dtype torch.float16\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:RANK 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:2023-09-22 19:42:01,667 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:2023-09-22 19:42:01,667 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:compute_dtype [1,mpirank:6,algo-1]<stdout>:torch.float16\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:RANK 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading (…)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading (…)lve/main/config.json: 100%|██████████| 483/483 [00:00<00:00, 39.6kB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading model.safetensors:   8%|▊         | 21.0M/268M [00:00<00:01, 154MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading model.safetensors:  20%|█▉        | 52.4M/268M [00:00<00:01, 214MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading model.safetensors:  39%|███▉      | 105M/268M [00:00<00:00, 319MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading model.safetensors:  59%|█████▊    | 157M/268M [00:00<00:00, 376MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading model.safetensors:  78%|███████▊  | 210M/268M [00:00<00:00, 422MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading model.safetensors:  98%|█████████▊| 262M/268M [00:00<00:00, 387MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading model.safetensors: 100%|██████████| 268M/268M [00:00<00:00, 354MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:2023-09-22 19:42:03,801 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:2023-09-22 19:42:03,801 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:compute_dtype[1,mpirank:2,algo-1]<stdout>: torch.float16\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:RANK 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:AFTER prepare_model_for_kbit_training trainable params: 0 || all params: 45721346 || trainable%: 0.00\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'classifier.bias', 'pre_classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'classifier.bias', 'pre_classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:AFTER prepare_model_for_kbit_training trainable params: 0 || all params: 45721346 || trainable%: 0.00\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:QLoRA Model trainable params: 589824 || all params: 46903300 || trainable%: 1.26\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 4.09kB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:2023-09-22 19:42:05,349 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:2023-09-22 19:42:05,349 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:compute_dtype torch.float16\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:RANK 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 53.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:QLoRA Model trainable params: 589824 || all params: 46903300 || trainable%: 1.26\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading (…)/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 39.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:2023-09-22 19:42:05,409 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:2023-09-22 19:42:05,409 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:compute_dtype torch.float16\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:RANK 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:2023-09-22 19:42:05,415 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:2023-09-22 19:42:05,415 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:compute_dtype [1,mpirank:7,algo-1]<stdout>:torch.float16\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:RANK 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:2023-09-22 19:42:05,629 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:2023-09-22 19:42:05,629 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:compute_dtype torch.float16[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:RANK 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'classifier.bias', 'pre_classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'classifier.bias', 'pre_classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:AFTER prepare_model_for_kbit_training trainable params: 0 || all params: 45721346 || trainable%: 0.00\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:QLoRA Model trainable params: 589824 || all params: 46903300 || trainable%: 1.26\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:2023-09-22 19:42:06,506 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:2023-09-22 19:42:06,506 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:compute_dtype[1,mpirank:5,algo-1]<stdout>: torch.float16\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:RANK 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:AFTER prepare_model_for_kbit_training trainable params: 0 || all params: 45721346 || trainable%: 0.00\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:AFTER prepare_model_for_kbit_training trainable params: 0 || all params: 45721346 || trainable%: 0.00\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'classifier.bias', 'pre_classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'classifier.bias', 'pre_classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:QLoRA Model trainable params: 589824 || all params: 46903300 || trainable%: 1.26\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:AFTER prepare_model_for_kbit_training trainable params: 0 || all params: 45721346 || trainable%: 0.00\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:AFTER prepare_model_for_kbit_training trainable params: 0 || all params: 45721346 || trainable%: 0.00\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:QLoRA Model trainable params: 589824 || all params: 46903300 || trainable%: 1.26\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:QLoRA Model trainable params: 589824 || all params: 46903300 || trainable%: 1.26\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:QLoRA Model trainable params: 589824 || all params: 46903300 || trainable%: 1.26\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:AFTER prepare_model_for_kbit_training trainable params: 0 || all params: 45721346 || trainable%: 0.00\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:QLoRA Model trainable params: 589824 || all params: 46903300 || trainable%: 1.26\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:NCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Running smdistributed.dataparallel v1.7.0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:SMDDP: Single node mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:2023-09-22 19:42:10,354 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:1 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:2023-09-22 19:42:10,354 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:1 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:2023-09-22 19:42:10,354 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:1 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:2023-09-22 19:42:10,354 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:1 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:2023-09-22 19:42:10,354 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:1 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:2023-09-22 19:42:10,354 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:1 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:2023-09-22 19:42:10,354 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:1 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:2023-09-22 19:42:10,354 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:1 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:2023-09-22 19:42:10,354 - torch.distributed.distributed_c10d - INFO - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:2023-09-22 19:42:10,354 - torch.distributed.distributed_c10d - INFO - Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:2023-09-22 19:42:10,354 - torch.distributed.distributed_c10d - INFO - Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:2023-09-22 19:42:10,354 - torch.distributed.distributed_c10d - INFO - Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:2023-09-22 19:42:10,354 - torch.distributed.distributed_c10d - INFO - Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:2023-09-22 19:42:10,354 - torch.distributed.distributed_c10d - INFO - Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:2023-09-22 19:42:10,354 - torch.distributed.distributed_c10d - INFO - Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:2023-09-22 19:42:10,355 - torch.distributed.distributed_c10d - INFO - Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:torch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:torch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:torch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:torch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:torch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:torch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:torch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:torch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:torch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:torch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:torch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:torch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:torch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:torch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:torch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:torch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:START TIME: 1695411730.3582966\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:START TIME: 1695411730.3583176[1,mpirank:1,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:START TIME: 1695411730.3584523\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:START TIME: 1695411730.3585289[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:START TIME: 1695411730.3585567\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:START TIME: [1,mpirank:6,algo-1]<stdout>:1695411730.35876\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:START TIME: [1,mpirank:7,algo-1]<stdout>:1695411730.3588521\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:START TIME: 1695411730.359347\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  0%|          | 0/13 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 19:42:10.656: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.34.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 19:42:10.657: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.34.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 19:42:10.657: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.34.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 19:42:10.657: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.34.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 19:42:10.657: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.34.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 19:42:10.657: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.34.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:2023-09-22 19:42:10,662 - root - INFO - Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:2023-09-22 19:42:10,663 - root - INFO - Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:2023-09-22 19:42:10,663 - root - INFO - Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:2023-09-22 19:42:10,663 - root - INFO - Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:2023-09-22 19:42:10,663 - root - INFO - Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:2023-09-22 19:42:10,664 - root - INFO - Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 19:42:10.666: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.34.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 19:42:10.666: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.34.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:2023-09-22 19:42:10,672 - root - INFO - Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:2023-09-22 19:42:10,673 - root - INFO - Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 19:42:10.697 algo-1:133 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 19:42:10.699 algo-1:146 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 19:42:10.700 algo-1:145 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 19:42:10.700 algo-1:130 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 19:42:10.700 algo-1:139 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 19:42:10.700 algo-1:143 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 19:42:10.709 algo-1:178 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 19:42:10.712 algo-1:141 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 19:42:10.741 algo-1:133 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 19:42:10.742 algo-1:133 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 19:42:10.742 algo-1:133 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 19:42:10.743 algo-1:146 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 19:42:10.743 algo-1:133 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 19:42:10.743 algo-1:145 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 19:42:10.743 algo-1:133 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 19:42:10.743 algo-1:130 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 19:42:10.743 algo-1:139 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 19:42:10.743 algo-1:146 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 19:42:10.743 algo-1:145 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 19:42:10.743 algo-1:130 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 19:42:10.744 algo-1:139 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 19:42:10.744 algo-1:146 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 19:42:10.744 algo-1:130 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 19:42:10.744 algo-1:139 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 19:42:10.744 algo-1:145 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 19:42:10.744 algo-1:143 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 19:42:10.744 algo-1:139 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 19:42:10.744 algo-1:130 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 19:42:10.745 algo-1:145 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 19:42:10.745 algo-1:130 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 19:42:10.745 algo-1:139 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 19:42:10.745 algo-1:146 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 19:42:10.745 algo-1:145 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 19:42:10.745 algo-1:146 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 19:42:10.745 algo-1:143 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 19:42:10.745 algo-1:143 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 19:42:10.746 algo-1:143 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 19:42:10.746 algo-1:143 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 19:42:10.753 algo-1:178 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 19:42:10.754 algo-1:178 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 19:42:10.754 algo-1:178 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 19:42:10.755 algo-1:178 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 19:42:10.755 algo-1:178 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 19:42:10.759 algo-1:141 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 19:42:10.760 algo-1:141 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 19:42:10.760 algo-1:141 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 19:42:10.761 algo-1:141 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 19:42:10.761 algo-1:141 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    return _run_code(code, main_globals, None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 87, in _run_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/mpi4py/__main__.py\", line 7, in <module>\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    main()\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/mpi4py/run.py\", line 198, in main\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    run_command_line(args)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/mpi4py/run.py\", line 47, in run_command_line\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    run_path(sys.argv[0], run_name='__main__')\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 288, in run_path\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    return _run_module_code(code, init_globals, run_name,\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 97, in _run_module_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    _run_code(code, mod_globals, init_globals,\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 87, in _run_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"train-QLORA.py\", line 204, in <module>\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    trainer.train()\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 1589, in train\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    return inner_training_loop(\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 1890, in _inner_training_loop\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    tr_loss_step = self.training_step(model, inputs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 2774, in training_step\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    loss = self.compute_loss(model, inputs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 2799, in compute_loss\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    outputs = model(**inputs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1040, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    output = self._run_ddp_forward(*inputs, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1000, in _run_ddp_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    return module_to_run(*inputs[0], **kwargs[0])\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/peft/peft_model.py\", line 736, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    return self.base_model(\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    output = old_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 789, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    distilbert_output = self.distilbert(\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    output = old_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 609, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    return self.transformer(\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    output = old_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 368, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    layer_outputs = torch.utils.checkpoint.checkpoint(\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/utils/checkpoint.py\", line 249, in checkpoint\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    return CheckpointFunction.apply(function, preserve, *args)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/utils/checkpoint.py\", line 107, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    outputs = run_function(*args)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 364, in custom_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    return module(*inputs, output_attentions)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    output = old_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 295, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    sa_output = self.attention(\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    output = old_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 227, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    weights = self.dropout(weights)  # (bs, n_heads, q_length, k_length)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/dropout.py\", line 59, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    return F.dropout(input, self.p, self.training, self.inplace)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/functional.py\", line 1252, in dropout\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.00 GiB (GPU 2; 15.77 GiB total capacity; 9.97 GiB already allocated; 2.54 GiB free; 12.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mMPI_ABORT was invoked on rank 2 in communicator MPI COMMUNICATOR 4 DUP FROM 0\u001b[0m\n",
      "\u001b[34mwith errorcode 1.\u001b[0m\n",
      "\u001b[34mNOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.\u001b[0m\n",
      "\u001b[34mYou may or may not see output from other processes, depending on\u001b[0m\n",
      "\u001b[34mexactly when Open MPI kills them.\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    return _run_code(code, main_globals, None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 87, in _run_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/mpi4py/__main__.py\", line 7, in <module>\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    main()\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/mpi4py/run.py\", line 198, in main\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    run_command_line(args)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/mpi4py/run.py\", line 47, in run_command_line\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    run_path(sys.argv[0], run_name='__main__')\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 288, in run_path\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    return _run_module_code(code, init_globals, run_name,\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 97, in _run_module_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    _run_code(code, mod_globals, init_globals,\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 87, in _run_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"train-QLORA.py\", line 204, in <module>\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    trainer.train()\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 1589, in train\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    return inner_training_loop(\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 1890, in _inner_training_loop\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    tr_loss_step = self.training_step(model, inputs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 2774, in training_step\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    loss = self.compute_loss(model, inputs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 2799, in compute_loss\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    outputs = model(**inputs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1040, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    output = self._run_ddp_forward(*inputs, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1000, in _run_ddp_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    return module_to_run(*inputs[0], **kwargs[0])\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/peft/peft_model.py\", line 736, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    return self.base_model(\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    output = old_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 789, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    distilbert_output = self.distilbert(\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    output = old_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 609, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    return self.transformer(\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    output = old_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 368, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    layer_outputs = torch.utils.checkpoint.checkpoint(\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/utils/checkpoint.py\", line 249, in checkpoint\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    return CheckpointFunction.apply(function, preserve, *args)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/utils/checkpoint.py\", line 107, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    outputs = run_function(*args)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 364, in custom_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    return module(*inputs, output_attentions)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  F[1,mpirank:4,algo-1]<stdout>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:    return _run_code(code, main_globals, None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 87, in _run_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/mpi4py/__main__.py\", line 7, in <module>\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:    main()\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/mpi4py/run.py\", line 198, in main\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:    run_command_line(args)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/mpi4py/run.py\", line 47, in run_command_line\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:    run_path(sys.argv[0], run_name='__main__')\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 288, in run_path\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:    return _run_module_code(code, init_globals, run_name,\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 97, in _run_module_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:    _run_code(code, mod_globals, init_globals,\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 87, in _run_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  File \"train-QLORA.py\", line 204, in <module>\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:    trainer.train()\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 1589, in train\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:    return inner_training_loop(\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 1890, in _inner_training_loop\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:    tr_loss_step = self.training_step(model, inputs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 2774, in training_step\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:    loss = self.compute_loss(model, inputs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 2799, in compute_loss\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:    outputs = model(**inputs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1040, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:    output = self._run_ddp_forward(*inputs, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1000, in _run_ddp_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:    return module_to_run(*inputs[0], **kwargs[0])\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/peft/peft_model.py\", line 736, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:    return self.base_model(\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:    output = old_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 789, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:    distilbert_output = self.distilbert(\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:    output = old_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 609, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:    return self.transformer(\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:    output = old_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 368, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:    layer_outputs = torch.utils.checkpoint.checkpoint(\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/utils/checkpoint.py\", line 249, in checkpoint\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:    return CheckpointFunction.apply(function, preserve, *args)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/utils/checkpoint.py\", line 107, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:    outputs = run_function(*args)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 364, in custom_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:    return module(*inputs, output_attentions)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  F[1,mpirank:5,algo-1]<stdout>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    return _run_code(code, main_globals, None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 87, in _run_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/mpi4py/__main__.py\", line 7, in <module>\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    main()\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/mpi4py/run.py\", line 198, in main\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    run_command_line(args)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/mpi4py/run.py\", line 47, in run_command_line\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    run_path(sys.argv[0], run_name='__main__')\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 288, in run_path\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    return _run_module_code(code, init_globals, run_name,\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 97, in _run_module_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    _run_code(code, mod_globals, init_globals,\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 87, in _run_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"train-QLORA.py\", line 204, in <module>\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    trainer.train()\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 1589, in train\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    return inner_training_loop(\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 1890, in _inner_training_loop\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    tr_loss_step = self.training_step(model, inputs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 2774, in training_step\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    loss = self.compute_loss(model, inputs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 2799, in compute_loss\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    outputs = model(**inputs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1040, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    output = self._run_ddp_forward(*inputs, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1000, in _run_ddp_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    return module_to_run(*inputs[0], **kwargs[0])\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/peft/peft_model.py\", line 736, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    return self.base_model(\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    output = old_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 789, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    distilbert_output = self.distilbert(\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    output = old_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 609, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    return self.transformer(\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    output = old_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 368, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    layer_outputs = torch.utils.checkpoint.checkpoint(\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/utils/checkpoint.py\", line 249, in checkpoint\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    return CheckpointFunction.apply(function, preserve, *args)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/utils/checkpoint.py\", line 107, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    outputs = run_function(*args)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 364, in custom_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    return module(*inputs, output_attentions)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  F[1,mpirank:6,algo-1]<stdout>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    return _run_code(code, main_globals, None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 87, in _run_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/mpi4py/__main__.py\", line 7, in <module>\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    main()\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/mpi4py/run.py\", line 198, in main\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    run_command_line(args)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/mpi4py/run.py\", line 47, in run_command_line\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    run_path(sys.argv[0], run_name='__main__')\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 288, in run_path\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    return _run_module_code(code, init_globals, run_name,\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 97, in _run_module_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    _run_code(code, mod_globals, init_globals,\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 87, in _run_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"train-QLORA.py\", line 204, in <module>\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    trainer.train()\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 1589, in train\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    return inner_training_loop(\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 1890, in _inner_training_loop\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    tr_loss_step = self.training_step(model, inputs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 2774, in training_step\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    loss = self.compute_loss(model, inputs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 2799, in compute_loss\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    outputs = model(**inputs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1040, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    output = self._run_ddp_forward(*inputs, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1000, in _run_ddp_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    return module_to_run(*inputs[0], **kwargs[0])\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/peft/peft_model.py\", line 736, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    return self.base_model(\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    output = old_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 789, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    distilbert_output = self.distilbert(\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    output = old_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 609, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    return self.transformer(\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    output = old_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 368, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    layer_outputs = torch.utils.checkpoint.checkpoint(\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/utils/checkpoint.py\", line 249, in checkpoint\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    return CheckpointFunction.apply(function, preserve, *args)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/utils/checkpoint.py\", line 107, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    outputs = run_function(*args)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 364, in custom_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    return module(*inputs, output_attentions)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  F[1,mpirank:3,algo-1]<stdout>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    return _run_code(code, main_globals, None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 87, in _run_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/mpi4py/__main__.py\", line 7, in <module>\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    main()\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/mpi4py/run.py\", line 198, in main\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    run_command_line(args)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/mpi4py/run.py\", line 47, in run_command_line\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    run_path(sys.argv[0], run_name='__main__')\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 288, in run_path\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    return _run_module_code(code, init_globals, run_name,\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 97, in _run_module_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    _run_code(code, mod_globals, init_globals,\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 87, in _run_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"train-QLORA.py\", line 204, in <module>\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    trainer.train()\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 1589, in train\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    return inner_training_loop(\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 1890, in _inner_training_loop\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    tr_loss_step = self.training_step(model, inputs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 2774, in training_step\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    loss = self.compute_loss(model, inputs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 2799, in compute_loss\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    outputs = model(**inputs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1040, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    output = self._run_ddp_forward(*inputs, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1000, in _run_ddp_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    return module_to_run(*inputs[0], **kwargs[0])\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/peft/peft_model.py\", line 736, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    return self.base_model(\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    output = old_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 789, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    distilbert_output = self.distilbert(\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    output = old_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 609, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    return self.transformer(\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    output = old_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 368, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    layer_outputs = torch.utils.checkpoint.checkpoint(\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/utils/checkpoint.py\", line 249, in checkpoint\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    return CheckpointFunction.apply(function, preserve, *args)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/utils/checkpoint.py\", line 107, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    outputs = run_function(*args)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 364, in custom_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    return module(*inputs, output_attentions)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  F[1,mpirank:0,algo-1]<stdout>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    return _run_code(code, main_globals, None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 87, in _run_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/mpi4py/__main__.py\", line 7, in <module>\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    main()\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/mpi4py/run.py\", line 198, in main\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    run_command_line(args)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/mpi4py/run.py\", line 47, in run_command_line\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    run_path(sys.argv[0], run_name='__main__')\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 288, in run_path\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    return _run_module_code(code, init_globals, run_name,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 97, in _run_module_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    _run_code(code, mod_globals, init_globals,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 87, in _run_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"train-QLORA.py\", line 204, in <module>\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    trainer.train()\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 1589, in train\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    return inner_training_loop(\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 1890, in _inner_training_loop\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    tr_loss_step = self.training_step(model, inputs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 2774, in training_step\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    loss = self.compute_loss(model, inputs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 2799, in compute_loss\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    outputs = model(**inputs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1040, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    output = self._run_ddp_forward(*inputs, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1000, in _run_ddp_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    return module_to_run(*inputs[0], **kwargs[0])\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/peft/peft_model.py\", line 736, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    return self.base_model(\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    output = old_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 789, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    distilbert_output = self.distilbert(\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    output = old_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 609, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    return self.transformer(\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    output = old_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 368, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    layer_outputs = torch.utils.checkpoint.checkpoint(\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/utils/checkpoint.py\", line 249, in checkpoint\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    return CheckpointFunction.apply(function, preserve, *args)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/utils/checkpoint.py\", line 107, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    outputs = run_function(*args)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 364, in custom_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    return module(*inputs, output_attentions)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  F[1,mpirank:7,algo-1]<stdout>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    return _run_code(code, main_globals, None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 87, in _run_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/mpi4py/__main__.py\", line 7, in <module>\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    main()\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/mpi4py/run.py\", line 198, in main\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    run_command_line(args)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/mpi4py/run.py\", line 47, in run_command_line\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    run_path(sys.argv[0], run_name='__main__')\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 288, in run_path\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    return _run_module_code(code, init_globals, run_name,\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 97, in _run_module_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    _run_code(code, mod_globals, init_globals,\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 87, in _run_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"train-QLORA.py\", line 204, in <module>\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    trainer.train()\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 1589, in train\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    return inner_training_loop(\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 1890, in _inner_training_loop\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    tr_loss_step = self.training_step(model, inputs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 2774, in training_step\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    loss = self.compute_loss(model, inputs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 2799, in compute_loss\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    outputs = model(**inputs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1040, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    output = self._run_ddp_forward(*inputs, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1000, in _run_ddp_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    return module_to_run(*inputs[0], **kwargs[0])\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/peft/peft_model.py\", line 736, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    return self.base_model(\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    output = old_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 789, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    distilbert_output = self.distilbert(\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    output = old_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 609, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    return self.transformer(\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    output = old_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 368, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    layer_outputs = torch.utils.checkpoint.checkpoint(\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/utils/checkpoint.py\", line 249, in checkpoint\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    return CheckpointFunction.apply(function, preserve, *args)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/utils/checkpoint.py\", line 107, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    outputs = run_function(*args)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 364, in custom_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    return module(*inputs, output_attentions)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  F[1,mpirank:7,algo-1]<stdout>:ile \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    output = old_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 295, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    sa_output = self.attention(\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    output = old_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 227, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    weights = self.dropout(weights)  # (bs, n_heads, q_length, k_length)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/dropout.py\", line 59, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    return F.dropout(input, self.p, self.training, self.inplace)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/functional.py\", line 1252, in dropout\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.00 GiB (GPU 7; 15.77 GiB total capacity; 9.97 GiB already allocated; 2.52 GiB free; 12.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:ile \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    output = old_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 295, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    sa_output = self.attention(\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    output = old_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 227, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    weights = self.dropout(weights)  # (bs, n_heads, q_length, k_length)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/dropout.py\", line 59, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    return F.dropout(input, self.p, self.training, self.inplace)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/functional.py\", line 1252, in dropout\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.00 GiB (GPU 1; 15.77 GiB total capacity; 9.97 GiB already allocated; 2.45 GiB free; 12.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:ile \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:    output = old_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 295, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:    sa_output = self.attention(\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:    output = old_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 227, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:    weights = self.dropout(weights)  # (bs, n_heads, q_length, k_length)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/dropout.py\", line 59, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:    return F.dropout(input, self.p, self.training, self.inplace)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/functional.py\", line 1252, in dropout\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.00 GiB (GPU 4; 15.77 GiB total capacity; 9.97 GiB already allocated; 2.47 GiB free; 12.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:ile \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    output = old_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 295, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    sa_output = self.attention(\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    output = old_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 227, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    weights = self.dropout(weights)  # (bs, n_heads, q_length, k_length)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/dropout.py\", line 59, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    return F.dropout(input, self.p, self.training, self.inplace)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/functional.py\", line 1252, in dropout\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.00 GiB (GPU 5; 15.77 GiB total capacity; 9.97 GiB already allocated; 2.45 GiB free; 12.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:ile \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    output = old_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 295, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    sa_output = self.attention(\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    output = old_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 227, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    weights = self.dropout(weights)  # (bs, n_heads, q_length, k_length)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/dropout.py\", line 59, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    return F.dropout(input, self.p, self.training, self.inplace)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/functional.py\", line 1252, in dropout\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.00 GiB (GPU 6; 15.77 GiB total capacity; 9.97 GiB already allocated; 2.50 GiB free; 12.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:ile \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    output = old_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 295, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    sa_output = self.attention(\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    output = old_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 227, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    weights = self.dropout(weights)  # (bs, n_heads, q_length, k_length)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/dropout.py\", line 59, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    return F.dropout(input, self.p, self.training, self.inplace)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/functional.py\", line 1252, in dropout\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.00 GiB (GPU 3; 15.77 GiB total capacity; 9.97 GiB already allocated; 2.54 GiB free; 12.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:ile \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    output = old_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 295, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    sa_output = self.attention(\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    output = old_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 227, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    weights = self.dropout(weights)  # (bs, n_heads, q_length, k_length)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/dropout.py\", line 59, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    return F.dropout(input, self.p, self.training, self.inplace)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/functional.py\", line 1252, in dropout\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.00 GiB (GPU 0; 15.77 GiB total capacity; 9.97 GiB already allocated; 2.47 GiB free; 12.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\u001b[0m\n",
      "\u001b[34m[algo-1:00117] 7 more processes have sent help message help-mpi-api.txt / mpi-abort\u001b[0m\n",
      "\u001b[34m[algo-1:00117] Set MCA parameter \"orte_base_help_aggregate\" to 0 to see all help / error messages\u001b[0m\n",
      "\u001b[34m2023-09-22 19:42:12,491 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-09-22 19:42:12,491 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 1 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-09-22 19:42:12,493 sagemaker-training-toolkit ERROR    Reporting training FAILURE\u001b[0m\n",
      "\u001b[34m2023-09-22 19:42:12,493 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mExitCode 1\u001b[0m\n",
      "\u001b[34mErrorMessage \"torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.00 GiB (GPU 2; 15.77 GiB total capacity; 9.97 GiB already allocated; 2.54 GiB free; 12.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      " Traceback (most recent call last)\n",
      " File \"/opt/conda/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      " return _run_code(code, main_globals, None,\n",
      " File \"/opt/conda/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      " exec(code, run_globals)\n",
      " File \"/opt/conda/lib/python3.9/site-packages/mpi4py/__main__.py\", line 7, in <module>\n",
      " main()\n",
      " File \"/opt/conda/lib/python3.9/site-packages/mpi4py/run.py\", line 198, in main\n",
      " run_command_line(args)\n",
      " File \"/opt/conda/lib/python3.9/site-packages/mpi4py/run.py\", line 47, in run_command_line\n",
      " run_path(sys.argv[0], run_name='__main__')\n",
      " File \"/opt/conda/lib/python3.9/runpy.py\", line 288, in run_path\n",
      " return _run_module_code(code, init_globals, run_name,\n",
      " File \"/opt/conda/lib/python3.9/runpy.py\", line 97, in _run_module_code\n",
      " _run_code(code, mod_globals, init_globals,\n",
      " File \"train-QLORA.py\", line 204, in <module>\n",
      " trainer.train()\n",
      " File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 1589, in train\n",
      " return inner_training_loop(\n",
      " File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 1890, in _inner_training_loop\n",
      " tr_loss_step = self.training_step(model, inputs)\n",
      " File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 2774, in training_step\n",
      " loss = self.compute_loss(model, inputs)\n",
      " File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 2799, in compute_loss\n",
      " outputs = model(**inputs)\n",
      " File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      " return forward_call(*input, **kwargs)\n",
      " File \"/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1040, in forward\n",
      " output = self._run_ddp_forward(*inputs, **kwargs)\n",
      " File \"/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1000, in _run_ddp_forward\n",
      " return module_to_run(*inputs[0], **kwargs[0])\n",
      " File \"/opt/conda/lib/python3.9/site-packages/peft/peft_model.py\", line 736, in forward\n",
      " return self.base_model(\n",
      " File \"/opt/conda/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
      " output = old_forward(*args, **kwargs)\n",
      " File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 789, in forward\n",
      " distilbert_output = self.distilbert(\n",
      " File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 609, in forward\n",
      " return self.transformer(\n",
      " File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 368, in forward\n",
      " layer_outputs = torch.utils.checkpoint.checkpoint(\n",
      " File \"/opt/conda/lib/python3.9/site-packages/torch/utils/checkpoint.py\", line 249, in checkpoint\n",
      " return CheckpointFunction.apply(function, preserve, *args)\n",
      " File \"/opt/conda/lib/python3.9/site-packages/torch/utils/checkpoint.py\", line 107, in forward\n",
      " outputs = run_function(*args)\n",
      " File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 364, in custom_forward\n",
      " return module(*inputs, output_attentions)\n",
      " F:Traceback (most recent call last)\n",
      " F:ile \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      " File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 295, in forward\n",
      " sa_output = self.attention(\n",
      " File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 227, in forward\n",
      " weights = self.dropout(weights)  # (bs, n_heads, q_length, k_length)\n",
      " File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/dropout.py\", line 59, in forward\n",
      " return F.dropout(input, self.p, self.training, self.inplace)\n",
      " File \"/opt/conda/lib/python3.9/site-packages/torch/nn/functional.py\", line 1252, in dropout\n",
      " return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\n",
      " torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.00 GiB (GPU 7; 15.77 GiB total capacity; 9.97 GiB already allocated; 2.52 GiB free; 12.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      " ile \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      " torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.00 GiB (GPU 1; 15.77 GiB total capacity; 9.97 GiB already allocated; 2.45 GiB free; 12.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      " torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.00 GiB (GPU 4; 15.77 GiB total capacity; 9.97 GiB already allocated; 2.47 GiB free; 12.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      " torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.00 GiB (GPU 5; 15.77 GiB total capacity; 9.97 GiB already allocated; 2.45 GiB free; 12.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      " torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.00 GiB (GPU 6; 15.77 GiB total capacity; 9.97 GiB already allocated; 2.50 GiB free; 12.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      " torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.00 GiB (GPU 3; 15.77 GiB total capacity; 9.97 GiB already allocated; 2.54 GiB free; 12.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      " torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.00 GiB (GPU 0; 15.77 GiB total capacity; 9.97 GiB already allocated; 2.47 GiB free; 12.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      " ERROR: Exception\n",
      " File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/cli/base_command.py\", line 160, in exc_logging_wrapper\n",
      " status = run_func(*args)\n",
      " File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/cli/req_command.py\", line 247, in wrapper\n",
      " return func(self, options, args)\n",
      " File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/commands/install.py\", line 503, in run\n",
      " installed = install_given_reqs(\n",
      " File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/req/__init__.py\", line 73, in install_given_reqs\n",
      " requirement.install(\n",
      " File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/req/req_install.py\", line 796, in install\n",
      " install_wheel(\n",
      " File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/operations/install/wheel.py\", line 729, in install_wheel\n",
      " _install_wheel(\n",
      " File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/operations/install/wheel.py\", line 618, in _install_wheel\n",
      " assert os.path.exists(pyc_path)\n",
      " AssertionError\n",
      " WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      " \n",
      " [notice] A new release of pip is available: 23.0 -> 23.2.1\n",
      " [notice] To update, run: pip install --upgrade pip\n",
      " ERROR: Wheel 'rouge-score' located at /root/.cache/pip/wheels/9b/3d/39/09558097d3119ca0a4d462df68f22c6f3c1b345ac63a09b86e/rouge_score-0.1.2-py3-none-any.whl is invalid.\n",
      " WARNING: Error parsing requirements for tokenizers: [Errno 2] No such file or directory: '/opt/conda/lib/python3.9/site-packages/tokenizers-0.13.2.dist-info/METADATA'\n",
      " WARNING: Error parsing requirements for huggingface-hub: [Errno 2] No such file or directory: '/opt/conda/lib/python3.9/site-packages/huggingface_hub-0.12.0.dist-info/METADATA'\n",
      " WARNING: No metadata found in /opt/conda/lib/python3.9/site-packages\n",
      " ERROR: Can't roll back huggingface-hub; was not uninstalled\n",
      " ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/opt/conda/lib/python3.9/site-packages/huggingface_hub/_snapshot_download.py'\n",
      " ERROR: Can't roll back transformers; was not uninstalled\n",
      " ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/opt/conda/lib/python3.9/site-packages/transformers/trainer_seq2seq.py'\n",
      " WARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\n",
      " WARNING: Error parsing requirements for accelerate: [Errno 2] No such file or directory: '/opt/conda/lib/python3.9/site-packages/accelerate-0.16.0.dist-info/METADATA'\n",
      " sh: 1: cannot open 0.14.0: No such file\n",
      " #015Downloading (…)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]\n",
      " #015Downloading (…)lve/main/config.json: 100%|██████████| 483/483 [00:00<00:00, 39.6kB/s]\n",
      " #015Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]\n",
      " #015Downloading model.safetensors:   8%|▊         | 21.0M/268M [00:00<00:01, 154MB/s]\n",
      " #015Downloading model.safetensors:  20%|█▉        | 52.4M/268M [00:00<00:01, 214MB/s]\n",
      " #015Downloading model.safetensors:  39%|███▉      | 105M/268M [00:00<00:00, 319MB/s]\n",
      " #015Downloading model.safetensors:  59%|█████▊    | 157M/268M [00:00<00:00, 376MB/s]\n",
      " #015Downloading model.safetensors:  78%|███████▊  | 210M/268M [00:00<00:00, 422MB/s]\n",
      " #015Downloading model.safetensors:  98%|█████████▊| 262M/268M [00:00<00:00, 387MB/s]\n",
      " #015Downloading model.safetensors: 100%|██████████| 268M/268M [00:00<00:00, 354MB/s]\n",
      " Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n",
      " You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'classifier.bias', 'pre_classifier.weight']\n",
      " #015Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]\n",
      " #015Downloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 4.09kB/s]\n",
      " #015Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]\n",
      " #015Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 53.4MB/s]\n",
      " #015Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]\n",
      " #015Downloading (…)/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 39.9MB/s]\n",
      " Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
      " Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n",
      " Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      " Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']\n",
      " Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
      " torch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\n",
      " #015  0%|          | 0/13 [00:00<?, ?it/s]\n",
      " You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " --------------------------------------------------------------------------\n",
      " MPI_ABORT was invoked on rank 2 in communicator MPI COMMUNICATOR 4 DUP FROM 0\n",
      " with errorcode 1.\n",
      " NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.\n",
      " You may or may not see output from other processes, depending on\n",
      " exactly when Open MPI kills them.\n",
      " [algo-1:00117] 7 more processes have sent help message help-mpi-api.txt / mpi-abort\n",
      " [algo-1:00117] Set MCA parameter \"orte_base_help_aggregate\" to 0 to see all help / error messages\"\u001b[0m\n",
      "\u001b[34mCommand \"mpirun --host algo-1 -np 8 --allow-run-as-root --tag-output --oversubscribe -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -mca plm_rsh_num_concurrent 1 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x SMDATAPARALLEL_USE_SINGLENODE=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1 -x LD_PRELOAD=/opt/conda/lib/python3.9/site-packages/gethostname.cpython-39-x86_64-linux-gnu.so -verbose -x NCCL_DEBUG=VERSION -x USE_SMDDP_COLLECTIVES=0 smddprun /opt/conda/bin/python3.9 -m mpi4py train-QLORA.py --epochs 1 --eval_batch_size 256 --learning_rate 0.0128 --model_name distilbert-base-uncased --train_batch_size 256\"\u001b[0m\n",
      "\u001b[34m2023-09-22 19:42:12,493 sagemaker-training-toolkit ERROR    Encountered exit_code 1\u001b[0m\n",
      "\n",
      "2023-09-22 19:42:31 Uploading - Uploading generated training model\n",
      "2023-09-22 19:42:31 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job huggingface-pytorch-training-2023-09-22-19-33-44-463: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.00 GiB (GPU 2; 15.77 GiB total capacity; 9.97 GiB already allocated; 2.54 GiB free; 12.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n Traceback (most recent call last)\n File \"/opt/conda/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n return _run_code(code, main_globals, None,\n File \"/opt/conda/lib/python3.9/runpy.py\", line 87, in _run_code\n exec(code, run_globals)\n File \"/opt/conda/lib/python3.9/site-packages/mpi4py/__main__.py\", line 7, in <module>\n main()\n File \"/opt/conda/lib/python3.9/site-packages/mpi4py/run.py\", line 198, in main\n run_command_line(args)\n File \"/opt/conda/lib/python3.9/site-packages/mpi4py/run.py\", line 47, in run_command_line\n run_path(sys.argv[0], run_name='__main__')\n File \"/opt/conda/lib/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-f16141f227f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m# starting the train job with our uploaded datasets as input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mhuggingface_estimator_DDP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtraining_input_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_input_path\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/workflow/pipeline_context.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_StepArguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretrieve_caller_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_instance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrun_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1310\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   2578\u001b[0m         \u001b[0;31m# If logs are requested, call logs_for_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2579\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2580\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2581\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   4847\u001b[0m             \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnexpectedStatusException\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mwaiting\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mjob\u001b[0m \u001b[0mfails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4848\u001b[0m         \"\"\"\n\u001b[0;32m-> 4849\u001b[0;31m         \u001b[0m_logs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboto_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4851\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlogs_for_processing_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoll\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_logs_for_job\u001b[0;34m(boto_session, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   6758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6759\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6760\u001b[0;31m         \u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TrainingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6761\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6762\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   6814\u001b[0m             \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6815\u001b[0m             \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6816\u001b[0;31m             \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6817\u001b[0m         )\n\u001b[1;32m   6818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job huggingface-pytorch-training-2023-09-22-19-33-44-463: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.00 GiB (GPU 2; 15.77 GiB total capacity; 9.97 GiB already allocated; 2.54 GiB free; 12.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n Traceback (most recent call last)\n File \"/opt/conda/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n return _run_code(code, main_globals, None,\n File \"/opt/conda/lib/python3.9/runpy.py\", line 87, in _run_code\n exec(code, run_globals)\n File \"/opt/conda/lib/python3.9/site-packages/mpi4py/__main__.py\", line 7, in <module>\n main()\n File \"/opt/conda/lib/python3.9/site-packages/mpi4py/run.py\", line 198, in main\n run_command_line(args)\n File \"/opt/conda/lib/python3.9/site-packages/mpi4py/run.py\", line 47, in run_command_line\n run_path(sys.argv[0], run_name='__main__')\n File \"/opt/conda/lib/"
     ]
    }
   ],
   "source": [
    "#####################################################################\n",
    "######################## DATA DISTRIBUTED ###########################\n",
    "#####################################################################\n",
    "\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "\n",
    "instance_type = 'ml.p3.16xlarge' #'ml.p3.16xlarge', 'ml.g5.12xlarge'\n",
    "partitions = 2\n",
    "Adj_BatchSize = 4\n",
    "LR_Adj_LoRA = 8\n",
    "\n",
    "if instance_type == 'ml.g5.12xlarge': #ValueError: Provided instance_type ml.g5.12xlarge is not supported by smdataparallel.\n",
    "    num_GPUs = 4\n",
    "elif instance_type == 'ml.p3.16xlarge':\n",
    "    num_GPUs = 8\n",
    "\n",
    "if dataset_name == 'imdb':\n",
    "    hyperparameters={'epochs': 1,\n",
    "                     'train_batch_size': 32 * Adj_BatchSize, # 32\n",
    "                     'model_name':'distilbert-base-uncased',\n",
    "                     \"eval_batch_size\": 256, # InExample: 64\n",
    "                     \"learning_rate\": 5e-5 * (num_GPUs/partitions) * Adj_BatchSize * LR_Adj_LoRA # LR for DATA PARALLEL training is LR_Single_GPU * #OfGPUs * LoRA Multiplier!!!\n",
    "                    }\n",
    "\n",
    "# configuration for running training on smdistributed data parallel\n",
    "distribution = {'smdistributed':{'dataparallel':{'enabled': True, #}}}\n",
    "                                                 \"custom_mpi_options\": \"-verbose -x NCCL_DEBUG=VERSION\", #}}}\n",
    "                                                 \"parameters\": {\"ddp\": True}}}}\n",
    "\n",
    "#entry_point = 'train.py' # NO LoRA!\n",
    "#entry_point = 'train-LORA.py'\n",
    "entry_point = 'train-QLORA.py'\n",
    "\n",
    "huggingface_estimator_DDP = HuggingFace(entry_point=entry_point,\n",
    "                            source_dir='./',\n",
    "                            instance_type=instance_type, #'ml.p3.16xlarge', 'ml.g5.12xlarge'\n",
    "                            instance_count=1,\n",
    "                            role=role,\n",
    "                            transformers_version='4.26.0', # InExample: '4.26' with Error about the name\n",
    "                            pytorch_version='1.13.1', # InExample: '1.13' with Error about the version\n",
    "                            py_version='py39',\n",
    "                            hyperparameters = hyperparameters,\n",
    "                            distribution = distribution)\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator_DDP.fit({'train': training_input_path, 'test': test_input_path})\n",
    "\n",
    "\n",
    "\n",
    "### INSTANCE_TYPE: ml.p3.16xlarge -- ALL MODELs as G5 Instances are not supporte by Distirbuted SageMaker!!!\n",
    "\n",
    "\n",
    "\n",
    "### NO LORA -- BENCHMARK\n",
    "\n",
    "## 8 GPU ANALYSIS, LR_8_GPU == LR_Single_GPU -- LOWER FIT MEASURES!!!\n",
    "\n",
    "# EXEC TIME: 56.322752952575684 - TRAIN & FIRST EVAL -- almost linear speedup!!!\n",
    "\n",
    "# eval_accuracy': 0.897, \n",
    "# 'eval_f1': 0.8962321176707637, \n",
    "# 'eval_precision': 0.9060908535343247, \n",
    "# 'eval_recall': 0.8865856089296392\n",
    "\n",
    "## 8 GPU ANALYSIS, LR_8_GPU == LR_Single_GPU * 8 -- HIGHER FIT MEASURES!!! but still not as high as on single GPU -- TBC...\n",
    "\n",
    "# EXEC TIME: 56.322752952575684 - TRAIN & FIRST EVAL -- almost linear speedup!!!\n",
    "\n",
    "# 'eval_accuracy': 0.917, \n",
    "# 'eval_f1': 0.9147493837304849, \n",
    "# 'eval_precision': 0.9436321254503073, # HIGHER THEN on SINGLE_GPU!!!\n",
    "# 'eval_recall': 0.8875822204504684\n",
    "\n",
    "## 8 GPU ANALYSIS, LR_8_GPU == LR_Single_GPU * 16 -- training breaks down!!!\n",
    "\n",
    "# EXEC TIME: 56.03306198120117\n",
    "\n",
    "# 'eval_loss': 0.5477789044380188, \n",
    "# 'eval_accuracy': 0.7475, \n",
    "# 'eval_f1': 0.6715233511122676, \n",
    "# 'eval_precision': 0.9634191862635312, \n",
    "# 'eval_recall': 0.5153753993610224,\n",
    "\n",
    "\n",
    "## IMPACT OF BATCH_SIZE (Standard BS = 32):\n",
    "\n",
    "# BS = 64 -- OOM Error!\n",
    "\n",
    "\n",
    "### WITH LORA\n",
    "\n",
    "## LR_8_GPU == LR_Single_GPU * 8\n",
    "\n",
    "# EXEC TIME: 44.2592236995697 -- comperable times for other runs\n",
    "\n",
    "# 'eval_accuracy': 0.688, \n",
    "# 'eval_f1': 0.6741854636591478, \n",
    "# 'eval_precision': 0.7080500109673173, \n",
    "# 'eval_recall': 0.6434123978473191,\n",
    "\n",
    "## LR_8_GPU == LR_Single_GPU * 2\n",
    "\n",
    "# 'eval_accuracy': 0.5162, \n",
    "# 'eval_f1': 0.11779722830051056, \n",
    "# 'eval_precision': 0.6916488222698073, \n",
    "# 'eval_recall': 0.06438110424556508,\n",
    "\n",
    "## LR_8_GPU == LR_Single_GPU * 8 * 2\n",
    "\n",
    "#'eval_accuracy': 0.8371, \n",
    "# 'eval_f1': 0.8364622025901014, \n",
    "# 'eval_precision': 0.8426375404530745, \n",
    "# 'eval_recall': 0.8303767191548734,\n",
    "\n",
    "## LR_8_GPU == LR_Single_GPU * 8 * 4\n",
    "\n",
    "# 'eval_accuracy': 0.8819, \n",
    "# 'eval_f1': 0.8771199667048174, \n",
    "# 'eval_precision': 0.9175010883761427, \n",
    "# 'eval_recall': 0.8401435120589994,\n",
    "\n",
    "## LR_8_GPU == LR_Single_GPU * 8 * 8 --- BEST AND MOST BALANCED!!!\n",
    "\n",
    "# 'eval_accuracy': 0.899, \n",
    "# 'eval_f1': 0.9017127286882055, \n",
    "# 'eval_precision': 0.8809659631108576, \n",
    "# 'eval_recall': 0.923460235200319,\n",
    "\n",
    "## LR_8_GPU == LR_Single_GPU * 8 * 16\n",
    "\n",
    "# 'eval_accuracy': 0.9001, \n",
    "# 'eval_f1': 0.896358543417367, \n",
    "# 'eval_precision': 0.9346603202077023, \n",
    "# 'eval_recall': 0.8610723539964122,\n",
    "\n",
    "\n",
    "\n",
    "### WITH QLORA\n",
    "\n",
    "## LR_8_GPU == LR_Single_GPU * 8 * 16  --- BEST: GOOD AND MOST BALANCED!!!\n",
    "\n",
    "# EXEC TIME: 36.523969411849976 -- comperable times for other runs\n",
    "\n",
    "# 'eval_loss': 0.24202285706996918, \n",
    "# 'eval_accuracy': 0.9067, \n",
    "# 'eval_f1': 0.9062028752387654, \n",
    "# 'eval_precision': 0.9079371474617244, \n",
    "# 'eval_recall': 0.9044752157334939\n",
    "\n",
    "### LR_8_GPU == LR_Single_GPU * 8 * 8\n",
    "\n",
    "# 'eval_loss': 0.25302860140800476, \n",
    "# 'eval_accuracy': 0.9028, \n",
    "# 'eval_f1': 0.9036860879904877, \n",
    "# 'eval_precision': 0.8925425719318849, \n",
    "# 'eval_recall': 0.9151113786875377,\n",
    "\n",
    "### LR_8_GPU == LR_Single_GPU * 8 * 32\n",
    "\n",
    "# 'eval_loss': 0.24021868407726288, \n",
    "# 'eval_accuracy': 0.9076, \n",
    "# 'eval_f1': 0.9105691056910569, \n",
    "# 'eval_precision': 0.8794167134043747, \n",
    "# 'eval_recall': 0.9440096327513546,\n",
    "\n",
    "### LR_8_GPU == LR_Single_GPU * 8 * 16  WITH GRADIENT CHECKPOINTING\n",
    "\n",
    "# EXEC TIME: 36.55616497993469 -- No increase in execution time !?!?!?\n",
    "\n",
    "# 'eval_loss': 0.24579724669456482, \n",
    "# 'eval_accuracy': 0.9045, \n",
    "# 'eval_f1': 0.9036229690180644, \n",
    "# 'eval_precision': 0.9088509947218839, \n",
    "# 'eval_recall': 0.8984547461368654,\n",
    "\n",
    "\n",
    "## IMPACT OF BATCH_SIZE (Standard BS = 32):\n",
    "\n",
    "# BS = 64 \n",
    "\n",
    "#[1,mpirank:0,algo-1]<stderr>:#015100%|██████████| 49/49 [00:31<00:00,  1.66it/s]\n",
    "\n",
    "# EXEC TIME: 35.3283314704895\n",
    "\n",
    "# 'eval_loss': 0.6055842041969299, \n",
    "# 'eval_accuracy': 0.809, \n",
    "# 'eval_f1': 0.829006266786034, \n",
    "# 'eval_precision': 0.7538261152718984, \n",
    "# 'eval_recall': 0.9208432776451869,\n",
    "\n",
    "\n",
    "# BS = 128\n",
    "\n",
    "# [1,mpirank:0,algo-1]<stderr>:#015100%|██████████| 25/25 [00:31<00:00,  1.18s/it]\n",
    "\n",
    "# EXEC TIME: 35.87735557556152\n",
    "\n",
    "# 'eval_loss': 0.674795389175415, \n",
    "# 'eval_accuracy': 0.7293, \n",
    "# 'eval_f1': 0.6782360632354689, \n",
    "# 'eval_precision': 0.8428360413589365, \n",
    "# 'eval_recall': 0.5674224343675418\n",
    "\n",
    "\n",
    "# BS = 256\n",
    "\n",
    "# OOM Error!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-pytorch-training-2023-09-22-20-27-32-127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-22 20:28:13 Starting - Starting the training job......\n",
      "2023-09-22 20:29:12 Starting - Preparing the instances for training.........\n",
      "2023-09-22 20:30:28 Downloading - Downloading input data...\n",
      "2023-09-22 20:30:53 Training - Downloading the training image..................\n",
      "2023-09-22 20:33:59 Training - Training image download completed. Training in progress....\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-09-22 20:34:31,064 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-09-22 20:34:31,126 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-22 20:34:31,139 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-09-22 20:34:31,142 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-09-22 20:34:37,782 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-22 20:34:37,859 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-22 20:34:37,873 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2023-09-22 20:34:37,873 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2023-09-22 20:34:37,875 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2023-09-22 20:34:37,875 sagemaker-training-toolkit INFO     Env Hosts: ['algo-1'] Hosts: ['algo-1:8'] process_per_hosts: 8 num_processes: 8\u001b[0m\n",
      "\u001b[34m2023-09-22 20:34:37,877 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[34m2023-09-22 20:34:37,940 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-22 20:34:38,016 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-22 20:34:38,030 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_distributed_dataparallel_enabled\": false,\n",
      "        \"sagemaker_instance_type\": \"ml.p3.16xlarge\",\n",
      "        \"sagemaker_mpi_custom_mpi_options\": \"\",\n",
      "        \"sagemaker_mpi_enabled\": true,\n",
      "        \"sagemaker_mpi_num_of_processes_per_host\": 8\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p3.16xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 1,\n",
      "        \"eval_batch_size\": 256,\n",
      "        \"learning_rate\": 0.0032,\n",
      "        \"model_name\": \"distilbert-base-uncased\",\n",
      "        \"mp_parameters\": {\n",
      "            \"microbatches\": 1,\n",
      "            \"placement_strategy\": \"spread\",\n",
      "            \"pipeline\": \"interleaved\",\n",
      "            \"optimize\": \"speed\",\n",
      "            \"partitions\": 2,\n",
      "            \"ddp\": true\n",
      "        },\n",
      "        \"train_batch_size\": 128\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p3.16xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": true,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2023-09-22-20-27-32-127\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-578864530451/huggingface-pytorch-training-2023-09-22-20-27-32-127/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train-LORA\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.16xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.16xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train-LORA.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":1,\"eval_batch_size\":256,\"learning_rate\":0.0032,\"model_name\":\"distilbert-base-uncased\",\"mp_parameters\":{\"ddp\":true,\"microbatches\":1,\"optimize\":\"speed\",\"partitions\":2,\"pipeline\":\"interleaved\",\"placement_strategy\":\"spread\"},\"train_batch_size\":128}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train-LORA.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p3.16xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.16xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p3.16xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train-LORA\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-578864530451/huggingface-pytorch-training-2023-09-22-20-27-32-127/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p3.16xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p3.16xlarge\",\"distribution_hosts\":[\"algo-1\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":1,\"eval_batch_size\":256,\"learning_rate\":0.0032,\"model_name\":\"distilbert-base-uncased\",\"mp_parameters\":{\"ddp\":true,\"microbatches\":1,\"optimize\":\"speed\",\"partitions\":2,\"pipeline\":\"interleaved\",\"placement_strategy\":\"spread\"},\"train_batch_size\":128},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-pytorch-training-2023-09-22-20-27-32-127\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-578864530451/huggingface-pytorch-training-2023-09-22-20-27-32-127/source/sourcedir.tar.gz\",\"module_name\":\"train-LORA\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.16xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train-LORA.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"1\",\"--eval_batch_size\",\"256\",\"--learning_rate\",\"0.0032\",\"--model_name\",\"distilbert-base-uncased\",\"--mp_parameters\",\"ddp=True,microbatches=1,optimize=speed,partitions=2,pipeline=interleaved,placement_strategy=spread\",\"--train_batch_size\",\"128\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_BATCH_SIZE=256\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0032\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=distilbert-base-uncased\u001b[0m\n",
      "\u001b[34mSM_HP_MP_PARAMETERS={\"ddp\":true,\"microbatches\":1,\"optimize\":\"speed\",\"partitions\":2,\"pipeline\":\"interleaved\",\"placement_strategy\":\"spread\"}\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=128\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1:8 -np 8 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/opt/conda/lib/python3.9/site-packages/gethostname.cpython-39-x86_64-linux-gnu.so -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_CURRENT_INSTANCE_TYPE -x SM_CURRENT_INSTANCE_GROUP -x SM_CURRENT_INSTANCE_GROUP_HOSTS -x SM_INSTANCE_GROUPS -x SM_INSTANCE_GROUPS_DICT -x SM_DISTRIBUTION_INSTANCE_GROUPS -x SM_IS_HETERO -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_NUM_NEURONS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM_CHANNEL_TEST -x SM_CHANNEL_TRAIN -x SM_HP_EPOCHS -x SM_HP_EVAL_BATCH_SIZE -x SM_HP_LEARNING_RATE -x SM_HP_MODEL_NAME -x SM_HP_MP_PARAMETERS -x SM_HP_TRAIN_BATCH_SIZE -x PYTHONPATH smddpmprun -i ml.p3.16xlarge --allow-bypass /opt/conda/bin/python3.9 -m mpi4py train-LORA.py --epochs 1 --eval_batch_size 256 --learning_rate 0.0032 --model_name distilbert-base-uncased --mp_parameters ddp=True,microbatches=1,optimize=speed,partitions=2,pipeline=interleaved,placement_strategy=spread --train_batch_size 128\u001b[0m\n",
      "\u001b[34m2023-09-22 20:34:38,092 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m[2023-09-22 20:34:40.298: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m2023-09-22 20:34:40,304 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m2023-09-22 20:34:40,336 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mData for JOB [41117,1] offset 0 Total slots allocated 8\n",
      " ========================   JOB MAP   ========================\n",
      " Data for node: algo-1#011Num slots: 8#011Max slots: 0#011Num procs: 8\n",
      " #011Process OMPI jobid: [41117,1] App: 0 Process rank: 0 Bound: N/A\n",
      " #011Process OMPI jobid: [41117,1] App: 0 Process rank: 1 Bound: N/A\n",
      " #011Process OMPI jobid: [41117,1] App: 0 Process rank: 2 Bound: N/A\n",
      " #011Process OMPI jobid: [41117,1] App: 0 Process rank: 3 Bound: N/A\n",
      " #011Process OMPI jobid: [41117,1] App: 0 Process rank: 4 Bound: N/A\n",
      " #011Process OMPI jobid: [41117,1] App: 0 Process rank: 5 Bound: N/A\n",
      " #011Process OMPI jobid: [41117,1] App: 0 Process rank: 6 Bound: N/A\n",
      " #011Process OMPI jobid: [41117,1] App: 0 Process rank: 7 Bound: N/A\n",
      " =============================================================\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:SMDDPCollectivesInitWarning: The system is not compatible or not configured to run SMDDP collectives optimized for AWS infrastructure. The training job will fall back to NCCL.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Refer to the following information to troubleshoot the issue:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:#011The instance type is not supported for running the training job. Please use one of the following: ml.p4d.24xlarge.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:SMDDPCollectivesInitWarning: The system is not compatible or not configured to run SMDDP collectives optimized for AWS infrastructure. The training job will fall back to NCCL.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Refer to the following information to troubleshoot the issue:\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:#011The instance type is not supported for running the training job. Please use one of the following: ml.p4d.24xlarge.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:SMDDPCollectivesInitWarning: The system is not compatible or not configured to run SMDDP collectives optimized for AWS infrastructure. The training job will fall back to NCCL.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Refer to the following information to troubleshoot the issue:\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:#011The instance type is not supported for running the training job. Please use one of the following: ml.p4d.24xlarge.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:SMDDPCollectivesInitWarning: The system is not compatible or not configured to run SMDDP collectives optimized for AWS infrastructure. The training job will fall back to NCCL.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Refer to the following information to troubleshoot the issue:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:#011The instance type is not supported for running the training job. Please use one of the following: ml.p4d.24xlarge.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:SMDDPCollectivesInitWarning: The system is not compatible or not configured to run SMDDP collectives optimized for AWS infrastructure. The training job will fall back to NCCL.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Refer to the following information to troubleshoot the issue:\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:#011The instance type is not supported for running the training job. Please use one of the following: ml.p4d.24xlarge.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:SMDDPCollectivesInitWarning: The system is not compatible or not configured to run SMDDP collectives optimized for AWS infrastructure. The training job will fall back to NCCL.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Refer to the following information to troubleshoot the issue:\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:#011The instance type is not supported for running the training job. Please use one of the following: ml.p4d.24xlarge.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:SMDDPCollectivesInitWarning: The system is not compatible or not configured to run SMDDP collectives optimized for AWS infrastructure. The training job will fall back to NCCL.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Refer to the following information to troubleshoot the issue:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:#011The instance type is not supported for running the training job. Please use one of the following: ml.p4d.24xlarge.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:SMDDPCollectivesInitWarning: The system is not compatible or not configured to run SMDDP collectives optimized for AWS infrastructure. The training job will fall back to NCCL.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Refer to the following information to troubleshoot the issue:\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:#011The instance type is not supported for running the training job. Please use one of the following: ml.p4d.24xlarge.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Collecting evaluate\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Collecting evaluate\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:#033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:#033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:     #033[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━#033[0m #033[32m0.0/81.4 kB#033[0m #033[31m?#033[0m eta #033[36m-:--:--#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:     #033[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━#033[0m #033[32m0.0/81.4 kB#033[0m #033[31m?#033[0m eta #033[36m-:--:--#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Collecting evaluate\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Collecting evaluate\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Collecting evaluate\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Collecting evaluate\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:#015#033[2K     #033[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━#033[0m #033[32m81.4/81.4 kB#033[0m #033[31m4.5 MB/s#033[0m eta #033[36m0:00:00#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  Using cached evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:#015#033[2K     #033[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━#033[0m #033[32m81.4/81.4 kB#033[0m #033[31m4.3 MB/s#033[0m eta #033[36m0:00:00#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Collecting evaluate\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Collecting evaluate\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  Using cached evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  Using cached evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:#033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:     #033[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━#033[0m #033[32m0.0/81.4 kB#033[0m #033[31m?#033[0m eta #033[36m-:--:--#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:#033[?25l[1,mpirank:7,algo-1]<stdout>:  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:#033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:     #033[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━#033[0m #033[32m0.0/81.4 kB#033[0m #033[31m?#033[0m eta #033[36m-:--:--#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:     #033[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━#033[0m #033[32m0.0/81.4 kB#033[0m #033[31m?#033[0m eta #033[36m-:--:--#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:#015#033[2K     #033[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━#033[0m #033[32m81.4/81.4 kB#033[0m #033[31m4.4 MB/s#033[0m eta #033[36m0:00:00#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:#015#033[2K     #033[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━#033[0m #033[32m81.4/81.4 kB#033[0m #033[31m4.8 MB/s#033[0m eta #033[36m0:00:00#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:#015#033[2K     #033[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━#033[0m #033[32m81.4/81.4 kB#033[0m #033[31m3.9 MB/s#033[0m eta #033[36m0:00:00#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.70.14)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2023.1.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.12.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: dill in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.3.6)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from evaluate) (3.2.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.9/site-packages (from evaluate) (4.64.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from evaluate) (1.5.3)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.12.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2.28.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from evaluate) (1.5.3)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.9/site-packages (from evaluate) (4.64.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.18.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: dill in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.3.6)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from evaluate) (1.23.5)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from evaluate) (3.2.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from evaluate) (1.23.5)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from evaluate) (1.23.5)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from evaluate) (23.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from evaluate) (23.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: dill in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.3.6)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.18.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from evaluate) (3.2.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2023.1.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.70.14)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.70.14)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2.28.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2023.1.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.12.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.9/site-packages (from evaluate) (4.64.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from evaluate) (1.5.3)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from evaluate) (23.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2.9.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2.9.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2.9.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.18.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2.28.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2023.1.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.70.14)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from evaluate) (1.5.3)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from evaluate) (1.23.5)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from evaluate) (3.2.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.12.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from evaluate) (23.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.9/site-packages (from evaluate) (4.64.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2.9.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from evaluate) (1.23.5)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2.28.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: dill in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.3.6)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2023.1.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.18.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2.9.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from evaluate) (1.5.3)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.9/site-packages (from evaluate) (4.64.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2.28.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from evaluate) (3.2.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.12.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.18.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from evaluate) (23.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: dill in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.3.6)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.70.14)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2.9.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2.28.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: dill in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.3.6)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.12.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from evaluate) (1.23.5)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.18.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.9/site-packages (from evaluate) (4.64.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from evaluate) (3.2.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.70.14)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from evaluate) (1.5.3)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2023.1.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from evaluate) (23.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.70.14)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.18.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from evaluate) (23.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2023.1.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from evaluate) (1.5.3)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: dill in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.3.6)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from evaluate) (1.23.5)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.12.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from evaluate) (3.2.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.9/site-packages (from evaluate) (4.64.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2.28.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2.9.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2.28.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from evaluate) (1.5.3)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from evaluate) (3.2.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2023.1.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.70.14)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from evaluate) (1.23.5)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.18.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from evaluate) (23.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.12.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.9/site-packages (from evaluate) (4.64.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2.9.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: dill in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.3.6)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (5.4.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (5.4.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (5.4.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (5.4.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (5.4.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (5.4.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (5.4.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (5.4.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.9.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.4.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.4.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.4.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.9.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.9.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2.1.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2022.12.7)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (3.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (1.26.14)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (1.26.14)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (3.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2.1.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2.1.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (3.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2022.12.7)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2022.12.7)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (1.26.14)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.9.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.4.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (1.26.14)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (3.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.9.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2022.12.7)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.4.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2.1.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2022.12.7)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (3.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (1.26.14)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2.1.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->evaluate) (2.8.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->evaluate) (2022.7.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->evaluate) (2.8.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->evaluate) (2.8.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->evaluate) (2022.7.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->evaluate) (2022.7.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.9.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.4.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.9.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.4.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (3.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->evaluate) (2.8.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2.1.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->evaluate) (2022.7.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (1.26.14)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2022.12.7)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.2.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.2.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.2.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2022.12.7)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (3.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2.1.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (1.26.14)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->evaluate) (2.8.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->evaluate) (2022.7.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.2.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->evaluate) (2.8.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->evaluate) (2022.7.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.2.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->evaluate) (2.8.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->evaluate) (2022.7.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.4.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.9.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.2.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2.1.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2022.12.7)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.2.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (3.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (1.26.14)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->evaluate) (2.8.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->evaluate) (2022.7.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.2.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Installing collected packages: evaluate\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Installing collected packages: evaluate\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Installing collected packages: evaluate\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Installing collected packages: evaluate\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Installing collected packages: evaluate\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Installing collected packages: evaluate\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Installing collected packages: evaluate\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Successfully installed evaluate-0.4.0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Successfully installed evaluate-0.4.0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Successfully installed evaluate-0.4.0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Successfully installed evaluate-0.4.0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Successfully installed evaluate-0.4.0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Successfully installed evaluate-0.4.0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Successfully installed evaluate-0.4.0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Installing collected packages: evaluate\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Successfully installed evaluate-0.4.0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Collecting rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Collecting rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Collecting rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Collecting rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  Downloading rouge_score-0.1.2.tar.gz (17 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  Downloading rouge_score-0.1.2.tar.gz (17 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  Preparing metadata (setup.py) ... #033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  Preparing metadata (setup.py) ... #033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Collecting rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  Downloading rouge_score-0.1.2.tar.gz (17 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  Using cached rouge_score-0.1.2.tar.gz (17 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  Preparing metadata (setup.py) ... #033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  Preparing metadata (setup.py) ... #033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  Downloading rouge_score-0.1.2.tar.gz (17 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  Preparing metadata (setup.py) ... #033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Collecting rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  Using cached rouge_score-0.1.2.tar.gz (17 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  Preparing metadata (setup.py) ... #033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Collecting rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  Using cached rouge_score-0.1.2.tar.gz (17 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  Preparing metadata (setup.py) ... #033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Collecting rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  Using cached rouge_score-0.1.2.tar.gz (17 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  Preparing metadata (setup.py) ... #033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:-\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:-\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:-\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:-\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:-\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:-\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:#010 #010done\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:#010 #010done\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:#010 #010done\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:-\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Collecting absl-py\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:#010 #010done\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  Downloading absl_py-2.0.0-py3-none-any.whl (130 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:#033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:#010 #010done\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Collecting absl-py\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:     #033[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━#033[0m #033[32m0.0/130.2 kB#033[0m #033[31m?#033[0m eta #033[36m-:--:--#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  Downloading absl_py-2.0.0-py3-none-any.whl (130 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:#033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:     #033[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━#033[0m #033[32m0.0/130.2 kB#033[0m #033[31m?#033[0m eta #033[36m-:--:--#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Collecting absl-py\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:#015#033[2K     #033[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━#033[0m #033[32m130.2/130.2 kB#033[0m #033[31m8.7 MB/s#033[0m eta #033[36m0:00:00#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  Using cached absl_py-2.0.0-py3-none-any.whl (130 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:#010 #010done\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:#015#033[2K     #033[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━#033[0m #033[32m130.2/130.2 kB#033[0m #033[31m8.0 MB/s#033[0m eta #033[36m0:00:00#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Collecting absl-py\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  Using cached absl_py-2.0.0-py3-none-any.whl (130 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Collecting absl-py\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  Using cached absl_py-2.0.0-py3-none-any.whl (130 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Collecting nltk\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Collecting nltk\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Collecting absl-py\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  Using cached absl_py-2.0.0-py3-none-any.whl (130 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Collecting nltk\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:#033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Collecting nltk\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:     #033[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━#033[0m #033[32m0.0/1.5 MB#033[0m #033[31m?#033[0m eta #033[36m-:--:--#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Collecting nltk\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:#033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:     #033[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━#033[0m #033[32m0.0/1.5 MB#033[0m #033[31m?#033[0m eta #033[36m-:--:--#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:#033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:-\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:#010 #010done\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:#033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:     #033[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━#033[0m #033[32m0.0/1.5 MB#033[0m #033[31m?#033[0m eta #033[36m-:--:--#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:     #033[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━#033[0m #033[32m0.0/1.5 MB#033[0m #033[31m?#033[0m eta #033[36m-:--:--#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:#015#033[2K     #033[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━#033[0m #033[32m1.5/1.5 MB#033[0m #033[31m61.4 MB/s#033[0m eta #033[36m0:00:00#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Collecting nltk\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:#015#033[2K     #033[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━#033[0m #033[32m1.5/1.5 MB#033[0m #033[31m67.5 MB/s#033[0m eta #033[36m0:00:00#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:#033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.23.5)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:     #033[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━#033[0m #033[32m0.0/1.5 MB#033[0m #033[31m?#033[0m eta #033[36m-:--:--#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.23.5)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.16.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.16.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Collecting absl-py\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  Using cached absl_py-2.0.0-py3-none-any.whl (130 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:#015#033[2K     #033[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━#033[0m#033[91m╸#033[0m #033[32m1.5/1.5 MB#033[0m #033[31m45.6 MB/s#033[0m eta #033[36m0:00:01#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:#015#033[2K     #033[91m━━━━━━━━#033[0m#033[91m╸#033[0m#033[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━#033[0m #033[32m0.3/1.5 MB#033[0m #033[31m10.0 MB/s#033[0m eta #033[36m0:00:01#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.23.5)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.16.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:#015#033[2K     #033[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━#033[0m #033[32m1.5/1.5 MB#033[0m #033[31m34.6 MB/s#033[0m eta #033[36m0:00:00#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (4.64.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (2022.10.31)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (8.1.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (1.2.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (8.1.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (2022.10.31)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (1.2.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (4.64.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.23.5)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.16.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (1.2.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (8.1.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (2022.10.31)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (4.64.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:#015#033[2K     #033[91m━━━━━━━━━━━━━━━━━━━━━━━━#033[0m#033[91m╸#033[0m#033[90m━━━━━━━━━━━━━━━#033[0m #033[32m0.9/1.5 MB#033[0m #033[31m34.0 MB/s#033[0m eta #033[36m0:00:01#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Building wheels for collected packages: rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  Building wheel for rouge_score (setup.py) ... #033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:#015#033[2K     #033[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━#033[0m #033[32m1.5/1.5 MB#033[0m #033[31m22.3 MB/s#033[0m eta #033[36m0:00:00#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Building wheels for collected packages: rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  Building wheel for rouge_score (setup.py) ... #033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Collecting nltk\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (8.1.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Building wheels for collected packages: rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (1.2.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  Building wheel for rouge_score (setup.py) ... #033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (2022.10.31)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (4.64.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.23.5)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:#010 #010done\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.16.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.23.5)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.16.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Building wheels for collected packages: rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  Building wheel for rouge_score (setup.py) ... #033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:#015#033[2K     #033[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━#033[0m #033[32m1.5/1.5 MB#033[0m #033[31m22.1 MB/s#033[0m eta #033[36m0:00:00#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (2022.10.31)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (1.2.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (4.64.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (8.1.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.23.5)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.16.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Collecting absl-py\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (8.1.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (2022.10.31)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (1.2.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  Using cached absl_py-2.0.0-py3-none-any.whl (130 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (4.64.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Building wheels for collected packages: rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  Building wheel for rouge_score (setup.py) ... #033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (8.1.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (4.64.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (1.2.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (2022.10.31)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Building wheels for collected packages: rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  Building wheel for rouge_score (setup.py) ... #033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Building wheels for collected packages: rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  Building wheel for rouge_score (setup.py) ... #033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Collecting nltk\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.23.5)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.16.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (4.64.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (2022.10.31)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (8.1.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (1.2.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Building wheels for collected packages: rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  Building wheel for rouge_score (setup.py) ... #033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:-[1,mpirank:2,algo-1]<stdout>:-\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:-\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:-\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:-\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:-\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:-\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:#010 #010\\\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:#010 #010\\\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:#010 #010\\\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:-\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:#010 #010\\\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:#010 #010\\\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:#010 #010\\\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:#010 #010\\\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:#010 #010|\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:#010 #010\\\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:#010 #010|\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:#010 #010|\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:#010 #010|\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:#010 #010|\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:#010 #010|\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:#010 #010|\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:#010 #010done\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=371904ea8d0262a6bf8db3d5263632d750f91df22962696313b0303dfe067ac9\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  Stored in directory: /root/.cache/pip/wheels/9b/3d/39/09558097d3119ca0a4d462df68f22c6f3c1b345ac63a09b86e\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Successfully built rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:#010 #010done\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:#010 #010done\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:#010 #010done\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=371904ea8d0262a6bf8db3d5263632d750f91df22962696313b0303dfe067ac9\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=371904ea8d0262a6bf8db3d5263632d750f91df22962696313b0303dfe067ac9\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  Stored in directory: /root/.cache/pip/wheels/9b/3d/39/09558097d3119ca0a4d462df68f22c6f3c1b345ac63a09b86e\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  Stored in directory: /root/.cache/pip/wheels/9b/3d/39/09558097d3119ca0a4d462df68f22c6f3c1b345ac63a09b86e\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=371904ea8d0262a6bf8db3d5263632d750f91df22962696313b0303dfe067ac9\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  Stored in directory: /root/.cache/pip/wheels/9b/3d/39/09558097d3119ca0a4d462df68f22c6f3c1b345ac63a09b86e\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Successfully built rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Successfully built rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Successfully built rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:#010 #010done\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=371904ea8d0262a6bf8db3d5263632d750f91df22962696313b0303dfe067ac9\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  Stored in directory: /root/.cache/pip/wheels/9b/3d/39/09558097d3119ca0a4d462df68f22c6f3c1b345ac63a09b86e\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:#010 #010done\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=371904ea8d0262a6bf8db3d5263632d750f91df22962696313b0303dfe067ac9\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Successfully built rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  Stored in directory: /root/.cache/pip/wheels/9b/3d/39/09558097d3119ca0a4d462df68f22c6f3c1b345ac63a09b86e\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:#010 #010done\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=371904ea8d0262a6bf8db3d5263632d750f91df22962696313b0303dfe067ac9\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  Stored in directory: /root/.cache/pip/wheels/9b/3d/39/09558097d3119ca0a4d462df68f22c6f3c1b345ac63a09b86e\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Successfully built rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Successfully built rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:#010 #010|\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:#010 #010done\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=371904ea8d0262a6bf8db3d5263632d750f91df22962696313b0303dfe067ac9\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  Stored in directory: /root/.cache/pip/wheels/9b/3d/39/09558097d3119ca0a4d462df68f22c6f3c1b345ac63a09b86e\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Successfully built rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Installing collected packages: nltk, absl-py, rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Installing collected packages: nltk, absl-py, rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Installing collected packages: nltk, absl-py, rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Installing collected packages: nltk, absl-py, rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Installing collected packages: nltk, absl-py, rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Installing collected packages: nltk, absl-py, rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Installing collected packages: nltk, absl-py, rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Installing collected packages: nltk, absl-py, rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Successfully installed absl-py-2.0.0 nltk-3.8.1 rouge_score-0.1.2\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Successfully installed absl-py-2.0.0 nltk-3.8.1 rouge_score-0.1.2\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Successfully installed absl-py-2.0.0 nltk-3.8.1 rouge_score-0.1.2\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Successfully installed absl-py-2.0.0 nltk-3.8.1 rouge_score-0.1.2\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Successfully installed absl-py-2.0.0 nltk-3.8.1 rouge_score-0.1.2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Successfully installed absl-py-2.0.0 nltk-3.8.1 rouge_score-0.1.2\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Successfully installed absl-py-2.0.0 nltk-3.8.1 rouge_score-0.1.2\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Successfully installed absl-py-2.0.0 nltk-3.8.1 rouge_score-0.1.2\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Collecting loralib\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  Downloading loralib-0.1.2-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Collecting loralib\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  Using cached loralib-0.1.2-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Collecting loralib\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  Using cached loralib-0.1.2-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Collecting loralib\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Collecting loralib\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  Using cached loralib-0.1.2-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  Using cached loralib-0.1.2-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Collecting loralib\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  Using cached loralib-0.1.2-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Collecting loralib\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  Using cached loralib-0.1.2-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Collecting loralib\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  Using cached loralib-0.1.2-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Installing collected packages: loralib\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Installing collected packages: loralib\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Successfully installed loralib-0.1.2\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Installing collected packages: loralib\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Successfully installed loralib-0.1.2\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Installing collected packages: loralib\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Successfully installed loralib-0.1.2\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Installing collected packages: loralib\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Installing collected packages: loralib\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Installing collected packages: loralib\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Successfully installed loralib-0.1.2\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Successfully installed loralib-0.1.2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Installing collected packages: loralib\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Successfully installed loralib-0.1.2\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Successfully installed loralib-0.1.2\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Successfully installed loralib-0.1.2\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Collecting peft==0.4.0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Collecting peft==0.4.0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Collecting peft==0.4.0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  Downloading peft-0.4.0-py3-none-any.whl (72 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:#033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:     #033[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━#033[0m #033[32m0.0/72.9 kB#033[0m #033[31m?#033[0m eta #033[36m-:--:--#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:#015#033[2K     #033[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━#033[0m #033[32m72.9/72.9 kB#033[0m #033[31m3.9 MB/s#033[0m eta #033[36m0:00:00#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  Downloading peft-0.4.0-py3-none-any.whl (72 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:#033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:     #033[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━#033[0m #033[32m0.0/72.9 kB#033[0m #033[31m?#033[0m eta #033[36m-:--:--#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  Downloading peft-0.4.0-py3-none-any.whl (72 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:#033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:     #033[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━#033[0m #033[32m0.0/72.9 kB#033[0m #033[31m?#033[0m eta #033[36m-:--:--#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:#015#033[2K     #033[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━#033[0m #033[32m72.9/72.9 kB#033[0m #033[31m3.7 MB/s#033[0m eta #033[36m0:00:00#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (5.4.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: transformers in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (4.26.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (23.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: accelerate in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (0.16.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (1.23.5)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:#015#033[2K     #033[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━#033[0m #033[32m72.9/72.9 kB#033[0m #033[31m4.1 MB/s#033[0m eta #033[36m0:00:00#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: accelerate in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (0.16.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Collecting peft==0.4.0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  Using cached peft-0.4.0-py3-none-any.whl (72 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Collecting peft==0.4.0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  Using cached peft-0.4.0-py3-none-any.whl (72 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Collecting peft==0.4.0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  Using cached peft-0.4.0-py3-none-any.whl (72 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: accelerate in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (0.16.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (23.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (5.4.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (1.23.5)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (5.9.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (23.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: transformers in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (4.26.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: accelerate in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (0.16.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Collecting peft==0.4.0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: transformers in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (4.26.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (5.4.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (5.9.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  Using cached peft-0.4.0-py3-none-any.whl (72 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (1.23.5)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: accelerate in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (0.16.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (5.4.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Collecting peft==0.4.0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  Using cached peft-0.4.0-py3-none-any.whl (72 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Collecting safetensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: accelerate in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (0.16.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  Downloading safetensors-0.3.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:#033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (5.4.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (1.23.5)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:     #033[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━#033[0m #033[32m0.0/1.3 MB#033[0m #033[31m?#033[0m eta #033[36m-:--:--#033[0m[1,mpirank:6,algo-1]<stdout>:Collecting safetensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (23.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (5.9.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: transformers in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (4.26.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  Downloading safetensors-0.3.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:#033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:     #033[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━#033[0m #033[32m0.0/1.3 MB#033[0m #033[31m?#033[0m eta #033[36m-:--:--#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Collecting safetensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:#015#033[2K     #033[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━#033[0m #033[32m1.3/1.3 MB#033[0m #033[31m50.8 MB/s#033[0m eta #033[36m0:00:00#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  Downloading safetensors-0.3.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:#033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Collecting safetensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:     #033[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━#033[0m #033[32m0.0/1.3 MB#033[0m #033[31m?#033[0m eta #033[36m-:--:--#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (5.9.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  Using cached safetensors-0.3.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Collecting safetensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:#015#033[2K     #033[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━#033[0m #033[32m1.3/1.3 MB#033[0m #033[31m47.4 MB/s#033[0m eta #033[36m0:00:00#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.4.0) (4.4.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: transformers in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (4.26.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (5.9.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (5.4.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  Using cached safetensors-0.3.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (1.23.5)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: transformers in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (4.26.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (5.4.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (1.23.5)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (23.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.4.0) (4.4.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (5.9.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:#015#033[2K     #033[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━#033[0m #033[32m1.3/1.3 MB#033[0m #033[31m53.2 MB/s#033[0m eta #033[36m0:00:00#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Collecting safetensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.4.0) (4.4.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.4.0) (4.4.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (5.9.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: accelerate in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (0.16.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (23.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: transformers in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (4.26.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (1.23.5)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  Using cached safetensors-0.3.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (5.4.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.4.0) (4.4.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (23.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.4.0) (4.4.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Collecting safetensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  Using cached safetensors-0.3.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (23.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (1.23.5)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (5.9.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: transformers in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (4.26.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: accelerate in /opt/conda/lib/python3.9/site-packages (from peft==0.4.0) (0.16.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.4.0) (4.4.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Collecting safetensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  Using cached safetensors-0.3.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.4.0) (4.4.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers->peft==0.4.0) (0.12.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers->peft==0.4.0) (2.28.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers->peft==0.4.0) (3.9.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers->peft==0.4.0) (2022.10.31)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers->peft==0.4.0) (4.64.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers->peft==0.4.0) (0.13.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers->peft==0.4.0) (2022.10.31)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers->peft==0.4.0) (4.64.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers->peft==0.4.0) (0.13.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers->peft==0.4.0) (2.28.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers->peft==0.4.0) (3.9.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers->peft==0.4.0) (0.12.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers->peft==0.4.0) (0.12.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers->peft==0.4.0) (3.9.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers->peft==0.4.0) (0.13.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers->peft==0.4.0) (4.64.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers->peft==0.4.0) (2022.10.31)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers->peft==0.4.0) (2.28.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers->peft==0.4.0) (4.64.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers->peft==0.4.0) (3.9.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers->peft==0.4.0) (0.13.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers->peft==0.4.0) (2022.10.31)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers->peft==0.4.0) (2.28.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers->peft==0.4.0) (3.9.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers->peft==0.4.0) (2022.10.31)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers->peft==0.4.0) (0.13.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers->peft==0.4.0) (4.64.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers->peft==0.4.0) (0.12.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers->peft==0.4.0) (2.28.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers->peft==0.4.0) (0.12.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers->peft==0.4.0) (4.64.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers->peft==0.4.0) (2.28.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers->peft==0.4.0) (2022.10.31)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers->peft==0.4.0) (0.13.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers->peft==0.4.0) (3.9.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers->peft==0.4.0) (2022.10.31)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers->peft==0.4.0) (0.12.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers->peft==0.4.0) (0.12.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers->peft==0.4.0) (3.9.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers->peft==0.4.0) (2.28.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers->peft==0.4.0) (0.13.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers->peft==0.4.0) (4.64.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft==0.4.0) (3.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft==0.4.0) (1.26.14)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft==0.4.0) (2.1.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft==0.4.0) (2022.12.7)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft==0.4.0) (2022.12.7)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft==0.4.0) (1.26.14)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft==0.4.0) (3.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft==0.4.0) (2.1.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft==0.4.0) (2022.12.7)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft==0.4.0) (1.26.14)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers->peft==0.4.0) (4.64.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft==0.4.0) (2.1.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers->peft==0.4.0) (2.28.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers->peft==0.4.0) (3.9.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft==0.4.0) (3.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers->peft==0.4.0) (2022.10.31)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers->peft==0.4.0) (0.13.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers->peft==0.4.0) (0.12.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft==0.4.0) (3.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft==0.4.0) (3.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft==0.4.0) (2.1.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft==0.4.0) (2.1.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft==0.4.0) (2022.12.7)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft==0.4.0) (1.26.14)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft==0.4.0) (1.26.14)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft==0.4.0) (2022.12.7)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft==0.4.0) (2.1.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft==0.4.0) (1.26.14)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft==0.4.0) (2022.12.7)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft==0.4.0) (3.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft==0.4.0) (3.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft==0.4.0) (2.1.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft==0.4.0) (1.26.14)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft==0.4.0) (2022.12.7)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft==0.4.0) (1.26.14)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft==0.4.0) (3.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft==0.4.0) (2022.12.7)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft==0.4.0) (2.1.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Installing collected packages: safetensors, peft\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Installing collected packages: safetensors, peft\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Installing collected packages: safetensors, peft\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Installing collected packages: safetensors, peft\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Installing collected packages: safetensors, peft\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Installing collected packages: safetensors, peft\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Installing collected packages: safetensors, peft\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Successfully installed peft-0.4.0 safetensors-0.3.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Successfully installed peft-0.4.0 safetensors-0.3.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Installing collected packages: safetensors, peft\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Successfully installed peft-0.4.0 safetensors-0.3.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Successfully installed peft-0.4.0 safetensors-0.3.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Successfully installed peft-0.4.0 safetensors-0.3.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Successfully installed peft-0.4.0 safetensors-0.3.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Successfully installed peft-0.4.0 safetensors-0.3.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Successfully installed peft-0.4.0 safetensors-0.3.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:34:59.063: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:34:59.138: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:34:59.152: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:34:59.209: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:34:59.219: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:34:59.230: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:34:59.246: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:34:59.336: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:00.186: W smdistributed/modelparallel/backend/core.py:306] smddp backend does not support training on single node. Falling back to nccl backend.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:00.187: I smdistributed/modelparallel/backend/core.py:364] [smddp] SMDATAPARALLEL_DEVICE_NAME is defined in os.environ: False.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:00.187: I smdistributed/modelparallel/backend/core.py:364] [smddp] SAGEMAKER_INSTANCE_TYPE is defined in os.environ: False.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:00.187: I smdistributed/modelparallel/torch/state_mod.py:106] [0] Initializing torch distributed process groups with nccl backend\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:00.187: W smdistributed/modelparallel/backend/core.py:306] smddp backend does not support training on single node. Falling back to nccl backend.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:00.187: W smdistributed/modelparallel/backend/core.py:306] smddp backend does not support training on single node. Falling back to nccl backend.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:00.188: W smdistributed/modelparallel/backend/core.py:306] smddp backend does not support training on single node. Falling back to nccl backend.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:00.188: W smdistributed/modelparallel/backend/core.py:306] smddp backend does not support training on single node. Falling back to nccl backend.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:00.189: I smdistributed/modelparallel/backend/core.py:364] [smddp] SMDATAPARALLEL_DEVICE_NAME is defined in os.environ: False.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:00.189: I smdistributed/modelparallel/backend/core.py:364] [smddp] SMDATAPARALLEL_DEVICE_NAME is defined in os.environ: False.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:00.189: W smdistributed/modelparallel/backend/core.py:306] smddp backend does not support training on single node. Falling back to nccl backend.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:00.189: I smdistributed/modelparallel/backend/core.py:364] [smddp] SMDATAPARALLEL_DEVICE_NAME is defined in os.environ: False.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:00.189: I smdistributed/modelparallel/backend/core.py:364] [smddp] SAGEMAKER_INSTANCE_TYPE is defined in os.environ: False.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:00.189: I smdistributed/modelparallel/backend/core.py:364] [smddp] SAGEMAKER_INSTANCE_TYPE is defined in os.environ: False.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:00.189: I smdistributed/modelparallel/backend/core.py:364] [smddp] SAGEMAKER_INSTANCE_TYPE is defined in os.environ: False.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:00.189: I smdistributed/modelparallel/torch/state_mod.py:106] [1] Initializing torch distributed process groups with nccl backend\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:00.189: I smdistributed/modelparallel/torch/state_mod.py:106] [3] Initializing torch distributed process groups with nccl backend\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:00.189: I smdistributed/modelparallel/torch/state_mod.py:106] [2] Initializing torch distributed process groups with nccl backend\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:00.189: W smdistributed/modelparallel/backend/core.py:306] smddp backend does not support training on single node. Falling back to nccl backend.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:00.190: I smdistributed/modelparallel/backend/core.py:364] [smddp] SMDATAPARALLEL_DEVICE_NAME is defined in os.environ: False.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:00.190: W smdistributed/modelparallel/backend/core.py:306] smddp backend does not support training on single node. Falling back to nccl backend.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:00.190: I smdistributed/modelparallel/backend/core.py:364] [smddp] SAGEMAKER_INSTANCE_TYPE is defined in os.environ: False.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:00.190: I smdistributed/modelparallel/torch/state_mod.py:106] [4] Initializing torch distributed process groups with nccl backend\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:00.190: I smdistributed/modelparallel/backend/core.py:364] [smddp] SMDATAPARALLEL_DEVICE_NAME is defined in os.environ: False.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:00.190: I smdistributed/modelparallel/backend/core.py:364] [smddp] SAGEMAKER_INSTANCE_TYPE is defined in os.environ: False.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:00.190: I smdistributed/modelparallel/torch/state_mod.py:106] [5] Initializing torch distributed process groups with nccl backend\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:00.191: I smdistributed/modelparallel/backend/core.py:364] [smddp] SMDATAPARALLEL_DEVICE_NAME is defined in os.environ: False.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:00.191: I smdistributed/modelparallel/backend/core.py:364] [smddp] SAGEMAKER_INSTANCE_TYPE is defined in os.environ: False.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:00.191: I smdistributed/modelparallel/backend/core.py:364] [smddp] SMDATAPARALLEL_DEVICE_NAME is defined in os.environ: False.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:00.191: I smdistributed/modelparallel/torch/state_mod.py:106] [6] Initializing torch distributed process groups with nccl backend\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:00.191: I smdistributed/modelparallel/backend/core.py:364] [smddp] SAGEMAKER_INSTANCE_TYPE is defined in os.environ: False.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:00.191: I smdistributed/modelparallel/torch/state_mod.py:106] [7] Initializing torch distributed process groups with nccl backend\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:20 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:20 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:20 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:20 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:20 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:20 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:20 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:20 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:21 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:02.246: I smdistributed/modelparallel/torch/state_mod.py:169] [7] Finished initializing torch distributed process groups. pp_rank: 1, tp_rank: 0, dp_rank: 3, rdp_rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:21 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:02.247: I smdistributed/modelparallel/torch/state_mod.py:169] [0] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:02.248: I smdistributed/modelparallel/torch/throttler.py:37] Using NCCL throttle limit of 8.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:21 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:21 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:21 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:21 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:02.256: I smdistributed/modelparallel/torch/state_mod.py:169] [2] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 2, rdp_rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:02.256: I smdistributed/modelparallel/torch/state_mod.py:169] [1] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 1, rdp_rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:02.256: I smdistributed/modelparallel/torch/state_mod.py:169] [3] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 3, rdp_rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:02.256: I smdistributed/modelparallel/torch/state_mod.py:169] [6] Finished initializing torch distributed process groups. pp_rank: 1, tp_rank: 0, dp_rank: 2, rdp_rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:21 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:21 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:02.256: I smdistributed/modelparallel/torch/state_mod.py:169] [5] Finished initializing torch distributed process groups. pp_rank: 1, tp_rank: 0, dp_rank: 1, rdp_rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:02.256: I smdistributed/modelparallel/torch/state_mod.py:169] [4] Finished initializing torch distributed process groups. pp_rank: 1, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:02.371: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:02.372: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:02.372: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:02.372: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:02.372: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:02.372: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:02.372: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:epochs 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:train_batch_size 128\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:eval_batch_size 256\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:warmup_steps 500\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:model_name distilbert-base-uncased\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:learning_rate 0.0032\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:args.output_data_dir /opt/ml/output/data\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:args.model_dir /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:args.n_gpus 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:args.training_dir /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:args.test_dir /opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:epochs 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:train_batch_size 128\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:eval_batch_size 256\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:warmup_steps 500\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:model_name distilbert-base-uncased\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:learning_rate [1,mpirank:6,algo-1]<stdout>:0.0032\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:args.output_data_dir /opt/ml/output/data\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:args.model_dir /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:args.n_gpus 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:args.training_dir /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:args.test_dir /opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:__main__: loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:__main__: loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:__main__: loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:__main__: loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:epochs 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:train_batch_size 128\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:eval_batch_size 256\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:warmup_steps 500\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:model_name[1,mpirank:4,algo-1]<stdout>: distilbert-base-uncased\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:learning_rate 0.0032\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:args.output_data_dir /opt/ml/output/data\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:args.model_dir /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:args.n_gpus 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:args.training_dir /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:args.test_dir /opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:__main__: loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:__main__: loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:epochs 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:train_batch_size 128\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:eval_batch_size 256\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:warmup_steps 500\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:model_name [1,mpirank:7,algo-1]<stdout>:distilbert-base-uncased\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:learning_rate 0.0032\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:args.output_data_dir /opt/ml/output/data\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:args.model_dir /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:args.n_gpus 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:args.training_dir /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:args.test_dir /opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:epochs 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:train_batch_size 128\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:eval_batch_size 256\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:warmup_steps 500\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:model_name distilbert-base-uncased\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:learning_rate 0.0032\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:args.output_data_dir /opt/ml/output/data\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:args.model_dir /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:args.n_gpus 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:args.training_dir /opt/ml/input/data/train[1,mpirank:1,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:args.test_dir /opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:epochs 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:train_batch_size [1,mpirank:5,algo-1]<stdout>:128\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:eval_batch_size 256\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:warmup_steps\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>: 500\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:model_name distilbert-base-uncased\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:learning_rate 0.0032\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:args.output_data_dir /opt/ml/output/data\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:args.model_dir /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:args.n_gpus[1,mpirank:5,algo-1]<stdout>: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:args.training_dir /opt/ml/input/data/train[1,mpirank:5,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:args.test_dir /opt/ml/input/data/test[1,mpirank:5,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:__main__: loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:__main__: loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:__main__: loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:__main__: loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:__main__: loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:__main__: loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:epochs 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:train_batch_size 128\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:eval_batch_size 256\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:warmup_steps 500[1,mpirank:2,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:model_name distilbert-base-uncased\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:learning_rate\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:0.0032\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:args.output_data_dir /opt/ml/output/data[1,mpirank:2,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:args.model_dir /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:args.n_gpus 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:args.training_dir [1,mpirank:2,algo-1]<stdout>:/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:args.test_dir /opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:__main__: loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:__main__: loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading (…)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading (…)lve/main/config.json: 100%|██████████| 483/483 [00:00<00:00, 55.8kB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:04.253: I smdistributed/modelparallel/backend/config.py:293] Configuration parameters:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:04.253: I smdistributed/modelparallel/backend/config.py:296]   pipeline_parallel_degree: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:04.253: I smdistributed/modelparallel/backend/config.py:296]   microbatches: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:04.253: I smdistributed/modelparallel/backend/config.py:296]   pipeline: interleaved\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:04.253: I smdistributed/modelparallel/backend/config.py:296]   horovod: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:04.254: I smdistributed/modelparallel/backend/config.py:296]   ddp: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:04.254: I smdistributed/modelparallel/backend/config.py:296]   tensor_parallel_degree: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:04.254: I smdistributed/modelparallel/backend/config.py:296]   sdp_reduce_bucket_size: 500000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:04.254: I smdistributed/modelparallel/backend/config.py:296]   sdp_param_persistence_threshold: 1000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:04.254: I smdistributed/modelparallel/backend/config.py:296]   sdp_max_live_parameters: 1000000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:04.254: I smdistributed/modelparallel/backend/config.py:296]   sdp_hierarchical_allgather: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:04.254: I smdistributed/modelparallel/backend/config.py:296]   sdp_gradient_clipping: 1.0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:04.254: I smdistributed/modelparallel/backend/config.py:296]   ddp_port: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:04.254: I smdistributed/modelparallel/backend/config.py:296]   ddp_dist_backend: auto\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:04.255: I smdistributed/modelparallel/backend/config.py:296]   contiguous: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:04.255: I smdistributed/modelparallel/backend/config.py:296]   placement_strategy: spread\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:04.255: I smdistributed/modelparallel/backend/config.py:296]   optimize: speed\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:04.255: I smdistributed/modelparallel/backend/config.py:296]   default_partition: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:04.255: I smdistributed/modelparallel/backend/config.py:296]   auto_partition: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:04.255: I smdistributed/modelparallel/backend/config.py:296]   prescaled_batch: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:04.255: I smdistributed/modelparallel/backend/config.py:296]   memory_weight: 0.8\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:04.255: I smdistributed/modelparallel/backend/config.py:296]   active_microbatches: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:04.255: I smdistributed/modelparallel/backend/config.py:296]   fp16: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:04.255: I smdistributed/modelparallel/backend/config.py:296]   fp16_params: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:04.256: I smdistributed/modelparallel/backend/config.py:296]   bf16: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:04.256: I smdistributed/modelparallel/backend/config.py:296]   tensor_parallel_seed: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:04.256: I smdistributed/modelparallel/backend/config.py:296]   offload_activations: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:04.256: I smdistributed/modelparallel/backend/config.py:296]   shard_optimizer_state: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:04.256: I smdistributed/modelparallel/backend/config.py:296]   sharded_data_parallel_degree: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:04.256: I smdistributed/modelparallel/backend/config.py:296]   delayed_parameter_initialization: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:04.256: I smdistributed/modelparallel/backend/config.py:296]   skip_tracing: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:04.256: I smdistributed/modelparallel/backend/config.py:296]   activation_loading_horizon: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:04.256: W smdistributed/modelparallel/backend/config.py:301] WARNING: \"fp16_params\" is a deprecated config key, please use \"fp16\" instead\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:04.257: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading (…)\"model.safetensors\";:   0%|          | 0.00/268M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading (…)\"model.safetensors\";:  12%|█▏        | 31.5M/268M [00:00<00:01, 209MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading (…)\"model.safetensors\";:  27%|██▋       | 73.4M/268M [00:00<00:00, 283MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading (…)\"model.safetensors\";:  43%|████▎     | 115M/268M [00:00<00:00, 335MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading (…)\"model.safetensors\";:  63%|██████▎   | 168M/268M [00:00<00:00, 380MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading (…)\"model.safetensors\";:  82%|████████▏ | 220M/268M [00:00<00:00, 401MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading (…)\"model.safetensors\";: 100%|██████████| 268M/268M [00:00<00:00, 409MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading (…)\"model.safetensors\";: 100%|██████████| 268M/268M [00:00<00:00, 370MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:epochs 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:train_batch_size 128\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:eval_batch_size 256\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:warmup_steps 500\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:model_name distilbert-base-uncased\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:learning_rate [1,mpirank:0,algo-1]<stdout>:0.0032\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:args.output_data_dir /opt/ml/output/data\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:args.model_dir /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:args.n_gpus 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:args.training_dir[1,mpirank:0,algo-1]<stdout>: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:args.test_dir /opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__: loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__: loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:ORIGINAL model\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>: trainable params: 66955010 || all params: 66955010 || trainable%: 100.00\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:ORIGINAL model\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:ORIGINAL model\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:ORIGINAL model\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>: trainable params: 66955010 || all params: 66955010 || trainable%: 100.00\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>: trainable params: 66955010 || all params: 66955010 || trainable%: 100.00\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>: trainable params: 66955010 || all params: 66955010 || trainable%: 100.00\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:ORIGINAL model\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:ORIGINAL model\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>: trainable params: 66955010 || all params: 66955010 || trainable%: 100.00\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>: trainable params: 66955010 || all params: 66955010 || trainable%: 100.00\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'classifier.bias', 'pre_classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'classifier.bias', 'pre_classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:ORIGINAL model\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>: trainable params: 66955010 || all params: 66955010 || trainable%: 100.00\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:PEFT model BEFORE WRAP\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>: trainable params: 1825540 || all params: 68136964 || trainable%: 2.68\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:PEFT model AFTER WRAP\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>: trainable params: 589824 || all params: 68136964 || trainable%: 0.87\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:PEFT model BEFORE WRAP\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:PEFT model BEFORE WRAP\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>: trainable params: 1825540 || all params: 68136964 || trainable%: 2.68\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>: trainable params: 1825540 || all params: 68136964 || trainable%: 2.68\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:PEFT model AFTER WRAP\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:PEFT model AFTER WRAP\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>: trainable params: 589824 || all params: 68136964 || trainable%: 0.87\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>: trainable params: 589824 || all params: 68136964 || trainable%: 0.87\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:PEFT model BEFORE WRAP\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>: trainable params: 1825540 || all params: 68136964 || trainable%: 2.68\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:PEFT model AFTER WRAP\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>: trainable params: 589824 || all params: 68136964 || trainable%: 0.87\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:PEFT model BEFORE WRAP\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>: trainable params: 1825540 || all params: 68136964 || trainable%: 2.68\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:PEFT model AFTER WRAP\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>: trainable params: 589824 || all params: 68136964 || trainable%: 0.87\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:PEFT model BEFORE WRAP\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>: trainable params: 1825540 || all params: 68136964 || trainable%: 2.68\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:PEFT model AFTER WRAP\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>: trainable params: 589824 || all params: 68136964 || trainable%: 0.87\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:PEFT model BEFORE WRAP\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>: trainable params: 1825540 || all params: 68136964 || trainable%: 2.68\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:PEFT model AFTER WRAP\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>: trainable params: 589824 || all params: 68136964 || trainable%: 0.87\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 3.47kB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 22.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Downloading (…)/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 24.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:ORIGINAL model\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>: trainable params: 66955010 || all params: 66955010 || trainable%: 100.00\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.571: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.embeddings.word_embeddings.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.571: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.embeddings.position_embeddings.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.571: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.embeddings.LayerNorm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.571: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.embeddings.LayerNorm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.571: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.q_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.571: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.q_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.572: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.k_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.572: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.k_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.572: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.v_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.572: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.v_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.572: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.out_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.572: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.out_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.572: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.sa_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.572: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.sa_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.573: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.ffn.lin1.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.573: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.ffn.lin1.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.573: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.ffn.lin2.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.573: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.ffn.lin2.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.573: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.output_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.573: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.output_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.573: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.q_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.573: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.q_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.573: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.k_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.574: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.k_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.574: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.v_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.574: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.v_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.574: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.out_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.574: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.out_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.574: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.sa_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.574: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.sa_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.574: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.ffn.lin1.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.574: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.ffn.lin1.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.575: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.ffn.lin2.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.575: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.ffn.lin2.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.575: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.output_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.575: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.output_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.575: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.q_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.575: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.q_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.575: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.k_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.575: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.k_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.575: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.v_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.576: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.v_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.576: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.out_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.576: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.out_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.576: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.sa_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.576: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.sa_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.576: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.ffn.lin1.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.576: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.ffn.lin1.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.576: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.ffn.lin2.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.576: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.ffn.lin2.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.577: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.output_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.577: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.output_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.577: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.q_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.577: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.q_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.577: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.k_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.577: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.k_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.577: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.v_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.577: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.v_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.577: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.out_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.578: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.out_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.578: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.sa_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.578: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.sa_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.578: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.ffn.lin1.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.578: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.ffn.lin1.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.578: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.ffn.lin2.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.578: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.ffn.lin2.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.578: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.output_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.578: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.output_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.578: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.q_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.579: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.q_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.579: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.k_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.579: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.k_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.579: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.v_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.579: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.v_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.579: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.out_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.579: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.out_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.579: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.sa_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.579: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.sa_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.580: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.ffn.lin1.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.580: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.ffn.lin1.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.580: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.ffn.lin2.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.580: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.ffn.lin2.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.580: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.embeddings.word_embeddings.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.580: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.output_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.580: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.embeddings.position_embeddings.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.580: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.output_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.580: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.embeddings.LayerNorm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.580: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.embeddings.word_embeddings.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.580: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.q_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.580: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.embeddings.LayerNorm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.580: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.q_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.580: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.embeddings.position_embeddings.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.581: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.q_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.581: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.k_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.581: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.embeddings.LayerNorm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.581: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.q_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.581: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.embeddings.LayerNorm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.581: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.k_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.581: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.k_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.581: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.v_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.581: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.q_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.581: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.k_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.581: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.q_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.581: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.v_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.581: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.v_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.581: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.out_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.581: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.v_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.581: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.k_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.581: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.out_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.581: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.out_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.581: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.k_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.581: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.out_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.581: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.sa_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.581: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.v_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.581: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.sa_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.581: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.sa_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.581: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.v_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.581: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.sa_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.581: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.embeddings.word_embeddings.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.581: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.ffn.lin1.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.581: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.out_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.581: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.ffn.lin1.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.582: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.ffn.lin1.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.582: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.out_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.582: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.ffn.lin1.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.582: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.embeddings.position_embeddings.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.582: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.ffn.lin2.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.582: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.sa_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.582: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.ffn.lin2.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.582: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.embeddings.LayerNorm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.582: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.ffn.lin2.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.582: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.sa_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.582: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.ffn.lin2.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.582: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.ffn.lin1.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.582: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.embeddings.LayerNorm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.582: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.output_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.582: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.output_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.582: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.ffn.lin1.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.582: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.output_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.582: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.q_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.582: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.output_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.582: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.ffn.lin2.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.582: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.q_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.582: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.q_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.582: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.pre_classifier.original_module.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.582: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.q_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.582: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.ffn.lin2.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.582: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.pre_classifier.original_module.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.582: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.k_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.582: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.k_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.582: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.pre_classifier.modules_to_save.default.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.582: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.output_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.582: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.k_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.582: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.k_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.582: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.output_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.582: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.pre_classifier.modules_to_save.default.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.582: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.v_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.582: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.v_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.582: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.q_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.583: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.v_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.583: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.classifier.original_module.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.583: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.v_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.583: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.q_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.583: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.out_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.583: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.classifier.original_module.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.583: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.k_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.583: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.out_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.583: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.out_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.583: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.classifier.modules_to_save.default.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.583: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.k_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.583: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.sa_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.583: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.out_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.583: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.classifier.modules_to_save.default.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.583: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.v_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.583: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.sa_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.583: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.sa_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.583: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.v_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.583: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.ffn.lin1.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.583: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.sa_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.583: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.out_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.583: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.ffn.lin1.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.583: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.ffn.lin1.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.583: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.out_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.583: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.ffn.lin2.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.583: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.sa_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.583: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.ffn.lin1.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.583: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.ffn.lin2.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.583: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.sa_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.583: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.output_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.583: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.ffn.lin2.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.583: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.output_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.583: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.ffn.lin1.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.583: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.ffn.lin2.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.584: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.ffn.lin1.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.584: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.q_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.584: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.output_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.584: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.q_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.584: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.ffn.lin2.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.584: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.output_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.584: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.ffn.lin2.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.584: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.k_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.584: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.output_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.584: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.q_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.584: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.k_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.584: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.output_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.584: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.v_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.584: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.q_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.584: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.q_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.584: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.v_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.584: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.k_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.584: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.q_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.584: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.out_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.584: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.k_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.584: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.k_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.584: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.out_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.584: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.v_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.584: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.sa_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.584: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.k_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.584: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.v_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.584: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.v_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.584: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.sa_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.584: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.out_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.585: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.ffn.lin1.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.585: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.v_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.585: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.out_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.585: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.ffn.lin1.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.585: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.sa_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.585: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.ffn.lin2.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.585: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.out_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.585: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.ffn.lin2.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.585: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.sa_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.585: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.out_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.585: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.output_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.585: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.ffn.lin1.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.585: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.sa_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.585: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.output_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.585: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.sa_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.585: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.q_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.585: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.ffn.lin1.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.585: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.ffn.lin1.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.585: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.q_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.585: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.ffn.lin2.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.585: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.embeddings.word_embeddings.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.585: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.ffn.lin1.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.585: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.ffn.lin2.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.585: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.k_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.585: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.ffn.lin2.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.585: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.embeddings.position_embeddings.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.585: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.k_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.585: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.output_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.585: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.ffn.lin2.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.585: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.embeddings.LayerNorm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.585: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.v_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.585: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.output_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.585: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.output_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.586: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.v_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.586: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.embeddings.LayerNorm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.586: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.output_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.586: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.q_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.586: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.out_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.586: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.q_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.586: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.q_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.586: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.q_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.586: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.out_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.586: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.q_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.586: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.q_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.586: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.sa_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.586: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.k_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.586: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.k_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.586: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.sa_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.586: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.k_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.586: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.k_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.586: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.k_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.586: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.ffn.lin1.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.586: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.k_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.586: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.v_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.586: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.v_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.586: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.ffn.lin1.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.586: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.v_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.586: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.v_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.586: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.ffn.lin2.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.586: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.v_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.586: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.v_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.586: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.out_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.586: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.ffn.lin2.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.586: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.out_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.586: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.out_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.586: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.out_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.586: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.output_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.586: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.out_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.586: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.output_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.586: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.out_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.586: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.sa_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.587: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.sa_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.587: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.q_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.587: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.sa_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.587: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.sa_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.587: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.sa_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.587: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.q_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.587: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.sa_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.587: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.ffn.lin1.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.587: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.ffn.lin1.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.587: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.k_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.587: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.ffn.lin1.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.587: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.ffn.lin1.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.587: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.ffn.lin1.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.587: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.k_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.587: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.ffn.lin2.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.587: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.ffn.lin1.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.587: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.ffn.lin2.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.587: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.v_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.587: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.ffn.lin2.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.587: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.ffn.lin2.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.587: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.v_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.587: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.ffn.lin2.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.587: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.ffn.lin2.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.587: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.output_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.587: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.output_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.587: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.out_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.587: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.output_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.587: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.output_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.587: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.out_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.587: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.q_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.587: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.output_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.587: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.output_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.587: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.q_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.587: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.sa_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.587: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.q_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.587: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.q_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.587: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.sa_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.587: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.k_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.587: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.q_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.588: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.q_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.588: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.ffn.lin1.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.588: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.k_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.588: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.ffn.lin1.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.588: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.k_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.588: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.k_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.588: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.v_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.588: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.ffn.lin2.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.588: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.v_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.588: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.k_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.588: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.k_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.588: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.ffn.lin2.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.588: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.out_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.588: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.v_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.588: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.v_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.588: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.output_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.588: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.out_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.588: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.v_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.588: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.v_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.588: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.output_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.588: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.sa_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.588: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.out_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.588: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.q_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.588: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.out_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.588: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.sa_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.588: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.out_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.588: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.out_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.588: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.q_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.588: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.ffn.lin1.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.588: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.k_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.588: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.sa_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.588: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.sa_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.588: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.ffn.lin1.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.588: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.k_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.588: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.sa_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.588: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.sa_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.589: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.ffn.lin2.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.589: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.v_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.589: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.ffn.lin1.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.589: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.ffn.lin1.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.589: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.v_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.589: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.ffn.lin2.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.589: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.ffn.lin1.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.589: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.ffn.lin1.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.589: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.output_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.589: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.out_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.589: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.ffn.lin2.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.589: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.ffn.lin2.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.589: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.out_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.589: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.output_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.589: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.ffn.lin2.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.589: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.ffn.lin2.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.589: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.q_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.589: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.sa_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.589: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.output_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.589: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.sa_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.589: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.q_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.589: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.output_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.589: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.output_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.589: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.ffn.lin1.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.589: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.output_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.589: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.k_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.589: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.ffn.lin1.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.589: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.q_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.589: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.k_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.589: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.q_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.589: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.ffn.lin2.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.589: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.q_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.589: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.v_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.589: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.q_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.589: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.ffn.lin2.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.589: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.v_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.589: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.k_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.589: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.output_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.589: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.k_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.590: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.out_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.590: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.k_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.590: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.output_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.590: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.k_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.590: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.out_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.590: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.v_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.590: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.pre_classifier.original_module.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.590: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.v_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.590: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.v_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.590: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.sa_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.590: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.pre_classifier.original_module.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.590: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.v_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.590: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.pre_classifier.modules_to_save.default.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.590: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.sa_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.590: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.out_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.590: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.pre_classifier.modules_to_save.default.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.590: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.out_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.590: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.ffn.lin1.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.590: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.out_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.590: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.classifier.original_module.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.590: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.ffn.lin1.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.590: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.out_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.590: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.sa_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.590: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.classifier.original_module.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.590: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.ffn.lin2.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.590: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.sa_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.590: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.sa_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.590: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.classifier.modules_to_save.default.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.590: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.ffn.lin2.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.590: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.ffn.lin1.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.590: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.sa_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.590: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.classifier.modules_to_save.default.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.590: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.output_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.590: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.ffn.lin1.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.590: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.ffn.lin1.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.590: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.output_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.590: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.ffn.lin1.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.590: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.ffn.lin2.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.590: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.pre_classifier.original_module.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.591: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.ffn.lin2.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.591: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.pre_classifier.original_module.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.591: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.ffn.lin2.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.591: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.output_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.591: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.pre_classifier.modules_to_save.default.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.591: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.ffn.lin2.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.591: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.pre_classifier.modules_to_save.default.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.591: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.output_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.591: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.output_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.591: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.classifier.original_module.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.591: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.q_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.591: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.output_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.591: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.classifier.original_module.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.591: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.q_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.591: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.q_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.591: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.classifier.modules_to_save.default.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.591: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.k_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.591: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.classifier.modules_to_save.default.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.591: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.q_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.591: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.k_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.591: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.k_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.591: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.v_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.591: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.k_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.591: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.v_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.591: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.v_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.592: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.out_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.592: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.v_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.592: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.out_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.592: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.out_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.592: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.sa_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.592: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.out_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.592: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.sa_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.592: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.sa_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.592: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.ffn.lin1.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.592: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.sa_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.592: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.ffn.lin1.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.592: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.ffn.lin1.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.592: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.ffn.lin2.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.592: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.ffn.lin2.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.592: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.ffn.lin1.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.592: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.output_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.592: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.ffn.lin2.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.593: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.output_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.593: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.ffn.lin2.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.593: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.q_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.593: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.output_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.593: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.q_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.593: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.output_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.593: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.k_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.593: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.pre_classifier.original_module.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.593: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.k_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.593: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.pre_classifier.original_module.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.593: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.v_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.593: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.pre_classifier.modules_to_save.default.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.593: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.v_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.593: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.pre_classifier.modules_to_save.default.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.593: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.out_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.593: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.classifier.original_module.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.593: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.out_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.593: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.classifier.original_module.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.594: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.sa_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.594: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.classifier.modules_to_save.default.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.594: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.sa_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.594: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.classifier.modules_to_save.default.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.594: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.ffn.lin1.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.594: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.ffn.lin1.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.594: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.ffn.lin2.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.594: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.embeddings.word_embeddings.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.594: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.ffn.lin2.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.594: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.output_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.594: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.embeddings.position_embeddings.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.594: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.output_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.594: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.embeddings.LayerNorm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.594: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.q_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.594: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.embeddings.LayerNorm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.594: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.q_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.594: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.q_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.595: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.k_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.595: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.q_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.595: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.k_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.595: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.v_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.595: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.k_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.595: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.v_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.595: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.k_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.595: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.v_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.595: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.out_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.595: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.v_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.595: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.out_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.595: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.sa_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.595: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.out_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.595: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.sa_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.595: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.out_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.595: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.ffn.lin1.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.595: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.sa_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.595: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.ffn.lin1.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.596: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.sa_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.596: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.ffn.lin2.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.596: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.ffn.lin1.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.596: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.ffn.lin2.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.596: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.ffn.lin1.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.596: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.output_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.596: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.ffn.lin2.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.596: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.output_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.596: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.ffn.lin2.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.596: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.pre_classifier.original_module.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.596: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.output_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.596: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.pre_classifier.original_module.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.596: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.output_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.596: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.pre_classifier.modules_to_save.default.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.596: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.q_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.596: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.pre_classifier.modules_to_save.default.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.596: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.q_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.596: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.classifier.original_module.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.597: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.classifier.original_module.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.597: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.k_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.597: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.classifier.modules_to_save.default.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.597: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.k_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.597: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.classifier.modules_to_save.default.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.597: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.v_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.597: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.v_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.597: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.out_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.597: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.out_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.597: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.sa_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.597: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.sa_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.597: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.ffn.lin1.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.598: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.ffn.lin1.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.598: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.ffn.lin2.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.598: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.ffn.lin2.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.598: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.output_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.598: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.output_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.598: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.q_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.598: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.q_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.598: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.k_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.598: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.k_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.599: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.v_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.599: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.v_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.599: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.out_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.599: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.out_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.599: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.sa_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.599: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.sa_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.599: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.ffn.lin1.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.599: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.ffn.lin1.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.599: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.ffn.lin2.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.600: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.ffn.lin2.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.600: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.output_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.600: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.output_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.600: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.q_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.600: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.q_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.600: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.k_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.600: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.k_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.600: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.v_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.601: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.v_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.601: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.out_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.601: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.out_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.601: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.sa_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.601: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.sa_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.601: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.ffn.lin1.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.601: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.ffn.lin1.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.601: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.ffn.lin2.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.601: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.ffn.lin2.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.602: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.output_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.602: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.output_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.602: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.q_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.602: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.q_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.602: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.k_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.602: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.k_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.602: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.v_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.602: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.v_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.602: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.out_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.603: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.out_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.603: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.sa_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.603: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.sa_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.603: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.ffn.lin1.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.603: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.ffn.lin1.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.603: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.ffn.lin2.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.603: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.ffn.lin2.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.603: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.output_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.603: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.output_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.603: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.q_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.604: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.q_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.604: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.k_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.604: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.k_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.604: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.v_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.604: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.v_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.604: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.out_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.604: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.out_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.604: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.sa_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.605: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.sa_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.605: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.ffn.lin1.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.605: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.ffn.lin1.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.605: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.ffn.lin2.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.605: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.ffn.lin2.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.605: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.embeddings.word_embeddings.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.605: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.output_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.605: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.embeddings.position_embeddings.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.605: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.output_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.605: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.embeddings.LayerNorm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.605: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.pre_classifier.original_module.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.605: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.embeddings.LayerNorm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.605: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.pre_classifier.original_module.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.605: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.q_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.606: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.pre_classifier.modules_to_save.default.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.606: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.q_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.606: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.pre_classifier.modules_to_save.default.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.606: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.k_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.606: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.classifier.original_module.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.606: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.k_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.606: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.classifier.original_module.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.606: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.v_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.606: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.classifier.modules_to_save.default.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.606: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.v_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.606: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.classifier.modules_to_save.default.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.606: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.out_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.606: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.out_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.606: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.sa_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.606: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.sa_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.606: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.ffn.lin1.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.607: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.ffn.lin1.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.607: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.ffn.lin2.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.607: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.ffn.lin2.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.607: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.output_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.607: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.output_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.607: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.q_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.607: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.q_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.607: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.k_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.607: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.k_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.607: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.v_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.607: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.v_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.608: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.out_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.608: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.out_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.608: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.sa_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.608: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.sa_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.608: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.ffn.lin1.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.608: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.ffn.lin1.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.608: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.ffn.lin2.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.608: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.ffn.lin2.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.608: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.output_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.609: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.output_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.609: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.q_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.609: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.q_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.609: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.k_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.609: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.k_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.609: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.v_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.609: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.v_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.609: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.out_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.609: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.out_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.609: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.sa_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.609: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.sa_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.610: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.ffn.lin1.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.610: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.ffn.lin1.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.610: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.ffn.lin2.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.610: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.ffn.lin2.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.610: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.output_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.610: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.output_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.610: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.q_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.610: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.q_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.610: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.k_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.610: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.k_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.611: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.v_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.611: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.v_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.611: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.out_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.611: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.out_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.611: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.sa_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.611: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.sa_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.611: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.ffn.lin1.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.611: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.ffn.lin1.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.611: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.ffn.lin2.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.611: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.ffn.lin2.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.611: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.output_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.612: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.output_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.612: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.q_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.612: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.q_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.612: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.k_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.612: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.k_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.612: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.v_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.612: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.v_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.612: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.out_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.612: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.out_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.612: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.sa_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.613: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.sa_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.613: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.ffn.lin1.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.613: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.ffn.lin1.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.613: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.ffn.lin2.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.613: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.ffn.lin2.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.613: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.output_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.613: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.output_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.613: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.q_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.613: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.q_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.613: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.k_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.614: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.k_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.614: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.v_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.614: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.v_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.614: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.out_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.614: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.out_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.614: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.sa_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.614: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.sa_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.614: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.ffn.lin1.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.614: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.ffn.lin1.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.614: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.ffn.lin2.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.614: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.ffn.lin2.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.615: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.output_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.615: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.output_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.615: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.pre_classifier.original_module.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.615: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.pre_classifier.original_module.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.615: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.pre_classifier.modules_to_save.default.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.615: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.pre_classifier.modules_to_save.default.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.615: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.classifier.original_module.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.615: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.classifier.original_module.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.615: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.classifier.modules_to_save.default.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.615: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.classifier.modules_to_save.default.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.618 algo-1:186 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.623 algo-1:184 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.626 algo-1:185 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.629 algo-1:183 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.631 algo-1:181 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.642 algo-1:176 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.647 algo-1:182 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.670 algo-1:186 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.670 algo-1:184 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.671 algo-1:186 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.671 algo-1:184 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.671 algo-1:186 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.671 algo-1:184 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.672 algo-1:186 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-09-22 20:35:06.672 algo-1:186 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.672 algo-1:184 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.672 algo-1:184 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.673 algo-1:185 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.673 algo-1:185 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.674 algo-1:185 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.674 algo-1:185 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.674 algo-1:185 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.678 algo-1:181 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.679 algo-1:183 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.679 algo-1:181 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.680 algo-1:183 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.680 algo-1:181 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.680 algo-1:183 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.680 algo-1:181 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-09-22 20:35:06.681 algo-1:181 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.681 algo-1:183 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-09-22 20:35:06.681 algo-1:183 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.691 algo-1:182 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.692 algo-1:176 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.692 algo-1:182 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.692 algo-1:176 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.692 algo-1:182 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.693 algo-1:176 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.693 algo-1:182 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:06.693 algo-1:182 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.693 algo-1:176 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.694 algo-1:176 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:PEFT model BEFORE WRAP\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>: trainable params: 1825540 || all params: 68136964 || trainable%: 2.68\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:PEFT model AFTER WRAP\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>: trainable params: 589824 || all params: 68136964 || trainable%: 0.87\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:06.745: W smdistributed/modelparallel/backend/split.py:167] Non-splittable object of type <class 'transformers.tokenization_utils_base.BatchEncoding'> passed to smp.step. If this object contains tensors that need to be split across microbatches, implement a 'smp_slice' method for this class. See SMP documentation for further information.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:06.748: W smdistributed/modelparallel/backend/split.py:167] Non-splittable object of type <class 'transformers.tokenization_utils_base.BatchEncoding'> passed to smp.step. If this object contains tensors that need to be split across microbatches, implement a 'smp_slice' method for this class. See SMP documentation for further information.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:06.771: W smdistributed/modelparallel/backend/split.py:167] Non-splittable object of type <class 'transformers.tokenization_utils_base.BatchEncoding'> passed to smp.step. If this object contains tensors that need to be split across microbatches, implement a 'smp_slice' method for this class. See SMP documentation for further information.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:The following columns in the training set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:The following columns in the training set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.803: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.embeddings.word_embeddings.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.803: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.embeddings.position_embeddings.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.803: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.embeddings.LayerNorm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.803: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.embeddings.LayerNorm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.803: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.q_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.804: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.q_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.804: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.k_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.804: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.k_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.804: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.v_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.804: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.v_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.804: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.out_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.804: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.attention.out_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.804: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.sa_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.804: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.sa_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.804: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.ffn.lin1.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.805: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.ffn.lin1.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.805: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.ffn.lin2.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.805: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.ffn.lin2.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.805: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.output_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.805: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.0.output_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.805: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.q_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.805: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.q_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.805: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.k_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.805: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.k_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.805: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.v_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.805: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.v_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.806: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.out_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.806: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.attention.out_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.806: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.sa_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.806: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.sa_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.806: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.ffn.lin1.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.806: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.ffn.lin1.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.806: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.ffn.lin2.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.806: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.ffn.lin2.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.806: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.output_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.806: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.1.output_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.806: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.q_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.807: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.q_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.807: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.k_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.807: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.k_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.807: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.v_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.807: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.v_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.807: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.out_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.807: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.attention.out_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.807: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.sa_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.807: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.sa_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.807: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.ffn.lin1.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.807: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.ffn.lin1.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.808: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.ffn.lin2.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.808: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.ffn.lin2.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.808: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.output_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.808: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.2.output_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.808: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.q_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.808: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.q_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.808: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.k_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.808: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.k_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.808: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.v_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.808: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.v_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.809: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.out_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.809: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.attention.out_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.809: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.sa_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.809: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.sa_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.809: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.ffn.lin1.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.809: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.ffn.lin1.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.809: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.ffn.lin2.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.809: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.ffn.lin2.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.809: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.output_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.809: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.3.output_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.809: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.q_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.809: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.q_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.810: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.k_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.810: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.k_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.810: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.v_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.810: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.v_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.810: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.out_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.810: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.attention.out_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.810: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.sa_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.810: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.sa_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.810: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.ffn.lin1.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.810: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.ffn.lin1.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.810: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.ffn.lin2.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.811: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.ffn.lin2.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.811: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.output_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.811: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.4.output_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.811: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.q_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.811: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.q_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.811: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.k_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.811: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.k_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.811: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.v_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.811: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.v_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.811: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.out_lin.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.811: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.attention.out_lin.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.811: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.sa_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.812: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.sa_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.812: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.ffn.lin1.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.812: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.ffn.lin1.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.812: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.ffn.lin2.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.812: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.ffn.lin2.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.812: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.output_layer_norm.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.812: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.distilbert.transformer.layer.5.output_layer_norm.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.812: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.pre_classifier.original_module.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.812: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.pre_classifier.original_module.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.812: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.pre_classifier.modules_to_save.default.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.812: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.pre_classifier.modules_to_save.default.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.813: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.classifier.original_module.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.813: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.classifier.original_module.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.813: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.classifier.modules_to_save.default.weight is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.813: W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.classifier.modules_to_save.default.bias is missing when loading optimizer's state_dict, skip.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:***** Running training *****\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  Num examples = 25000\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  Instantaneous batch size per device = 128\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  Total train batch size (w. parallel, distributed & accumulation) = 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:***** Running training *****\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  Num examples = 25000\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  Instantaneous batch size per device = 128\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  Total train batch size (w. parallel, distributed & accumulation) = 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  Total optimization steps = 49\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  Total optimization steps = 49\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  Number of trainable parameters = 589824\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  Number of trainable parameters = 589824\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  0%|          | 0/49 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.843 algo-1:175 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.885 algo-1:175 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.885 algo-1:175 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.886 algo-1:175 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.886 algo-1:175 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.886 algo-1:175 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.950: W smdistributed/modelparallel/backend/split.py:167] Non-splittable object of type <class 'transformers.tokenization_utils_base.BatchEncoding'> passed to smp.step. If this object contains tensors that need to be split across microbatches, implement a 'smp_slice' method for this class. See SMP documentation for further information.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:06.951: I smdistributed/modelparallel/torch/worker.py:300] Tracing on GPU. If the model parameters do not fit in a single GPU, you can set trace_device to `cpu`.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:10.789: I smdistributed/modelparallel/torch/model.py:682] Partition assignments:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:10.790: I smdistributed/modelparallel/torch/model.py:691] main: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:10.790: I smdistributed/modelparallel/torch/model.py:691] main/module: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:10.790: I smdistributed/modelparallel/torch/model.py:691] main/module/module: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:10.790: I smdistributed/modelparallel/torch/model.py:691] main/module/module/base_model: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:10.790: I smdistributed/modelparallel/torch/model.py:691] main/module/module/base_model/model: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:10.790: I smdistributed/modelparallel/torch/model.py:691] main/module/module/base_model/model/distilbert: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:10.790: I smdistributed/modelparallel/torch/model.py:691] main/module/module/base_model/model/pre_classifier: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:10.790: I smdistributed/modelparallel/torch/model.py:691] main/module/module/base_model/model/classifier: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:10.790: I smdistributed/modelparallel/torch/model.py:691] main/module/module/base_model/model/dropout: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:10.791: I smdistributed/modelparallel/torch/model.py:691] main/module/module/base_model/model/distilbert/embeddings: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:10.791: I smdistributed/modelparallel/torch/model.py:691] main/module/module/base_model/model/distilbert/transformer: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:10.791: I smdistributed/modelparallel/torch/model.py:691] main/module/module/base_model/model/distilbert/transformer/layer: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:10.791: I smdistributed/modelparallel/torch/model.py:691] main/module/module/base_model/model/distilbert/transformer/layer/0: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:10.791: I smdistributed/modelparallel/torch/model.py:691] main/module/module/base_model/model/distilbert/transformer/layer/1: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:10.791: I smdistributed/modelparallel/torch/model.py:691] main/module/module/base_model/model/distilbert/transformer/layer/2: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:10.791: I smdistributed/modelparallel/torch/model.py:691] main/module/module/base_model/model/distilbert/transformer/layer/3: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:10.791: I smdistributed/modelparallel/torch/model.py:691] main/module/module/base_model/model/distilbert/transformer/layer/4: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:10.791: I smdistributed/modelparallel/torch/model.py:691] main/module/module/base_model/model/distilbert/transformer/layer/5: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:10.836: I smdistributed/modelparallel/torch/model.py:616] Number of parameters on partition 0 are 72. 12 require grads\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:10.837: I smdistributed/modelparallel/torch/model.py:645] Number of buffers on partition 0 are 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:10.852: I smdistributed/modelparallel/torch/model.py:616] Number of parameters on partition 1 are 60. 12 require grads\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:10.874: I smdistributed/modelparallel/torch/model.py:742] Finished partitioning the model\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:175:175 [0] NCCL INFO Bootstrap : Using eth0:10.2.107.241<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:175:175 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:175:175 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:175:175 [0] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:NCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:185:185 [3] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:176:176 [2] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:184:184 [1] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:182:182 [4] NCCL INFO Bootstrap : Using eth0:10.2.107.241<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:182:182 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:182:182 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:182:182 [4] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:NCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:186:186 [6] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:181:181 [5] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:183:183 [7] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:175:1840 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:175:1840 [0] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:182:1841 [4] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:182:1841 [4] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:184:184 [1] NCCL INFO Bootstrap : Using eth0:10.2.107.241<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:185:185 [3] NCCL INFO Bootstrap : Using eth0:10.2.107.241<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:184:184 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:184:184 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:185:185 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:185:185 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:176:176 [2] NCCL INFO Bootstrap : Using eth0:10.2.107.241<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:186:186 [6] NCCL INFO Bootstrap : Using eth0:10.2.107.241<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:176:176 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:176:176 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:181:181 [5] NCCL INFO Bootstrap : Using eth0:10.2.107.241<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:186:186 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:186:186 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:183:183 [7] NCCL INFO Bootstrap : Using eth0:10.2.107.241<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:181:181 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:181:181 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:183:183 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:183:183 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:175:1840 [0] ofi_init:1304 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:175:1840 [0] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:175:1840 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:175:1840 [0] NCCL INFO NET/Socket : Using [0]eth0:10.2.107.241<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:175:1840 [0] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:184:1842 [1] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:184:1842 [1] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:185:1843 [3] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:185:1843 [3] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:176:1844 [2] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:176:1844 [2] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:186:1845 [6] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:186:1845 [6] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:181:1846 [5] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:181:1846 [5] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:183:1847 [7] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:183:1847 [7] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:182:1841 [4] ofi_init:1304 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:182:1841 [4] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:182:1841 [4] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:182:1841 [4] NCCL INFO NET/Socket : Using [0]eth0:10.2.107.241<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:182:1841 [4] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:184:1842 [1] ofi_init:1304 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:184:1842 [1] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:184:1842 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:184:1842 [1] NCCL INFO NET/Socket : Using [0]eth0:10.2.107.241<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:184:1842 [1] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:181:1846 [5] ofi_init:1304 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:181:1846 [5] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:181:1846 [5] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:181:1846 [5] NCCL INFO NET/Socket : Using [0]eth0:10.2.107.241<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:181:1846 [5] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:183:1847 [7] ofi_init:1304 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:183:1847 [7] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:183:1847 [7] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:183:1847 [7] NCCL INFO NET/Socket : Using [0]eth0:10.2.107.241<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:183:1847 [7] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:176:1844 [2] ofi_init:1304 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:176:1844 [2] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:176:1844 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:176:1844 [2] NCCL INFO NET/Socket : Using [0]eth0:10.2.107.241<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:176:1844 [2] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:186:1845 [6] ofi_init:1304 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:186:1845 [6] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:186:1845 [6] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:186:1845 [6] NCCL INFO NET/Socket : Using [0]eth0:10.2.107.241<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:186:1845 [6] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:185:1843 [3] ofi_init:1304 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:185:1843 [3] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:185:1843 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:185:1843 [3] NCCL INFO NET/Socket : Using [0]eth0:10.2.107.241<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:185:1843 [3] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:176:1844 [2] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:176:1844 [2] NCCL INFO Trees [0] 1/-1/-1->2->3 [1] 3/-1/-1->2->1 [2] 1/-1/-1->2->3 [3] 3/-1/-1->2->1 [4] 1/-1/-1->2->3 [5] 3/-1/-1->2->1 [6] 1/-1/-1->2->3 [7] 3/-1/-1->2->1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:175:1840 [0] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:175:1840 [0] NCCL INFO Channel 00/08 :    0   1   2   3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:175:1840 [0] NCCL INFO Channel 01/08 :    0   3   2   1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:175:1840 [0] NCCL INFO Channel 02/08 :    0   3   1   2\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:175:1840 [0] NCCL INFO Channel 03/08 :    0   2   1   3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:175:1840 [0] NCCL INFO Channel 04/08 :    0   1   2   3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:175:1840 [0] NCCL INFO Channel 05/08 :    0   3   2   1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:175:1840 [0] NCCL INFO Channel 06/08 :    0   3   1   2\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:175:1840 [0] NCCL INFO Channel 07/08 :    0   2   1   3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:175:1840 [0] NCCL INFO Trees [0] 3/-1/-1->0->-1 [1] -1/-1/-1->0->3 [2] 3/-1/-1->0->-1 [3] -1/-1/-1->0->3 [4] 3/-1/-1->0->-1 [5] -1/-1/-1->0->3 [6] 3/-1/-1->0->-1 [7] -1/-1/-1->0->3\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:184:1842 [1] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:184:1842 [1] NCCL INFO Trees [0] -1/-1/-1->1->2 [1] 2/-1/-1->1->-1 [2] -1/-1/-1->1->2 [3] 2/-1/-1->1->-1 [4] -1/-1/-1->1->2 [5] 2/-1/-1->1->-1 [6] -1/-1/-1->1->2 [7] 2/-1/-1->1->-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:185:1843 [3] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:185:1843 [3] NCCL INFO Trees [0] 2/-1/-1->3->0 [1] 0/-1/-1->3->2 [2] 2/-1/-1->3->0 [3] 0/-1/-1->3->2 [4] 2/-1/-1->3->0 [5] 0/-1/-1->3->2 [6] 2/-1/-1->3->0 [7] 0/-1/-1->3->2\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:181:1846 [5] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:181:1846 [5] NCCL INFO Trees [0] -1/-1/-1->1->2 [1] 2/-1/-1->1->-1 [2] -1/-1/-1->1->2 [3] 2/-1/-1->1->-1 [4] -1/-1/-1->1->2 [5] 2/-1/-1->1->-1 [6] -1/-1/-1->1->2 [7] 2/-1/-1->1->-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:182:1841 [4] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:182:1841 [4] NCCL INFO Channel 00/08 :    0   1   2   3\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:182:1841 [4] NCCL INFO Channel 01/08 :    0   3   2   1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:182:1841 [4] NCCL INFO Channel 02/08 :    0   3   1   2\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:182:1841 [4] NCCL INFO Channel 03/08 :    0   2   1   3\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:182:1841 [4] NCCL INFO Channel 04/08 :    0   1   2   3\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:182:1841 [4] NCCL INFO Channel 05/08 :    0   3   2   1\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:186:1845 [6] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:183:1847 [7] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:183:1847 [7] NCCL INFO Trees [0] 2/-1/-1->3->0 [1] 0/-1/-1->3->2 [2] 2/-1/-1->3->0 [3] 0/-1/-1->3->2 [4] 2/-1/-1->3->0 [5] 0/-1/-1->3->2 [6] 2/-1/-1->3->0 [7] 0/-1/-1->3->2\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:182:1841 [4] NCCL INFO Channel 06/08 :    0   3   1   2\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:182:1841 [4] NCCL INFO Channel 07/08 :    0   2   1   3\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:182:1841 [4] NCCL INFO Trees [0] 3/-1/-1->0->-1 [1] -1/-1/-1->0->3 [2] 3/-1/-1->0->-1 [3] -1/-1/-1->0->3 [4] 3/-1/-1->0->-1 [5] -1/-1/-1->0->3 [6] 3/-1/-1->0->-1 [7] -1/-1/-1->0->3\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:186:1845 [6] NCCL INFO Trees [0] 1/-1/-1->2->3 [1] 3/-1/-1->2->1 [2] 1/-1/-1->2->3 [3] 3/-1/-1->2->1 [4] 1/-1/-1->2->3 [5] 3/-1/-1->2->1 [6] 1/-1/-1->2->3 [7] 3/-1/-1->2->1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:185:1843 [3] NCCL INFO Channel 00/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:184:1842 [1] NCCL INFO Channel 00/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:185:1843 [3] NCCL INFO Channel 03/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:175:1840 [0] NCCL INFO Channel 00/0 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:184:1842 [1] NCCL INFO Channel 02/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:185:1843 [3] NCCL INFO Channel 04/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:175:1840 [0] NCCL INFO Channel 04/0 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:176:1844 [2] NCCL INFO Channel 00/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:184:1842 [1] NCCL INFO Channel 04/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:185:1843 [3] NCCL INFO Channel 07/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:184:1842 [1] NCCL INFO Channel 06/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:176:1844 [2] NCCL INFO Channel 04/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:181:1846 [5] NCCL INFO Channel 00/0 : 1[1c0] -> 2[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:183:1847 [7] NCCL INFO Channel 00/0 : 3[1e0] -> 0[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:181:1846 [5] NCCL INFO Channel 02/0 : 1[1c0] -> 2[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:183:1847 [7] NCCL INFO Channel 03/0 : 3[1e0] -> 0[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:182:1841 [4] NCCL INFO Channel 00/0 : 0[1b0] -> 1[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:181:1846 [5] NCCL INFO Channel 04/0 : 1[1c0] -> 2[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:183:1847 [7] NCCL INFO Channel 04/0 : 3[1e0] -> 0[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:186:1845 [6] NCCL INFO Channel 00/0 : 2[1d0] -> 3[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:182:1841 [4] NCCL INFO Channel 04/0 : 0[1b0] -> 1[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:181:1846 [5] NCCL INFO Channel 06/0 : 1[1c0] -> 2[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:183:1847 [7] NCCL INFO Channel 07/0 : 3[1e0] -> 0[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:186:1845 [6] NCCL INFO Channel 04/0 : 2[1d0] -> 3[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:185:1843 [3] NCCL INFO Channel 02/0 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:176:1844 [2] NCCL INFO Channel 02/0 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:184:1842 [1] NCCL INFO Channel 03/0 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:176:1844 [2] NCCL INFO Channel 06/0 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:185:1843 [3] NCCL INFO Channel 06/0 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:184:1842 [1] NCCL INFO Channel 07/0 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:175:1840 [0] NCCL INFO Channel 03/0 : 0[170] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:175:1840 [0] NCCL INFO Channel 07/0 : 0[170] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:181:1846 [5] NCCL INFO Channel 03/0 : 1[1c0] -> 3[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:183:1847 [7] NCCL INFO Channel 02/0 : 3[1e0] -> 1[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:186:1845 [6] NCCL INFO Channel 02/0 : 2[1d0] -> 0[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:181:1846 [5] NCCL INFO Channel 07/0 : 1[1c0] -> 3[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:183:1847 [7] NCCL INFO Channel 06/0 : 3[1e0] -> 1[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:186:1845 [6] NCCL INFO Channel 06/0 : 2[1d0] -> 0[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:182:1841 [4] NCCL INFO Channel 03/0 : 0[1b0] -> 2[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:182:1841 [4] NCCL INFO Channel 07/0 : 0[1b0] -> 2[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:176:1844 [2] NCCL INFO Channel 01/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:175:1840 [0] NCCL INFO Channel 01/0 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:185:1843 [3] NCCL INFO Channel 01/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:184:1842 [1] NCCL INFO Channel 01/0 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:176:1844 [2] NCCL INFO Channel 03/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:175:1840 [0] NCCL INFO Channel 02/0 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:185:1843 [3] NCCL INFO Channel 05/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:176:1844 [2] NCCL INFO Channel 05/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:184:1842 [1] NCCL INFO Channel 05/0 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:175:1840 [0] NCCL INFO Channel 05/0 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:176:1844 [2] NCCL INFO Channel 07/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:175:1840 [0] NCCL INFO Channel 06/0 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:186:1845 [6] NCCL INFO Channel 01/0 : 2[1d0] -> 1[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:182:1841 [4] NCCL INFO Channel 01/0 : 0[1b0] -> 3[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:183:1847 [7] NCCL INFO Channel 01/0 : 3[1e0] -> 2[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:181:1846 [5] NCCL INFO Channel 01/0 : 1[1c0] -> 0[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:186:1845 [6] NCCL INFO Channel 03/0 : 2[1d0] -> 1[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:182:1841 [4] NCCL INFO Channel 02/0 : 0[1b0] -> 3[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:183:1847 [7] NCCL INFO Channel 05/0 : 3[1e0] -> 2[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:181:1846 [5] NCCL INFO Channel 05/0 : 1[1c0] -> 0[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:186:1845 [6] NCCL INFO Channel 05/0 : 2[1d0] -> 1[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:182:1841 [4] NCCL INFO Channel 05/0 : 0[1b0] -> 3[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:186:1845 [6] NCCL INFO Channel 07/0 : 2[1d0] -> 1[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:182:1841 [4] NCCL INFO Channel 06/0 : 0[1b0] -> 3[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:175:1840 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:185:1843 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:184:1842 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:176:1844 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:184:1842 [1] NCCL INFO Channel 01/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:184:1842 [1] NCCL INFO Channel 03/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:184:1842 [1] NCCL INFO Channel 05/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:184:1842 [1] NCCL INFO Channel 07/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:183:1847 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:176:1844 [2] NCCL INFO Channel 01/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:182:1841 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:181:1846 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:181:1846 [5] NCCL INFO Channel 01/0 : 1[1c0] -> 2[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:186:1845 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:176:1844 [2] NCCL INFO Channel 02/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:181:1846 [5] NCCL INFO Channel 03/0 : 1[1c0] -> 2[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:185:1843 [3] NCCL INFO Channel 01/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:176:1844 [2] NCCL INFO Channel 03/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:181:1846 [5] NCCL INFO Channel 05/0 : 1[1c0] -> 2[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:185:1843 [3] NCCL INFO Channel 02/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:176:1844 [2] NCCL INFO Channel 05/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:181:1846 [5] NCCL INFO Channel 07/0 : 1[1c0] -> 2[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:185:1843 [3] NCCL INFO Channel 05/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:176:1844 [2] NCCL INFO Channel 06/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:186:1845 [6] NCCL INFO Channel 01/0 : 2[1d0] -> 3[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:185:1843 [3] NCCL INFO Channel 06/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:176:1844 [2] NCCL INFO Channel 07/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:183:1847 [7] NCCL INFO Channel 01/0 : 3[1e0] -> 0[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:186:1845 [6] NCCL INFO Channel 02/0 : 2[1d0] -> 3[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:183:1847 [7] NCCL INFO Channel 02/0 : 3[1e0] -> 0[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:186:1845 [6] NCCL INFO Channel 03/0 : 2[1d0] -> 3[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:175:1840 [0] NCCL INFO Channel 00/0 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:183:1847 [7] NCCL INFO Channel 05/0 : 3[1e0] -> 0[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:186:1845 [6] NCCL INFO Channel 05/0 : 2[1d0] -> 3[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:183:1847 [7] NCCL INFO Channel 06/0 : 3[1e0] -> 0[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:175:1840 [0] NCCL INFO Channel 03/0 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:186:1845 [6] NCCL INFO Channel 06/0 : 2[1d0] -> 3[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:175:1840 [0] NCCL INFO Channel 04/0 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:186:1845 [6] NCCL INFO Channel 07/0 : 2[1d0] -> 3[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:175:1840 [0] NCCL INFO Channel 07/0 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:182:1841 [4] NCCL INFO Channel 00/0 : 0[1b0] -> 3[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:182:1841 [4] NCCL INFO Channel 03/0 : 0[1b0] -> 3[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:182:1841 [4] NCCL INFO Channel 04/0 : 0[1b0] -> 3[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:182:1841 [4] NCCL INFO Channel 07/0 : 0[1b0] -> 3[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:185:1843 [3] NCCL INFO Channel 00/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:185:1843 [3] NCCL INFO Channel 02/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:185:1843 [3] NCCL INFO Channel 03/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:176:1844 [2] NCCL INFO Channel 00/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:185:1843 [3] NCCL INFO Channel 04/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:176:1844 [2] NCCL INFO Channel 02/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:176:1844 [2] NCCL INFO Channel 04/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:185:1843 [3] NCCL INFO Channel 06/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:183:1847 [7] NCCL INFO Channel 00/0 : 3[1e0] -> 2[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:176:1844 [2] NCCL INFO Channel 06/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:185:1843 [3] NCCL INFO Channel 07/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:183:1847 [7] NCCL INFO Channel 02/0 : 3[1e0] -> 2[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:183:1847 [7] NCCL INFO Channel 03/0 : 3[1e0] -> 2[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:186:1845 [6] NCCL INFO Channel 00/0 : 2[1d0] -> 1[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:183:1847 [7] NCCL INFO Channel 04/0 : 3[1e0] -> 2[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:186:1845 [6] NCCL INFO Channel 02/0 : 2[1d0] -> 1[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:183:1847 [7] NCCL INFO Channel 06/0 : 3[1e0] -> 2[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:184:1842 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:184:1842 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:184:1842 [1] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:175:1840 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:175:1840 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:175:1840 [0] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:186:1845 [6] NCCL INFO Channel 04/0 : 2[1d0] -> 1[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:183:1847 [7] NCCL INFO Channel 07/0 : 3[1e0] -> 2[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:186:1845 [6] NCCL INFO Channel 06/0 : 2[1d0] -> 1[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:176:1844 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:176:1844 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:176:1844 [2] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:185:1843 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:185:1843 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:185:1843 [3] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:182:1841 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:182:1841 [4] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:182:1841 [4] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:181:1846 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:181:1846 [5] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:181:1846 [5] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:183:1847 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:183:1847 [7] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:183:1847 [7] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:186:1845 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:186:1845 [6] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:186:1845 [6] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:176:1844 [2] NCCL INFO comm 0x55aaffe002e0 rank 2 nranks 4 cudaDev 2 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:185:1843 [3] NCCL INFO comm 0x5560c3785510 rank 3 nranks 4 cudaDev 3 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:175:1840 [0] NCCL INFO comm 0x564f1380b2a0 rank 0 nranks 4 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:184:1842 [1] NCCL INFO comm 0x56153ae8a790 rank 1 nranks 4 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:183:1847 [7] NCCL INFO comm 0x55e279103eb0 rank 3 nranks 4 cudaDev 7 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:11.348: I smdistributed/modelparallel/torch/model.py:751] Broadcasted parameters and buffers for partition 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:181:1846 [5] NCCL INFO comm 0x55ccda7ae4b0 rank 1 nranks 4 cudaDev 5 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:186:1845 [6] NCCL INFO comm 0x561dc314f920 rank 2 nranks 4 cudaDev 6 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:182:1841 [4] NCCL INFO comm 0x55ff41dd8f40 rank 0 nranks 4 cudaDev 4 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-09-22 20:35:11.356: I smdistributed/modelparallel/torch/model.py:751] Broadcasted parameters and buffers for partition 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:11.613: C smdistributed/modelparallel/torch/worker.py:110] [0] Hit an exception for 0/0 on thread 0: CUDA out of memory. Tried to allocate 1.50 GiB (GPU 0; 15.77 GiB total capacity; 12.88 GiB already allocated; 1.42 GiB free; 13.05 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:11.616: C smdistributed/modelparallel/torch/worker.py:115] [0]   File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/worker.py\", line 508, in _thread_compute\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    self.thread_execute_step(req)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/worker.py\", line 318, in thread_execute_step\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    self._exec_step_on_device(req, device)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/worker.py\", line 286, in _exec_step_on_device\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    outputs = step_fn(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer_pt_utils.py\", line 1073, in smp_forward_backward\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    outputs = model(**inputs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 396, in distributed_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    outputs = module.execute_locally(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 338, in execute_module\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    outputs = actual_forward(module, *args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 276, in actual_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    outputs = module._orig_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/model.py\", line 1351, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    return self.module(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 396, in distributed_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    outputs = module.execute_locally(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 338, in execute_module\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    outputs = actual_forward(module, *args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 276, in actual_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    outputs = module._orig_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/ddp_model.py\", line 440, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    return self.module(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 396, in distributed_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    outputs = module.execute_locally(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 338, in execute_module\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    outputs = actual_forward(module, *args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 276, in actual_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    outputs = module._orig_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/peft/peft_model.py\", line 736, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    return self.base_model(\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 759, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    distilbert_output = self.distilbert(\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 396, in distributed_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    outputs = module.execute_locally(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 338, in execute_module\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    outputs = actual_forward(module, *args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"[1,mpirank:0,algo-1]<stdout>:/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 276, in actual_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    outputs = module._orig_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 579, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    return self.transformer(\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 396, in distributed_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    outputs = module.execute_locally(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 338, in execute_module\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    outputs = actual_forward(module, *args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 276, in actual_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    outputs = module._orig_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 355, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    layer_outputs = layer_module(\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 396, in distributed_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    outputs = module.execute_locally(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 338, in execute_module\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    outputs = actual_forward(module, *args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 276, in actual_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    outputs = module._orig_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 290, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    sa_output = self.attention(\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 396, in distributed_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    outputs = module.execute_locally(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 338, in execute_module\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    outputs = actual_forward(module, *args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 276, in actual_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    outputs = module._orig_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 215, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    scores = torch.matmul(q, k.transpose(2, 3))  # (bs, n_heads, q_length, k_length)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:11.616: C smdistributed/modelparallel/torch/worker.py:116] [0] Parent exec stack ['main', 'main/module', 'main/module/module', 'main/module/module/base_model/model/distilbert', 'main/module/module/base_model/model/distilbert/transformer', 'main/module/module/base_model/model/distilbert/transformer/layer/2', 'main/module/module/base_model/model/distilbert/transformer/layer/2/attention']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-09-22 20:35:11.617: C smdistributed/modelparallel/torch/worker.py:117] [0] Req <StepExecReq::mb:0, requester:0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:12.489: C smdistributed/modelparallel/torch/worker.py:110] [3] Hit an exception for 0/0 on thread 0: CUDA out of memory. Tried to allocate 1.50 GiB (GPU 3; 15.77 GiB total capacity; 12.88 GiB already allocated; 1.38 GiB free; 13.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:12.490: C smdistributed/modelparallel/torch/worker.py:110] [2] Hit an exception for 0/0 on thread 0: CUDA out of memory. Tried to allocate 1.50 GiB (GPU 2; 15.77 GiB total capacity; 12.88 GiB already allocated; 1.38 GiB free; 13.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:12.493: C smdistributed/modelparallel/torch/worker.py:115] [3]   File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/worker.py\", line 508, in _thread_compute\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    self.thread_execute_step(req)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/worker.py\", line 318, in thread_execute_step\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    self._exec_step_on_device(req, device)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/worker.py\", line 286, in _exec_step_on_device\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    outputs = step_fn(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer_pt_utils.py\", line 1073, in smp_forward_backward\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    outputs = model(**inputs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 396, in distributed_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    outputs = module.execute_locally(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 338, in execute_module\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    outputs = actual_forward(module, *args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 276, in actual_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    outputs = module._orig_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/model.py\", line 1351, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    return self.module(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 396, in distributed_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    outputs = module.execute_locally(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 338, in execute_module\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    outputs = actual_forward(module, *args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 276, in actual_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    outputs = module._orig_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/ddp_model.py\", line 440, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    return self.module(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 396, in distributed_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    outputs = module.execute_locally(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 338, in execute_module\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    outputs = actual_forward(module, *args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 276, in actual_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    outputs = module._orig_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/peft/peft_model.py\", line 736, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    return self.base_model(\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 759, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    distilbert_output = self.distilbert(\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 396, in distributed_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    outputs = module.execute_locally(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 338, in execute_module\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    outputs = actual_forward(module, *args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 276, in actual_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    outputs = module._orig_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 579, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    return self.transformer(\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 396, in distributed_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    outputs = module.execute_locally(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 338, in execute_module\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    outputs = actual_forward(module, *args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 276, in actual_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    outputs = module._orig_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 355, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    layer_outputs = layer_module(\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 396, in distributed_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    outputs = module.execute_locally(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 338, in execute_module\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    outputs = actual_forward(module, *args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 276, in actual_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    outputs = module._orig_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 290, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    sa_output = self.attention(\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 396, in distributed_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    outputs = module.execute_locally(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 338, in execute_module\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    outputs = actual_forward(module, *args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 276, in actual_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    outputs = module._orig_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 215, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    scores = torch.matmul(q, k.transpose(2, 3))  # (bs, n_heads, q_length, k_length)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:12.493: C smdistributed/modelparallel/torch/worker.py:116] [3] Parent exec stack ['main', 'main/module', 'main/module/module', 'main/module/module/base_model/model/distilbert', 'main/module/module/base_model/model/distilbert/transformer', 'main/module/module/base_model/model/distilbert/transformer/layer/2', 'main/module/module/base_model/model/distilbert/transformer/layer/2/attention']\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-09-22 20:35:12.493: C smdistributed/modelparallel/torch/worker.py:117] [3] Req <StepExecReq::mb:0, requester:0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:12.493: C smdistributed/modelparallel/torch/worker.py:115] [2]   File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/worker.py\", line 508, in _thread_compute\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    self.thread_execute_step(req)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/worker.py\", line 318, in thread_execute_step\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    self._exec_step_on_device(req, device)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/worker.py\", line 286, in _exec_step_on_device\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    outputs = step_fn(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer_pt_utils.py\", line 1073, in smp_forward_backward\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    outputs = model(**inputs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 396, in distributed_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    outputs = module.execute_locally(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 338, in execute_module\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    outputs = actual_forward(module, *args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 276, in actual_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    outputs = module._orig_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/model.py\", line 1351, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    return self.module(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 396, in distributed_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    outputs = module.execute_locally(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 338, in execute_module\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    outputs = actual_forward(module, *args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 276, in actual_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    outputs = module._orig_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/ddp_model.py\", line 440, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    return self.module(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 396, in distributed_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    outputs = module.execute_locally(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 338, in execute_module\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    outputs = actual_forward(module, *args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 276, in actual_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    outputs = module._orig_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/peft/peft_model.py\", line 736, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    return self.base_model(\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 759, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    distilbert_output = self.distilbert(\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 396, in distributed_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    outputs = module.execute_locally(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 338, in execute_module\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    outputs = actual_forward(module, *args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"[1,mpirank:2,algo-1]<stdout>:/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 276, in actual_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    outputs = module._orig_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 579, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    return self.transformer(\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 396, in distributed_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    outputs = module.execute_locally(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 338, in execute_module\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    outputs = actual_forward(module, *args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 276, in actual_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    outputs = module._orig_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 355, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    layer_outputs = layer_module(\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 396, in distributed_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    outputs = module.execute_locally(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 338, in execute_module\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    outputs = actual_forward(module, *args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 276, in actual_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    outputs = module._orig_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 290, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    sa_output = self.attention(\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 396, in distributed_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    outputs = module.execute_locally(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 338, in execute_module\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    outputs = actual_forward(module, *args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 276, in actual_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    outputs = module._orig_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 215, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    scores = torch.matmul(q, k.transpose(2, 3))  # (bs, n_heads, q_length, k_length)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:12.493: C smdistributed/modelparallel/torch/worker.py:116] [2] Parent exec stack ['main', 'main/module', 'main/module/module', 'main/module/module/base_model/model/distilbert', 'main/module/module/base_model/model/distilbert/transformer', 'main/module/module/base_model/model/distilbert/transformer/layer/2', 'main/module/module/base_model/model/distilbert/transformer/layer/2/attention']\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-09-22 20:35:12.493: C smdistributed/modelparallel/torch/worker.py:117] [2] Req <StepExecReq::mb:0, requester:0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:12.496: C smdistributed/modelparallel/torch/worker.py:110] [1] Hit an exception for 0/0 on thread 0: CUDA out of memory. Tried to allocate 1.50 GiB (GPU 1; 15.77 GiB total capacity; 12.88 GiB already allocated; 1.45 GiB free; 13.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:12.499: C smdistributed/modelparallel/torch/worker.py:115] [1]   File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/worker.py\", line 508, in _thread_compute\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    self.thread_execute_step(req)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/worker.py\", line 318, in thread_execute_step\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    self._exec_step_on_device(req, device)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/worker.py\", line 286, in _exec_step_on_device\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    outputs = step_fn(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer_pt_utils.py\", line 1073, in smp_forward_backward\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    outputs = model(**inputs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 396, in distributed_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    outputs = module.execute_locally(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 338, in execute_module\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    outputs = actual_forward(module, *args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 276, in actual_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    outputs = module._orig_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/model.py\", line 1351, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    return self.module(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 396, in distributed_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    outputs = module.execute_locally(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 338, in execute_module\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    outputs = actual_forward(module, *args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 276, in actual_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    outputs = module._orig_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/ddp_model.py\", line 440, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    return self.module(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 396, in distributed_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    outputs = module.execute_locally(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 338, in execute_module\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    outputs = actual_forward(module, *args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 276, in actual_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    outputs = module._orig_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/peft/peft_model.py\", line 736, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    return self.base_model(\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 759, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    distilbert_output = self.distilbert(\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 396, in distributed_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    outputs = module.execute_locally(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 338, in execute_module\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    outputs = actual_forward(module, *args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"[1,mpirank:1,algo-1]<stdout>:/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 276, in actual_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    outputs = module._orig_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 579, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    return self.transformer(\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 396, in distributed_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    outputs = module.execute_locally(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 338, in execute_module\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    outputs = actual_forward(module, *args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 276, in actual_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    outputs = module._orig_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 355, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    layer_outputs = layer_module(\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 396, in distributed_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    outputs = module.execute_locally(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 338, in execute_module\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    outputs = actual_forward(module, *args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 276, in actual_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    outputs = module._orig_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 290, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    sa_output = self.attention(\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 396, in distributed_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    outputs = module.execute_locally(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 338, in execute_module\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    outputs = actual_forward(module, *args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/patches/execution.py\", line 276, in actual_forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    outputs = module._orig_forward(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 215, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    scores = torch.matmul(q, k.transpose(2, 3))  # (bs, n_heads, q_length, k_length)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:12.499: C smdistributed/modelparallel/torch/worker.py:116] [1] Parent exec stack ['main', 'main/module', 'main/module/module', 'main/module/module/base_model/model/distilbert', 'main/module/module/base_model/model/distilbert/transformer', 'main/module/module/base_model/model/distilbert/transformer/layer/2', 'main/module/module/base_model/model/distilbert/transformer/layer/2/attention']\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-09-22 20:35:12.499: C smdistributed/modelparallel/torch/worker.py:117] [1] Req <StepExecReq::mb:0, requester:0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:╭───────────────────── Traceback (most recent call last) ──────────────────────╮\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/runpy.py:197 in _run_module_as_main                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   194 │   main_globals = sys.modules[\"__main__\"].__dict__                    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   195 │   if alter_argv:                                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   196 │   │   sys.argv[0] = mod_spec.origin                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱ 197 │   return _run_code(code, main_globals, None,                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   198 │   │   │   │   │    \"__main__\", mod_spec)                             │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   199                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   200 def run_module(mod_name, init_globals=None,                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/runpy.py:87 in _run_code                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    84 │   │   │   │   │      __loader__ = loader,                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    85 │   │   │   │   │      __package__ = pkg_name,                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    86 │   │   │   │   │      __spec__ = mod_spec)                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱  87 │   exec(code, run_globals)                                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    88 │   return run_globals                                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    89                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    90 def _run_module_code(code, init_globals=None,                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/mpi4py/__main__.py:7 in <module>      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   4 from .run import main                                                    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   5                                                                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   6 if __name__ == '__main__':                                               │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱ 7 │   main()                                                               │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   8                                                                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/mpi4py/run.py:198 in main             │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   195 │   # Run user code. In case of an unhandled exception, abort          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   196 │   # execution of the MPI program by calling 'MPI_Abort()'.           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   197 │   try:                                                               │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱ 198 │   │   run_command_line(args)                                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   199 │   except SystemExit as exc:                                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   200 │   │   set_abort_status(exc.code)                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   201 │   │   raise                                                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/mpi4py/run.py:47 in run_command_line  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    44 │   │   from os.path import realpath, dirname                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    45 │   │   if not getattr(sys.flags, 'isolated', 0):  # pragma: no branch │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    46 │   │   │   sys.path[0] = realpath(dirname(sys.argv[0]))  # Fix sys.pa │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱  47 │   │   run_path(sys.argv[0], run_name='__main__')                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    48                                                                        [1,mpirank:0,algo-1]<stderr>:│\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    49                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    50 def set_abort_status(status):                                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/runpy.py:288 in run_path                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   285 │   │   # Not a valid sys.path entry, so run the code directly         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   286 │   │   # execfile() doesn't help as we want to allow compiled files   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   287 │   │   code, fname = _get_code_from_file(run_name, path_name)         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱ 288 │   │   return _run_module_code(code, init_globals, run_name,          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   289 │   │   │   │   │   │   │   │   pkg_name=pkg_name, script_name=fname)  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   290 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   291 │   │   # Finder is defined for path, so add it to                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/runpy.py:97 in _run_module_code                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    94 │   fname = script_name if mod_spec is None else mod_spec.origin       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    95 │   with _TempModule(mod_name) as temp_module, _ModifiedArgv0(fname):  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    96 │   │   mod_globals = temp_module.module.__dict__                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱  97 │   │   _run_code(code, mod_globals, init_globals,                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    98 │   │   │   │     mod_name, mod_spec, pkg_name, script_name)           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    99 │   # Copy the globals of the temporary module, as they                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   100 │   # may be cleared when the temporary module goes away               │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/runpy.py:87 in _run_code                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    84 │   │   │   │   │      __loader__ = loader,                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    85 │   │   │   │   │      __package__ = pkg_name,                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    86 │   │   │   │   │      __spec__ = mod_spec)                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱  87 │   exec(code, run_globals)                                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    88 │   return run_globals                                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    89                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    90 def _run_module_code(code, init_globals=None,                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/ml/code/train-LORA.py:158 in <module>                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   155 │   import time                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   156 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   157 │   start_time = time.time()                                           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱ 158 │   trainer.train()                                                    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   159 │   print(\"EXEC TIME:\", time.time() - start_time)                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   160 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   161 │   # evaluate model                                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1543 in train │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1540 │   │   inner_training_loop = find_executable_batch_size(             │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1541 │   │   │   self._inner_training_loop, self[1,mpirank:0,algo-1]<stderr>:._train_batch_size, args.a │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1542 │   │   )                                                             │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱ 1543 │   │   return inner_training_loop(                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1544 │   │   │   args=args,                                                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1545 │   │   │   resume_from_checkpoint=resume_from_checkpoint,            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1546 │   │   │   trial=trial,                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1791 in       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ _inner_training_loop                                                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1788 │   │   │   │   │   with model.no_sync():                             │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1789 │   │   │   │   │   │   tr_loss_step = self.training_step(model, inpu │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1790 │   │   │   │   else:                                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱ 1791 │   │   │   │   │   tr_loss_step = self.training_step(model, inputs)  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1792 │   │   │   │                                                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1793 │   │   │   │   if (                                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1794 │   │   │   │   │   args.logging_nan_inf_filter                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/transformers/trainer.py:2535 in       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ training_step                                                                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   2532 │   │   inputs = self._prepare_inputs(inputs)                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   2533 │   │                                                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   2534 │   │   if is_sagemaker_mp_enabled():                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱ 2535 │   │   │   loss_mb = smp_forward_backward(model, inputs, self.args.g │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   2536 │   │   │   return loss_mb.reduce_mean().detach().to(self.args.device │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   2537 │   │                                                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   2538 │   │   with self.compute_loss_context_manager():                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/ste │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ p.py:261 in __call__                                                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   258 │   │   │   │   │   args, kwargs, state.num_microbatches()             │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   259 │   │   │   │   )                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   260 │   │   │   │   state.pipeline.init_step()                             │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱ 261 │   │   │   │   state.exec_server.run_step_leader(mb_args, mb_kwargs,  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   262 │   │   │   else:                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   263 │   │   │   │   state.exec_server.run_step_follower()                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   264                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/ser │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ver.py:402 in run_step_leader                                                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   399 │   │   │   │   │   │   )                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   400 │   │   │   │   │   │   state.core.timeline_record_pipeline_event(req. │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   401 │   │   │   │   │   │   state.pipeline.promote_status(mb)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱ 402 │   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   │   │   │   self.execute_request(req)                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   403 │   │   │   │   │   │   state.core.timeline_record_pipeline_event(req. │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   404 │   │   │   │   │   elif mb_status == MbStatus.READY_FOR_BWD:          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   405 │   │   │   │   │   │   req = self._get_pending_wait_for_req(mb, WaitF │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/ser │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ver.py:119 in execute_request                                                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   116 │   │   │   self._workers[new_worker.id] = new_worker                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   117 │   │   │   chosen_worker = new_worker                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   118 │   │                                                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱ 119 │   │   chosen_worker.execute(req)                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   120 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   121 │   def _get_pending_wait_for_req(self, mb, req_type):                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   122 │   │   # hacky, todo clean this up                                    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/wor │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ker.py:155 in execute                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   152 │   │   # pass the request                                             │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   153 │   │   self.comm_queue.put(req)                                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   154 │   │   state.switching_to_worker(self.id, self.req)                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱ 155 │   │   self._resume_thread_common()                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   156 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   157 │   def resume(self, result: ForwardExecutionResult):                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   158 │   │   \"\"\"                                                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/wor │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ker.py:186 in _resume_thread_common                                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   183 │   │   self.status = WorkerExecStatus.EXECUTING                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   184 │   │   self.acquire_and_notify(ThreadName.WORKER)                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   185 │   │   self.acquire_and_wait(ThreadName.SERVER)                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱ 186 │   │   self._check_queue_after_thread_return()                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   187 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   188 │   def thread_get_forward_result(self, request: ExecutionRequest):    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   189 │   │   \"\"\"                                                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/wor │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ker.py:121 in _check_queue_after_thread_return                               │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   118 │   │   │   raise self.exception                                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   119 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   120 │   def _check_queue_after_thread_return(self):                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱ 121 │   │   self._check_exception()                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   122 │   │   r = self.comm_queue.get()                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   123 │   │   if isinstance(r, ExecutionRequest):   [1,mpirank:0,algo-1]<stderr>:                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   124 │   │   │   state.exec_server.process_request(r)                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/wor │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ker.py:118 in _check_exception                                               │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   115 │   │   │   logger.fatal(rmsg(f\"{''.join(traceback.format_tb(self.exce │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   116 │   │   │   logger.fatal(rmsg(f\"Parent exec stack {state.module_manage │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   117 │   │   │   logger.fatal(rmsg(f\"Req {self.req}\"))                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱ 118 │   │   │   raise self.exception                                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   119 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   120 │   def _check_queue_after_thread_return(self):                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   121 │   │   self._check_exception()                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/wor │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ker.py:508 in _thread_compute                                                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   505 │   │   │   │   │   if isinstance(req, TraceStepExecutionRequest):     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   506 │   │   │   │   │   │   self.thread_execute_tracing(req)               │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   507 │   │   │   │   │   elif isinstance(req, StepExecutionRequest):        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱ 508 │   │   │   │   │   │   self.thread_execute_step(req)                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   509 │   │   │   │   │   elif (                                             │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   510 │   │   │   │   │   │   isinstance(req, (ModuleExecutionRequest, Seque │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   511 │   │   │   │   │   │   and req.phase == MbStatus.FWD                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/wor │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ker.py:318 in thread_execute_step                                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   315 │   │   for the server.                                                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   316 │   │   \"\"\"                                                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   317 │   │   device = torch.device(\"cuda\", local_rank())                    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱ 318 │   │   self._exec_step_on_device(req, device)                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   319 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   320 │   def thread_execute_forward(                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   321 │   │   self, req: Union[ModuleExecutionRequest, SequentialModulesExec │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/wor │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ker.py:286 in _exec_step_on_device                                           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   283 │   │   \"\"\"                                                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   284 │   │   args, kwargs = convert_args_to_device(req.args, req.kwargs, de │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   285 │   │   step_fn = state.step_func[req.step_fn_id].func                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱ 286 │   │   outputs = step_fn(*args, **kwargs)                             │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   287 │   │   self.comm_queue.put(ForwardExecutionResult(self.req, outputs)) │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   288 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   289 │   def thread_execute_tracing(self, req: TraceStepExecutionRequest):  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                   [1,mpirank:0,algo-1]<stderr>:           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/transformers/trainer_pt_utils.py:1073 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ in smp_forward_backward                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1070 │                                                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1071 │   @smp.step()                                                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1072 │   def smp_forward_backward(model, inputs, gradient_accumulation_ste │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱ 1073 │   │   outputs = model(**inputs)                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1074 │   │   loss = outputs[\"loss\"] if isinstance(outputs, dict) else outp │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1075 │   │   loss /= gradient_accumulation_steps                           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1076 │   │   model.backward(loss)                                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1194 in    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ _call_impl                                                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1191 │   │   # this function, and just call forward.                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1192 │   │   if not (self._backward_hooks or self._forward_hooks or self._ │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1193 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱ 1194 │   │   │   return forward_call(*input, **kwargs)                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1195 │   │   # Do not call functions when jit is used                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1196 │   │   full_backward_hooks, non_full_backward_hooks = [], []         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1197 │   │   if self._backward_hooks or _global_backward_hooks:            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ches/execution.py:396 in distributed_forward                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   393 │   # When a module has no parameter, run as no partition. It's to res │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   394 │   # used in a multiple parent modules in which some parent module ha │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   395 │   if state.module_manager.is_executor(module) or num_params == 0:    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱ 396 │   │   outputs = module.execute_locally(*args, **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   397 │   elif state.module_manager.is_parent_executor(module):              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   398 │   │   outputs = execute_as_parent(module, *args, **kwargs)           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   399 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ches/execution.py:338 in execute_module                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   335 │   │   │   preserve_rng_state=config.preserve_rng_state,              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   336 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   337 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱ 338 │   │   outputs = actual_forward(module, *args, **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   339 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   340 │   if not state.is_in_fwd_on_checkpointed_fn:                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   341 │   │   module_name = state.module_manager.get_module_name(module)     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ches/execution.py:276 in actual_forward                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   273 │   │   │   *tensors,                                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   274 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   275 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱ 276 │   │   outputs = module._orig_forward(*args, **kwargs)                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   277 │   return outputs                                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   278                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   279                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/mod │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ el.py:1351 in forward                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1348 │   def forward(self, *args, **kwargs):                               │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1349 │   │   if state.cfg.pipeline_parallel_degree > 1 and not state.in_st │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1350 │   │   │   raise FwdExecutionNotInStepFnError                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱ 1351 │   │   return self.module(*args, **kwargs)                           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1352 │                                                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1353 │   @property                                                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1354 │   def partitioned(self):                                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1194 in    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ _call_impl                                                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1191 │   │   # this function, and just call forward.                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1192 │   │   if not (self._backward_hooks or self._forward_hooks or self._ │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1193 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱ 1194 │   │   │   return forward_call(*input, **kwargs)                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1195 │   │   # Do not call functions when jit is used                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1196 │   │   full_backward_hooks, non_full_backward_hooks = [], []         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1197 │   │   if self._backward_hooks or _global_backward_hooks:            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ches/execution.py:396 in distributed_forward                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   393 │   # When a module has no parameter, run as no partition. It's to res │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   394 │   # used in a multiple parent modules in which some parent module ha │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   395 │   if state.module_manager.is_executor(module) or num_params == 0:    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱ 396 │   │   outputs = module.execute_locally(*args, **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   397 │   elif state.module_manager.is_parent_executor(module):              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   398 │   │   outputs = execute_as_parent(module, *args, **kwargs)           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   399 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ches/execution.py:338 in execute_module                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   335 │   │   │   preserve_rng_state=config.preserve_rng_state,              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   336 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   337 │   else:                                   [1,mpirank:0,algo-1]<stderr>:                           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱ 338 │   │   outputs = actual_forward(module, *args, **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   339 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   340 │   if not state.is_in_fwd_on_checkpointed_fn:                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   341 │   │   module_name = state.module_manager.get_module_name(module)     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ches/execution.py:276 in actual_forward                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   273 │   │   │   *tensors,                                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   274 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   275 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱ 276 │   │   outputs = module._orig_forward(*args, **kwargs)                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   277 │   return outputs                                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   278                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   279                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/ddp │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ _model.py:440 in forward                                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   437 │   # -----                                                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   438 │   # Redirecting these methods so the extra wrapper is all hidden fro │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   439 │   def forward(self, *args, **kwargs):                                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱ 440 │   │   return self.module(*args, **kwargs)                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   441 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   442 │   def named_parameters(self, *args, **kwargs):                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   443 │   │   return self.module.named_parameters(*args, **kwargs)           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1194 in    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ _call_impl                                                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1191 │   │   # this function, and just call forward.                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1192 │   │   if not (self._backward_hooks or self._forward_hooks or self._ │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1193 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱ 1194 │   │   │   return forward_call(*input, **kwargs)                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1195 │   │   # Do not call functions when jit is used                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1196 │   │   full_backward_hooks, non_full_backward_hooks = [], []         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1197 │   │   if self._backward_hooks or _global_backward_hooks:            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ches/execution.py:396 in distributed_forward                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   393 │   # When a module has no parameter, run as no partition. It's to res │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   394 │   # used in a multiple parent modules in which some parent module ha │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   395 │   if state.module_manager.is_executor(module) or num_params == 0:    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱ 396 │   │   outputs = module.execute_locally(*args, **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   397 │   elif state.module_manager.is_parent_executor(module):              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   398 │   │   outputs = execute_as_parent(m[1,mpirank:0,algo-1]<stderr>:odule, *args, **kwargs)           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   399 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ches/execution.py:338 in execute_module                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   335 │   │   │   preserve_rng_state=config.preserve_rng_state,              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   336 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   337 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱ 338 │   │   outputs = actual_forward(module, *args, **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   339 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   340 │   if not state.is_in_fwd_on_checkpointed_fn:                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   341 │   │   module_name = state.module_manager.get_module_name(module)     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ches/execution.py:276 in actual_forward                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   273 │   │   │   *tensors,                                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   274 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   275 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱ 276 │   │   outputs = module._orig_forward(*args, **kwargs)                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   277 │   return outputs                                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   278                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   279                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/peft/peft_model.py:736 in forward     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    733 │   │   return_dict = return_dict if return_dict is not None else sel │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    734 │   │   peft_config = self.active_peft_config                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    735 │   │   if not isinstance(peft_config, PromptLearningConfig):         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱  736 │   │   │   return self.base_model(                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    737 │   │   │   │   input_ids=input_ids,                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    738 │   │   │   │   attention_mask=attention_mask,                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    739 │   │   │   │   inputs_embeds=inputs_embeds,                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1194 in    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ _call_impl                                                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1191 │   │   # this function, and just call forward.                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1192 │   │   if not (self._backward_hooks or self._forward_hooks or self._ │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1193 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱ 1194 │   │   │   return forward_call(*input, **kwargs)                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1195 │   │   # Do not call functions when jit is used                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1196 │   │   full_backward_hooks, non_full_backward_hooks = [], []         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1197 │   │   if self._backward_hooks or _global_backward_hooks:            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeli │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ng_distilbert.py[1,mpirank:0,algo-1]<stderr>::759 in forward                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    756 │   │   \"\"\"                                                           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    757 │   │   return_dict = return_dict if return_dict is not None else sel │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    758 │   │                                                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱  759 │   │   distilbert_output = self.distilbert(                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    760 │   │   │   input_ids=input_ids,                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    761 │   │   │   attention_mask=attention_mask,                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    762 │   │   │   head_mask=head_mask,                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1194 in    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ _call_impl                                                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1191 │   │   # this function, and just call forward.                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1192 │   │   if not (self._backward_hooks or self._forward_hooks or self._ │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1193 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱ 1194 │   │   │   return forward_call(*input, **kwargs)                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1195 │   │   # Do not call functions when jit is used                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1196 │   │   full_backward_hooks, non_full_backward_hooks = [], []         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1197 │   │   if self._backward_hooks or _global_backward_hooks:            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ches/execution.py:396 in distributed_forward                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   393 │   # When a module has no parameter, run as no partition. It's to res │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   394 │   # used in a multiple parent modules in which some parent module ha │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   395 │   if state.module_manager.is_executor(module) or num_params == 0:    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱ 396 │   │   outputs = module.execute_locally(*args, **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   397 │   elif state.module_manager.is_parent_executor(module):              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   398 │   │   outputs = execute_as_parent(module, *args, **kwargs)           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   399 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ches/execution.py:338 in execute_module                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   335 │   │   │   preserve_rng_state=config.preserve_rng_state,              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   336 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   337 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱ 338 │   │   outputs = actual_forward(module, *args, **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   339 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   340 │   if not state.is_in_fwd_on_checkpointed_fn:                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   341 │   │   module_name = state.module_manager.get_module_name(module)     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ches/execution.py:276 in actual_forward                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   273 │   │   │   *tensors,                                                  �[1,mpirank:0,algo-1]<stderr>:��\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   274 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   275 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱ 276 │   │   outputs = module._orig_forward(*args, **kwargs)                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   277 │   return outputs                                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   278                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   279                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeli │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ng_distilbert.py:579 in forward                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    576 │   │                                                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    577 │   │   if inputs_embeds is None:                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    578 │   │   │   inputs_embeds = self.embeddings(input_ids)  # (bs, seq_le │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱  579 │   │   return self.transformer(                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    580 │   │   │   x=inputs_embeds,                                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    581 │   │   │   attn_mask=attention_mask,                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    582 │   │   │   head_mask=head_mask,                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1194 in    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ _call_impl                                                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1191 │   │   # this function, and just call forward.                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1192 │   │   if not (self._backward_hooks or self._forward_hooks or self._ │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1193 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱ 1194 │   │   │   return forward_call(*input, **kwargs)                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1195 │   │   # Do not call functions when jit is used                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1196 │   │   full_backward_hooks, non_full_backward_hooks = [], []         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1197 │   │   if self._backward_hooks or _global_backward_hooks:            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ches/execution.py:396 in distributed_forward                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   393 │   # When a module has no parameter, run as no partition. It's to res │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   394 │   # used in a multiple parent modules in which some parent module ha │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   395 │   if state.module_manager.is_executor(module) or num_params == 0:    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱ 396 │   │   outputs = module.execute_locally(*args, **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   397 │   elif state.module_manager.is_parent_executor(module):              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   398 │   │   outputs = execute_as_parent(module, *args, **kwargs)           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   399 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ches/execution.py:338 in execute_module                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   335 │   │   │   preserve_rng_state=config.preserve_rng_state,              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   336 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   337 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱ 338 │   │   outputs = actual_forward(module, *args,[1,mpirank:0,algo-1]<stderr>: **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   339 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   340 │   if not state.is_in_fwd_on_checkpointed_fn:                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   341 │   │   module_name = state.module_manager.get_module_name(module)     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ches/execution.py:276 in actual_forward                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   273 │   │   │   *tensors,                                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   274 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   275 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱ 276 │   │   outputs = module._orig_forward(*args, **kwargs)                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   277 │   return outputs                                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   278                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   279                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeli │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ng_distilbert.py:355 in forward                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    352 │   │   │   if output_hidden_states:                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    353 │   │   │   │   all_hidden_states = all_hidden_states + (hidden_state │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    354 │   │   │                                                             │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱  355 │   │   │   layer_outputs = layer_module(                             │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    356 │   │   │   │   x=hidden_state, attn_mask=attn_mask, head_mask=head_m │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    357 │   │   │   )                                                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    358 │   │   │   hidden_state = layer_outputs[-1]                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1194 in    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ _call_impl                                                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1191 │   │   # this function, and just call forward.                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1192 │   │   if not (self._backward_hooks or self._forward_hooks or self._ │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1193 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱ 1194 │   │   │   return forward_call(*input, **kwargs)                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1195 │   │   # Do not call functions when jit is used                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1196 │   │   full_backward_hooks, non_full_backward_hooks = [], []         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1197 │   │   if self._backward_hooks or _global_backward_hooks:            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ches/execution.py:396 in distributed_forward                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   393 │   # When a module has no parameter, run as no partition. It's to res │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   394 │   # used in a multiple parent modules in which some parent module ha │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   395 │   if state.module_manager.is_executor(module) or num_params == 0:    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱ 396 │   │   outputs = module.execute_locally(*args, **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   397 │   elif state.module_manager.is_parent_executor(module):              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   398 │   │   outputs = execute_as_parent(module, *args, **kwargs)           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   399 │   else:       [1,mpirank:0,algo-1]<stderr>:                                                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ches/execution.py:338 in execute_module                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   335 │   │   │   preserve_rng_state=config.preserve_rng_state,              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   336 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   337 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱ 338 │   │   outputs = actual_forward(module, *args, **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   339 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   340 │   if not state.is_in_fwd_on_checkpointed_fn:                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   341 │   │   module_name = state.module_manager.get_module_name(module)     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ches/execution.py:276 in actual_forward                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   273 │   │   │   *tensors,                                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   274 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   275 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱ 276 │   │   outputs = module._orig_forward(*args, **kwargs)                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   277 │   return outputs                                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   278                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   279                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeli │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ng_distilbert.py:290 in forward                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    287 │   │   │   torch.tensor(bs, seq_length, dim) The output of the trans │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    288 │   │   \"\"\"                                                           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    289 │   │   # Self-Attention                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱  290 │   │   sa_output = self.attention(                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    291 │   │   │   query=x,                                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    292 │   │   │   key=x,                                                    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    293 │   │   │   value=x,                                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1194 in    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ _call_impl                                                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1191 │   │   # this function, and just call forward.                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1192 │   │   if not (self._backward_hooks or self._forward_hooks or self._ │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1193 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱ 1194 │   │   │   return forward_call(*input, **kwargs)                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1195 │   │   # Do not call functions when jit is used                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1196 │   │   full_backward_hooks, non_full_backward_hooks = [], []         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   1197 │   │   if self._backward_hooks or _global_backward_hooks:            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ che[1,mpirank:0,algo-1]<stderr>:s/execution.py:396 in distributed_forward                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   393 │   # When a module has no parameter, run as no partition. It's to res │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   394 │   # used in a multiple parent modules in which some parent module ha │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   395 │   if state.module_manager.is_executor(module) or num_params == 0:    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱ 396 │   │   outputs = module.execute_locally(*args, **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   397 │   elif state.module_manager.is_parent_executor(module):              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   398 │   │   outputs = execute_as_parent(module, *args, **kwargs)           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   399 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ches/execution.py:338 in execute_module                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   335 │   │   │   preserve_rng_state=config.preserve_rng_state,              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   336 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   337 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱ 338 │   │   outputs = actual_forward(module, *args, **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   339 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   340 │   if not state.is_in_fwd_on_checkpointed_fn:                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   341 │   │   module_name = state.module_manager.get_module_name(module)     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ches/execution.py:276 in actual_forward                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   273 │   │   │   *tensors,                                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   274 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   275 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱ 276 │   │   outputs = module._orig_forward(*args, **kwargs)                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   277 │   return outputs                                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   278                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│   279                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeli │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ng_distilbert.py:215 in forward                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    212 │   │   v = shape(self.v_lin(value))  # (bs, n_heads, k_length, dim_p │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    213 │   │                                                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    214 │   │   q = q / math.sqrt(dim_per_head)  # (bs, n_heads, q_length, di │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│ ❱  215 │   │   scores = torch.matmul(q, k.transpose(2, 3))  # (bs, n_heads,  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    216 │   │   mask = (mask == 0).view(mask_reshp).expand_as(scores)  # (bs, │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    217 │   │   scores = scores.masked_fill(                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:│    218 │   │   │   mask, torch.tensor(torch.finfo(scores.dtype).min)         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:OutOfMemoryError: CUDA out of memory. Tried to allocate 1.50 GiB (GPU 0; 15.77 \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:GiB total capacity; 12.88 GiB already allocated; 1.42 GiB free; 13.05 GiB \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:reserved in total by PyTorch) If reserv[1,mpirank:0,algo-1]<stderr>:ed memory is >> allocated memory try \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:setting max_split_size_mb to avoid fragmentation.  See documentation for Memory \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:Management and PYTORCH_CUDA_ALLOC_CONF\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mPrimary job  terminated normally, but 1 process returned\u001b[0m\n",
      "\u001b[34ma non-zero exit code. Per user-direction, the job has been aborted.\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:╭───────────────────── Traceback (most recent call last) ──────────────────────╮\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/runpy.py:197 in _run_module_as_main                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   194 │   main_globals = sys.modules[\"__main__\"].__dict__                    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   195 │   if alter_argv:                                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   196 │   │   sys.argv[0] = mod_spec.origin                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱ 197 │   return _run_code(code, main_globals, None,                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   198 │   │   │   │   │    \"__main__\", mod_spec)                             │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   199                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   200 def run_module(mod_name, init_globals=None,                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/runpy.py:87 in _run_code                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    84 │   │   │   │   │      __loader__ = loader,                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    85 │   │   │   │   │      __package__ = pkg_name,                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    86 │   │   │   │   │      __spec__ = mod_spec)                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱  87 │   exec(code, run_globals)                                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    88 │   return run_globals                                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    89                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    90 def _run_module_code(code, init_globals=None,                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/mpi4py/__main__.py:7 in <module>      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   4 from .run import main                                                    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   5                                                                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   6 if __name__ == '__main__':                                               │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱ 7 │   main()                                                               │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   8                                                                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/mpi4py/run.py:198 in main             │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   195 │   # Run user code. In case of an unhandled exception, abort          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   196 │   # execution of the MPI program by calling 'MPI_Abort()'.           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   197 │   try:                                                               │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱ 198 │   │   run_command_line(args)                                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   199 │   except SystemExit as exc:                                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   200 │   │   set_abort_status(exc.code)                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   201 │   │   raise                                                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/mpi4py/run.py:47 in run_command_line  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    44 │   │   from os.path import realpath, dirname                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    45 │   │   if not getattr(sys.flags, 'isolated', 0):  # pragma: no branch │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    46 │   │   │   sys.path[0] = realpath(dirname(sys.argv[0]))  # Fix sys.pa │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱  47 │   │   run_path(sys.argv[0], run_name='__main__')                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    48                                                                        [1,mpirank:2,algo-1]<stderr>:╭───────────────────── Traceback (most recent call last) ──────────────────────╮\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/runpy.py:197 in _run_module_as_main                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   194 │   main_globals = sys.modules[\"__main__\"].__dict__                    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   195 │   if alter_argv:                                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   196 │   │   sys.argv[0] = mod_spec.origin                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱ 197 │   return _run_code(code, main_globals, None,                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   198 │   │   │   │   │    \"__main__\", mod_spec)                             │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   199                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   200 def run_module(mod_name, init_globals=None,                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/runpy.py:87 in _run_code                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    84 │   │   │   │   │      __loader__ = loader,                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    85 │   │   │   │   │      __package__ = pkg_name,                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    86 │   │   │   │   │      __spec__ = mod_spec)                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱  87 │   exec(code, run_globals)                                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    88 │   return run_globals                                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    89                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    90 def _run_module_code(code, init_globals=None,                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/mpi4py/__main__.py:7 in <module>      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   4 from .run import main                                                    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   5                                                                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   6 if __name__ == '__main__':                                               │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱ 7 │   main()                                                               │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   8                                                                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/mpi4py/run.py:198 in main             │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   195 │   # Run user code. In case of an unhandled exception, abort          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   196 │   # execution of the MPI program by calling 'MPI_Abort()'.           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   197 │   try:                                                               │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱ 198 │   │   run_command_line(args)                                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   199 │   except SystemExit as exc:                                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   200 │   │   set_abort_status(exc.code)                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   201 │   │   raise                                                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/mpi4py/run.py:47 in run_command_line  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    44 │   │   from os.path import realpath, dirname                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    45 │   │   if not getattr(sys.flags, 'isolated', 0):  # pragma: no branch │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    46 │   │   │   sys.path[0] = realpath(dirname(sys.argv[0]))  # Fix sys.pa │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱  47 │   │   run_path(sys.argv[0], run_name='__main__')                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    48                                                                        [1,mpirank:1,algo-1]<stderr>:╭───────────────────── Traceback (most recent call last) ──────────────────────╮\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/runpy.py:197 in _run_module_as_main                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   194 │   main_globals = sys.modules[\"__main__\"].__dict__                    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   195 │   if alter_argv:                                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   196 │   │   sys.argv[0] = mod_spec.origin                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱ 197 │   return _run_code(code, main_globals, None,                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   198 │   │   │   │   │    \"__main__\", mod_spec)                             │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   199                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   200 def run_module(mod_name, init_globals=None,                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/runpy.py:87 in _run_code                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    84 │   │   │   │   │      __loader__ = loader,                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    85 │   │   │   │   │      __package__ = pkg_name,                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    86 │   │   │   │   │      __spec__ = mod_spec)                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱  87 │   exec(code, run_globals)                                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    88 │   return run_globals                                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    89                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    90 def _run_module_code(code, init_globals=None,                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/mpi4py/__main__.py:7 in <module>      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   4 from .run import main                                                    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   5                                                                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   6 if __name__ == '__main__':                                               │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱ 7 │   main()                                                               │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   8                                                                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/mpi4py/run.py:198 in main             │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   195 │   # Run user code. In case of an unhandled exception, abort          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   196 │   # execution of the MPI program by calling 'MPI_Abort()'.           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   197 │   try:                                                               │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱ 198 │   │   run_command_line(args)                                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   199 │   except SystemExit as exc:                                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   200 │   │   set_abort_status(exc.code)                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   201 │   │   raise                                                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/mpi4py/run.py:47 in run_command_line  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    44 │   │   from os.path import realpath, dirname                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    45 │   │   if not getattr(sys.flags, 'isolated', 0):  # pragma: no branch │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    46 │   │   │   sys.path[0] = realpath(dirname(sys.argv[0]))  # Fix sys.pa │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱  47 │   │   run_path(sys.argv[0], run_name='__main__')                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    48                                                                        [1,mpirank:3,algo-1]<stderr>:│\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    49                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    50 def set_abort_status(status):                                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/runpy.py:288 in run_path                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   285 │   │   # Not a valid sys.path entry, so run the code directly         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   286 │   │   # execfile() doesn't help as we want to allow compiled files   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   287 │   │   code, fname = _get_code_from_file(run_name, path_name)         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱ 288 │   │   return _run_module_code(code, init_globals, run_name,          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   289 │   │   │   │   │   │   │   │   pkg_name=pkg_name, script_name=fname)  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   290 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   291 │   │   # Finder is defined for path, so add it to                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/runpy.py:97 in _run_module_code                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    94 │   fname = script_name if mod_spec is None else mod_spec.origin       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    95 │   with _TempModule(mod_name) as temp_module, _ModifiedArgv0(fname):  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    96 │   │   mod_globals = temp_module.module.__dict__                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱  97 │   │   _run_code(code, mod_globals, init_globals,                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    98 │   │   │   │     mod_name, mod_spec, pkg_name, script_name)           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    99 │   # Copy the globals of the temporary module, as they                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   100 │   # may be cleared when the temporary module goes away               │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/runpy.py:87 in _run_code                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    84 │   │   │   │   │      __loader__ = loader,                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    85 │   │   │   │   │      __package__ = pkg_name,                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    86 │   │   │   │   │      __spec__ = mod_spec)                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱  87 │   exec(code, run_globals)                                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    88 │   return run_globals                                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    89                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    90 def _run_module_code(code, init_globals=None,                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/ml/code/train-LORA.py:158 in <module>                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   155 │   import time                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   156 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   157 │   start_time = time.time()                                           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱ 158 │   trainer.train()                                                    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   159 │   print(\"EXEC TIME:\", time.time() - start_time)                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   160 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   161 │   # evaluate model                                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1543 in train │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1540 │   │   inner_training_loop = find_executable_batch_size(             │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1541 │   │   │   self._inner_training_loop, self[1,mpirank:2,algo-1]<stderr>:│\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    49                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    50 def set_abort_status(status):                                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/runpy.py:288 in run_path                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   285 │   │   # Not a valid sys.path entry, so run the code directly         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   286 │   │   # execfile() doesn't help as we want to allow compiled files   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   287 │   │   code, fname = _get_code_from_file(run_name, path_name)         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱ 288 │   │   return _run_module_code(code, init_globals, run_name,          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   289 │   │   │   │   │   │   │   │   pkg_name=pkg_name, script_name=fname)  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   290 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   291 │   │   # Finder is defined for path, so add it to                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/runpy.py:97 in _run_module_code                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    94 │   fname = script_name if mod_spec is None else mod_spec.origin       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    95 │   with _TempModule(mod_name) as temp_module, _ModifiedArgv0(fname):  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    96 │   │   mod_globals = temp_module.module.__dict__                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱  97 │   │   _run_code(code, mod_globals, init_globals,                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    98 │   │   │   │     mod_name, mod_spec, pkg_name, script_name)           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    99 │   # Copy the globals of the temporary module, as they                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   100 │   # may be cleared when the temporary module goes away               │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/runpy.py:87 in _run_code                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    84 │   │   │   │   │      __loader__ = loader,                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    85 │   │   │   │   │      __package__ = pkg_name,                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    86 │   │   │   │   │      __spec__ = mod_spec)                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱  87 │   exec(code, run_globals)                                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    88 │   return run_globals                                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    89                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    90 def _run_module_code(code, init_globals=None,                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/ml/code/train-LORA.py:158 in <module>                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   155 │   import time                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   156 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   157 │   start_time = time.time()                                           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱ 158 │   trainer.train()                                                    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   159 │   print(\"EXEC TIME:\", time.time() - start_time)                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   160 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   161 │   # evaluate model                                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1543 in train │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1540 │   │   inner_training_loop = find_executable_batch_size(             │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1541 │   │   │   self._inner_training_loop, self[1,mpirank:1,algo-1]<stderr>:│\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    49                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    50 def set_abort_status(status):                                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/runpy.py:288 in run_path                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   285 │   │   # Not a valid sys.path entry, so run the code directly         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   286 │   │   # execfile() doesn't help as we want to allow compiled files   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   287 │   │   code, fname = _get_code_from_file(run_name, path_name)         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱ 288 │   │   return _run_module_code(code, init_globals, run_name,          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   289 │   │   │   │   │   │   │   │   pkg_name=pkg_name, script_name=fname)  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   290 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   291 │   │   # Finder is defined for path, so add it to                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/runpy.py:97 in _run_module_code                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    94 │   fname = script_name if mod_spec is None else mod_spec.origin       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    95 │   with _TempModule(mod_name) as temp_module, _ModifiedArgv0(fname):  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    96 │   │   mod_globals = temp_module.module.__dict__                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱  97 │   │   _run_code(code, mod_globals, init_globals,                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    98 │   │   │   │     mod_name, mod_spec, pkg_name, script_name)           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    99 │   # Copy the globals of the temporary module, as they                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   100 │   # may be cleared when the temporary module goes away               │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/runpy.py:87 in _run_code                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    84 │   │   │   │   │      __loader__ = loader,                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    85 │   │   │   │   │      __package__ = pkg_name,                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    86 │   │   │   │   │      __spec__ = mod_spec)                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱  87 │   exec(code, run_globals)                                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    88 │   return run_globals                                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    89                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    90 def _run_module_code(code, init_globals=None,                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/ml/code/train-LORA.py:158 in <module>                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   155 │   import time                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   156 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   157 │   start_time = time.time()                                           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱ 158 │   trainer.train()                                                    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   159 │   print(\"EXEC TIME:\", time.time() - start_time)                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   160 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   161 │   # evaluate model                                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1543 in train │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1540 │   │   inner_training_loop = find_executable_batch_size(             │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1541 │   │   │   self._inner_training_loop, self[1,mpirank:2,algo-1]<stderr>:._train_batch_size, args.a │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1542 │   │   )                                                             │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱ 1543 │   │   return inner_training_loop(                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1544 │   │   │   args=args,                                                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1545 │   │   │   resume_from_checkpoint=resume_from_checkpoint,            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1546 │   │   │   trial=trial,                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1791 in       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ _inner_training_loop                                                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1788 │   │   │   │   │   with model.no_sync():                             │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1789 │   │   │   │   │   │   tr_loss_step = self.training_step(model, inpu │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1790 │   │   │   │   else:                                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱ 1791 │   │   │   │   │   tr_loss_step = self.training_step(model, inputs)  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1792 │   │   │   │                                                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1793 │   │   │   │   if (                                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1794 │   │   │   │   │   args.logging_nan_inf_filter                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/transformers/trainer.py:2535 in       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ training_step                                                                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   2532 │   │   inputs = self._prepare_inputs(inputs)                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   2533 │   │                                                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   2534 │   │   if is_sagemaker_mp_enabled():                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱ 2535 │   │   │   loss_mb = smp_forward_backward(model, inputs, self.args.g │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   2536 │   │   │   return loss_mb.reduce_mean().detach().to(self.args.device │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   2537 │   │                                                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   2538 │   │   with self.compute_loss_context_manager():                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/ste │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ p.py:261 in __call__                                                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   258 │   │   │   │   │   args, kwargs, state.num_microbatches()             │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   259 │   │   │   │   )                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   260 │   │   │   │   state.pipeline.init_step()                             │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱ 261 │   │   │   │   state.exec_server.run_step_leader(mb_args, mb_kwargs,  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   262 │   │   │   else:                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   263 │   │   │   │   state.exec_server.run_step_follower()                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   264                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/ser │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ver.py:402 in run_step_leader                                                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   399 │   │   │   │   │   │   )                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   400 │   │   │   │   │   │   state.core.timeline_record_pipeline_event(req. │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   401 │   │   │   │   │   │   state.pipeline.promote_status(mb)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱ 402 │   │   [1,mpirank:3,algo-1]<stderr>:._train_batch_size, args.a │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1542 │   │   )                                                             │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱ 1543 │   │   return inner_training_loop(                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1544 │   │   │   args=args,                                                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1545 │   │   │   resume_from_checkpoint=resume_from_checkpoint,            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1546 │   │   │   trial=trial,                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1791 in       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ _inner_training_loop                                                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1788 │   │   │   │   │   with model.no_sync():                             │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1789 │   │   │   │   │   │   tr_loss_step = self.training_step(model, inpu │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1790 │   │   │   │   else:                                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱ 1791 │   │   │   │   │   tr_loss_step = self.training_step(model, inputs)  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1792 │   │   │   │                                                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1793 │   │   │   │   if (                                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1794 │   │   │   │   │   args.logging_nan_inf_filter                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/transformers/trainer.py:2535 in       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ training_step                                                                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   2532 │   │   inputs = self._prepare_inputs(inputs)                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   2533 │   │                                                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   2534 │   │   if is_sagemaker_mp_enabled():                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱ 2535 │   │   │   loss_mb = smp_forward_backward(model, inputs, self.args.g │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   2536 │   │   │   return loss_mb.reduce_mean().detach().to(self.args.device │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   2537 │   │                                                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   2538 │   │   with self.compute_loss_context_manager():                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/ste │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ p.py:261 in __call__                                                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   258 │   │   │   │   │   args, kwargs, state.num_microbatches()             │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   259 │   │   │   │   )                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   260 │   │   │   │   state.pipeline.init_step()                             │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱ 261 │   │   │   │   state.exec_server.run_step_leader(mb_args, mb_kwargs,  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   262 │   │   │   else:                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   263 │   │   │   │   state.exec_server.run_step_follower()                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   264                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/ser │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ver.py:402 in run_step_leader                                                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   399 │   │   │   │   │   │   )                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   400 │   │   │   │   │   │   state.core.timeline_record_pipeline_event(req. │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   401 │   │   │   │   │   │   state.pipeline.promote_status(mb)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱ 402 │   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:._train_batch_size, args.a │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1542 │   │   )                                                             │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱ 1543 │   │   return inner_training_loop(                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1544 │   │   │   args=args,                                                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1545 │   │   │   resume_from_checkpoint=resume_from_checkpoint,            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1546 │   │   │   trial=trial,                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1791 in       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ _inner_training_loop                                                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1788 │   │   │   │   │   with model.no_sync():                             │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1789 │   │   │   │   │   │   tr_loss_step = self.training_step(model, inpu │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1790 │   │   │   │   else:                                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱ 1791 │   │   │   │   │   tr_loss_step = self.training_step(model, inputs)  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1792 │   │   │   │                                                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1793 │   │   │   │   if (                                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1794 │   │   │   │   │   args.logging_nan_inf_filter                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/transformers/trainer.py:2535 in       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ training_step                                                                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   2532 │   │   inputs = self._prepare_inputs(inputs)                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   2533 │   │                                                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   2534 │   │   if is_sagemaker_mp_enabled():                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱ 2535 │   │   │   loss_mb = smp_forward_backward(model, inputs, self.args.g │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   2536 │   │   │   return loss_mb.reduce_mean().detach().to(self.args.device │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   2537 │   │                                                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   2538 │   │   with self.compute_loss_context_manager():                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/ste │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ p.py:261 in __call__                                                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   258 │   │   │   │   │   args, kwargs, state.num_microbatches()             │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   259 │   │   │   │   )                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   260 │   │   │   │   state.pipeline.init_step()                             │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱ 261 │   │   │   │   state.exec_server.run_step_leader(mb_args, mb_kwargs,  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   262 │   │   │   else:                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   263 │   │   │   │   state.exec_server.run_step_follower()                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   264                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/ser │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ver.py:402 in run_step_leader                                                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   399 │   │   │   │   │   │   )                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   400 │   │   │   │   │   │   state.core.timeline_record_pipeline_event(req. │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   401 │   │   │   │   │   │   state.pipeline.promote_status(mb)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱ 402 │   │   [1,mpirank:3,algo-1]<stderr>:│   │   │   │   self.execute_request(req)                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   403 │   │   │   │   │   │   state.core.timeline_record_pipeline_event(req. │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   404 │   │   │   │   │   elif mb_status == MbStatus.READY_FOR_BWD:          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   405 │   │   │   │   │   │   req = self._get_pending_wait_for_req(mb, WaitF │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/ser │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ver.py:119 in execute_request                                                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   116 │   │   │   self._workers[new_worker.id] = new_worker                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   117 │   │   │   chosen_worker = new_worker                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   118 │   │                                                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱ 119 │   │   chosen_worker.execute(req)                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   120 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   121 │   def _get_pending_wait_for_req(self, mb, req_type):                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   122 │   │   # hacky, todo clean this up                                    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/wor │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ker.py:155 in execute                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   152 │   │   # pass the request                                             │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   153 │   │   self.comm_queue.put(req)                                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   154 │   │   state.switching_to_worker(self.id, self.req)                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱ 155 │   │   self._resume_thread_common()                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   156 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   157 │   def resume(self, result: ForwardExecutionResult):                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   158 │   │   \"\"\"                                                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/wor │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ker.py:186 in _resume_thread_common                                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   183 │   │   self.status = WorkerExecStatus.EXECUTING                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   184 │   │   self.acquire_and_notify(ThreadName.WORKER)                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   185 │   │   self.acquire_and_wait(ThreadName.SERVER)                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱ 186 │   │   self._check_queue_after_thread_return()                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   187 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   188 │   def thread_get_forward_result(self, request: ExecutionRequest):    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   189 │   │   \"\"\"                                                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/wor │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ker.py:121 in _check_queue_after_thread_return                               │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   118 │   │   │   raise self.exception                                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   119 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   120 │   def _check_queue_after_thread_return(self):                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱ 121 │   │   self._check_exception()                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   122 │   │   r = self.comm_queue.get()                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   123 │   │   if isinstance(r, ExecutionRequest):   [1,mpirank:2,algo-1]<stderr>:│   │   │   │   self.execute_request(req)                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   403 │   │   │   │   │   │   state.core.timeline_record_pipeline_event(req. │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   404 │   │   │   │   │   elif mb_status == MbStatus.READY_FOR_BWD:          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   405 │   │   │   │   │   │   req = self._get_pending_wait_for_req(mb, WaitF │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/ser │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ver.py:119 in execute_request                                                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   116 │   │   │   self._workers[new_worker.id] = new_worker                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   117 │   │   │   chosen_worker = new_worker                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   118 │   │                                                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱ 119 │   │   chosen_worker.execute(req)                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   120 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   121 │   def _get_pending_wait_for_req(self, mb, req_type):                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   122 │   │   # hacky, todo clean this up                                    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/wor │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ker.py:155 in execute                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   152 │   │   # pass the request                                             │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   153 │   │   self.comm_queue.put(req)                                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   154 │   │   state.switching_to_worker(self.id, self.req)                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱ 155 │   │   self._resume_thread_common()                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   156 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   157 │   def resume(self, result: ForwardExecutionResult):                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   158 │   │   \"\"\"                                                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/wor │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ker.py:186 in _resume_thread_common                                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   183 │   │   self.status = WorkerExecStatus.EXECUTING                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   184 │   │   self.acquire_and_notify(ThreadName.WORKER)                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   185 │   │   self.acquire_and_wait(ThreadName.SERVER)                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱ 186 │   │   self._check_queue_after_thread_return()                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   187 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   188 │   def thread_get_forward_result(self, request: ExecutionRequest):    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   189 │   │   \"\"\"                                                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/wor │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ker.py:121 in _check_queue_after_thread_return                               │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   118 │   │   │   raise self.exception                                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   119 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   120 │   def _check_queue_after_thread_return(self):                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱ 121 │   │   self._check_exception()                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   122 │   │   r = self.comm_queue.get()                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   123 │   │   if isinstance(r, ExecutionRequest):   [1,mpirank:1,algo-1]<stderr>:│   │   │   │   self.execute_request(req)                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   403 │   │   │   │   │   │   state.core.timeline_record_pipeline_event(req. │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   404 │   │   │   │   │   elif mb_status == MbStatus.READY_FOR_BWD:          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   405 │   │   │   │   │   │   req = self._get_pending_wait_for_req(mb, WaitF │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/ser │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ver.py:119 in execute_request                                                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   116 │   │   │   self._workers[new_worker.id] = new_worker                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   117 │   │   │   chosen_worker = new_worker                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   118 │   │                                                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱ 119 │   │   chosen_worker.execute(req)                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   120 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   121 │   def _get_pending_wait_for_req(self, mb, req_type):                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   122 │   │   # hacky, todo clean this up                                    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/wor │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ker.py:155 in execute                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   152 │   │   # pass the request                                             │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   153 │   │   self.comm_queue.put(req)                                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   154 │   │   state.switching_to_worker(self.id, self.req)                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱ 155 │   │   self._resume_thread_common()                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   156 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   157 │   def resume(self, result: ForwardExecutionResult):                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   158 │   │   \"\"\"                                                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/wor │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ker.py:186 in _resume_thread_common                                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   183 │   │   self.status = WorkerExecStatus.EXECUTING                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   184 │   │   self.acquire_and_notify(ThreadName.WORKER)                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   185 │   │   self.acquire_and_wait(ThreadName.SERVER)                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱ 186 │   │   self._check_queue_after_thread_return()                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   187 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   188 │   def thread_get_forward_result(self, request: ExecutionRequest):    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   189 │   │   \"\"\"                                                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/wor │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ker.py:121 in _check_queue_after_thread_return                               │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   118 │   │   │   raise self.exception                                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   119 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   120 │   def _check_queue_after_thread_return(self):                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱ 121 │   │   self._check_exception()                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   122 │   │   r = self.comm_queue.get()                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   123 │   │   if isinstance(r, ExecutionRequest):   [1,mpirank:2,algo-1]<stderr>:                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   124 │   │   │   state.exec_server.process_request(r)                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/wor │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ker.py:118 in _check_exception                                               │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   115 │   │   │   logger.fatal(rmsg(f\"{''.join(traceback.format_tb(self.exce │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   116 │   │   │   logger.fatal(rmsg(f\"Parent exec stack {state.module_manage │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   117 │   │   │   logger.fatal(rmsg(f\"Req {self.req}\"))                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱ 118 │   │   │   raise self.exception                                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   119 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   120 │   def _check_queue_after_thread_return(self):                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   121 │   │   self._check_exception()                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/wor │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ker.py:508 in _thread_compute                                                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   505 │   │   │   │   │   if isinstance(req, TraceStepExecutionRequest):     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   506 │   │   │   │   │   │   self.thread_execute_tracing(req)               │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   507 │   │   │   │   │   elif isinstance(req, StepExecutionRequest):        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱ 508 │   │   │   │   │   │   self.thread_execute_step(req)                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   509 │   │   │   │   │   elif (                                             │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   510 │   │   │   │   │   │   isinstance(req, (ModuleExecutionRequest, Seque │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   511 │   │   │   │   │   │   and req.phase == MbStatus.FWD                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/wor │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ker.py:318 in thread_execute_step                                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   315 │   │   for the server.                                                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   316 │   │   \"\"\"                                                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   317 │   │   device = torch.device(\"cuda\", local_rank())                    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱ 318 │   │   self._exec_step_on_device(req, device)                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   319 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   320 │   def thread_execute_forward(                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   321 │   │   self, req: Union[ModuleExecutionRequest, SequentialModulesExec │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/wor │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ker.py:286 in _exec_step_on_device                                           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   283 │   │   \"\"\"                                                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   284 │   │   args, kwargs = convert_args_to_device(req.args, req.kwargs, de │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   285 │   │   step_fn = state.step_func[req.step_fn_id].func                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱ 286 │   │   outputs = step_fn(*args, **kwargs)                             │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   287 │   │   self.comm_queue.put(ForwardExecutionResult(self.req, outputs)) │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   288 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   289 │   def thread_execute_tracing(self, req: TraceStepExecutionRequest):  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                   [1,mpirank:3,algo-1]<stderr>:                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   124 │   │   │   state.exec_server.process_request(r)                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/wor │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ker.py:118 in _check_exception                                               │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   115 │   │   │   logger.fatal(rmsg(f\"{''.join(traceback.format_tb(self.exce │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   116 │   │   │   logger.fatal(rmsg(f\"Parent exec stack {state.module_manage │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   117 │   │   │   logger.fatal(rmsg(f\"Req {self.req}\"))                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱ 118 │   │   │   raise self.exception                                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   119 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   120 │   def _check_queue_after_thread_return(self):                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   121 │   │   self._check_exception()                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/wor │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ker.py:508 in _thread_compute                                                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   505 │   │   │   │   │   if isinstance(req, TraceStepExecutionRequest):     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   506 │   │   │   │   │   │   self.thread_execute_tracing(req)               │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   507 │   │   │   │   │   elif isinstance(req, StepExecutionRequest):        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱ 508 │   │   │   │   │   │   self.thread_execute_step(req)                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   509 │   │   │   │   │   elif (                                             │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   510 │   │   │   │   │   │   isinstance(req, (ModuleExecutionRequest, Seque │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   511 │   │   │   │   │   │   and req.phase == MbStatus.FWD                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/wor │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ker.py:318 in thread_execute_step                                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   315 │   │   for the server.                                                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   316 │   │   \"\"\"                                                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   317 │   │   device = torch.device(\"cuda\", local_rank())                    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱ 318 │   │   self._exec_step_on_device(req, device)                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   319 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   320 │   def thread_execute_forward(                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   321 │   │   self, req: Union[ModuleExecutionRequest, SequentialModulesExec │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/wor │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ker.py:286 in _exec_step_on_device                                           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   283 │   │   \"\"\"                                                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   284 │   │   args, kwargs = convert_args_to_device(req.args, req.kwargs, de │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   285 │   │   step_fn = state.step_func[req.step_fn_id].func                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱ 286 │   │   outputs = step_fn(*args, **kwargs)                             │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   287 │   │   self.comm_queue.put(ForwardExecutionResult(self.req, outputs)) │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   288 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   289 │   def thread_execute_tracing(self, req: TraceStepExecutionRequest):  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   124 │   │   │   state.exec_server.process_request(r)                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/wor │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ker.py:118 in _check_exception                                               │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   115 │   │   │   logger.fatal(rmsg(f\"{''.join(traceback.format_tb(self.exce │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   116 │   │   │   logger.fatal(rmsg(f\"Parent exec stack {state.module_manage │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   117 │   │   │   logger.fatal(rmsg(f\"Req {self.req}\"))                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱ 118 │   │   │   raise self.exception                                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   119 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   120 │   def _check_queue_after_thread_return(self):                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   121 │   │   self._check_exception()                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/wor │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ker.py:508 in _thread_compute                                                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   505 │   │   │   │   │   if isinstance(req, TraceStepExecutionRequest):     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   506 │   │   │   │   │   │   self.thread_execute_tracing(req)               │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   507 │   │   │   │   │   elif isinstance(req, StepExecutionRequest):        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱ 508 │   │   │   │   │   │   self.thread_execute_step(req)                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   509 │   │   │   │   │   elif (                                             │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   510 │   │   │   │   │   │   isinstance(req, (ModuleExecutionRequest, Seque │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   511 │   │   │   │   │   │   and req.phase == MbStatus.FWD                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/wor │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ker.py:318 in thread_execute_step                                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   315 │   │   for the server.                                                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   316 │   │   \"\"\"                                                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   317 │   │   device = torch.device(\"cuda\", local_rank())                    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱ 318 │   │   self._exec_step_on_device(req, device)                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   319 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   320 │   def thread_execute_forward(                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   321 │   │   self, req: Union[ModuleExecutionRequest, SequentialModulesExec │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/wor │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ker.py:286 in _exec_step_on_device                                           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   283 │   │   \"\"\"                                                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   284 │   │   args, kwargs = convert_args_to_device(req.args, req.kwargs, de │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   285 │   │   step_fn = state.step_func[req.step_fn_id].func                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱ 286 │   │   outputs = step_fn(*args, **kwargs)                             │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   287 │   │   self.comm_queue.put(ForwardExecutionResult(self.req, outputs)) │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   288 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   289 │   def thread_execute_tracing(self, req: TraceStepExecutionRequest):  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                   [1,mpirank:3,algo-1]<stderr>:           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/transformers/trainer_pt_utils.py:1073 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ in smp_forward_backward                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1070 │                                                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1071 │   @smp.step()                                                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1072 │   def smp_forward_backward(model, inputs, gradient_accumulation_ste │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱ 1073 │   │   outputs = model(**inputs)                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1074 │   │   loss = outputs[\"loss\"] if isinstance(outputs, dict) else outp │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1075 │   │   loss /= gradient_accumulation_steps                           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1076 │   │   model.backward(loss)                                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1194 in    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ _call_impl                                                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1191 │   │   # this function, and just call forward.                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1192 │   │   if not (self._backward_hooks or self._forward_hooks or self._ │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1193 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱ 1194 │   │   │   return forward_call(*input, **kwargs)                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1195 │   │   # Do not call functions when jit is used                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1196 │   │   full_backward_hooks, non_full_backward_hooks = [], []         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1197 │   │   if self._backward_hooks or _global_backward_hooks:            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ches/execution.py:396 in distributed_forward                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   393 │   # When a module has no parameter, run as no partition. It's to res │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   394 │   # used in a multiple parent modules in which some parent module ha │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   395 │   if state.module_manager.is_executor(module) or num_params == 0:    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱ 396 │   │   outputs = module.execute_locally(*args, **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   397 │   elif state.module_manager.is_parent_executor(module):              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   398 │   │   outputs = execute_as_parent(module, *args, **kwargs)           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   399 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ches/execution.py:338 in execute_module                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   335 │   │   │   preserve_rng_state=config.preserve_rng_state,              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   336 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   337 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱ 338 │   │   outputs = actual_forward(module, *args, **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   339 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   340 │   if not state.is_in_fwd_on_checkpointed_fn:                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   341 │   │   module_name = state.module_manager.get_module_name(module)     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ches/execution.py:276 in actual_forward                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                              [1,mpirank:2,algo-1]<stderr>:           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/transformers/trainer_pt_utils.py:1073 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ in smp_forward_backward                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1070 │                                                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1071 │   @smp.step()                                                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1072 │   def smp_forward_backward(model, inputs, gradient_accumulation_ste │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱ 1073 │   │   outputs = model(**inputs)                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1074 │   │   loss = outputs[\"loss\"] if isinstance(outputs, dict) else outp │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1075 │   │   loss /= gradient_accumulation_steps                           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1076 │   │   model.backward(loss)                                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1194 in    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ _call_impl                                                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1191 │   │   # this function, and just call forward.                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1192 │   │   if not (self._backward_hooks or self._forward_hooks or self._ │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1193 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱ 1194 │   │   │   return forward_call(*input, **kwargs)                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1195 │   │   # Do not call functions when jit is used                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1196 │   │   full_backward_hooks, non_full_backward_hooks = [], []         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1197 │   │   if self._backward_hooks or _global_backward_hooks:            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ches/execution.py:396 in distributed_forward                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   393 │   # When a module has no parameter, run as no partition. It's to res │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   394 │   # used in a multiple parent modules in which some parent module ha │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   395 │   if state.module_manager.is_executor(module) or num_params == 0:    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱ 396 │   │   outputs = module.execute_locally(*args, **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   397 │   elif state.module_manager.is_parent_executor(module):              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   398 │   │   outputs = execute_as_parent(module, *args, **kwargs)           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   399 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ches/execution.py:338 in execute_module                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   335 │   │   │   preserve_rng_state=config.preserve_rng_state,              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   336 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   337 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱ 338 │   │   outputs = actual_forward(module, *args, **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   339 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   340 │   if not state.is_in_fwd_on_checkpointed_fn:                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   341 │   │   module_name = state.module_manager.get_module_name(module)     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ches/execution.py:276 in actual_forward                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                              [1,mpirank:1,algo-1]<stderr>:           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/transformers/trainer_pt_utils.py:1073 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ in smp_forward_backward                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1070 │                                                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1071 │   @smp.step()                                                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1072 │   def smp_forward_backward(model, inputs, gradient_accumulation_ste │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱ 1073 │   │   outputs = model(**inputs)                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1074 │   │   loss = outputs[\"loss\"] if isinstance(outputs, dict) else outp │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1075 │   │   loss /= gradient_accumulation_steps                           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1076 │   │   model.backward(loss)                                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1194 in    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ _call_impl                                                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1191 │   │   # this function, and just call forward.                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1192 │   │   if not (self._backward_hooks or self._forward_hooks or self._ │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1193 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱ 1194 │   │   │   return forward_call(*input, **kwargs)                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1195 │   │   # Do not call functions when jit is used                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1196 │   │   full_backward_hooks, non_full_backward_hooks = [], []         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1197 │   │   if self._backward_hooks or _global_backward_hooks:            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ches/execution.py:396 in distributed_forward                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   393 │   # When a module has no parameter, run as no partition. It's to res │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   394 │   # used in a multiple parent modules in which some parent module ha │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   395 │   if state.module_manager.is_executor(module) or num_params == 0:    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱ 396 │   │   outputs = module.execute_locally(*args, **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   397 │   elif state.module_manager.is_parent_executor(module):              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   398 │   │   outputs = execute_as_parent(module, *args, **kwargs)           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   399 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ches/execution.py:338 in execute_module                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   335 │   │   │   preserve_rng_state=config.preserve_rng_state,              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   336 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   337 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱ 338 │   │   outputs = actual_forward(module, *args, **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   339 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   340 │   if not state.is_in_fwd_on_checkpointed_fn:                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   341 │   │   module_name = state.module_manager.get_module_name(module)     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ches/execution.py:276 in actual_forward                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                              [1,mpirank:2,algo-1]<stderr>:                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   273 │   │   │   *tensors,                                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   274 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   275 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱ 276 │   │   outputs = module._orig_forward(*args, **kwargs)                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   277 │   return outputs                                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   278                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   279                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/mod │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ el.py:1351 in forward                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1348 │   def forward(self, *args, **kwargs):                               │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1349 │   │   if state.cfg.pipeline_parallel_degree > 1 and not state.in_st │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1350 │   │   │   raise FwdExecutionNotInStepFnError                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱ 1351 │   │   return self.module(*args, **kwargs)                           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1352 │                                                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1353 │   @property                                                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1354 │   def partitioned(self):                                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1194 in    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ _call_impl                                                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1191 │   │   # this function, and just call forward.                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1192 │   │   if not (self._backward_hooks or self._forward_hooks or self._ │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1193 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱ 1194 │   │   │   return forward_call(*input, **kwargs)                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1195 │   │   # Do not call functions when jit is used                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1196 │   │   full_backward_hooks, non_full_backward_hooks = [], []         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1197 │   │   if self._backward_hooks or _global_backward_hooks:            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ches/execution.py:396 in distributed_forward                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   393 │   # When a module has no parameter, run as no partition. It's to res │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   394 │   # used in a multiple parent modules in which some parent module ha │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   395 │   if state.module_manager.is_executor(module) or num_params == 0:    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱ 396 │   │   outputs = module.execute_locally(*args, **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   397 │   elif state.module_manager.is_parent_executor(module):              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   398 │   │   outputs = execute_as_parent(module, *args, **kwargs)           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   399 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ches/execution.py:338 in execute_module                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   335 │   │   │   preserve_rng_state=config.preserve_rng_state,              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   336 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   337 │   else:                                   [1,mpirank:3,algo-1]<stderr>:                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   273 │   │   │   *tensors,                                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   274 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   275 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱ 276 │   │   outputs = module._orig_forward(*args, **kwargs)                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   277 │   return outputs                                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   278                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   279                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/mod │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ el.py:1351 in forward                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1348 │   def forward(self, *args, **kwargs):                               │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1349 │   │   if state.cfg.pipeline_parallel_degree > 1 and not state.in_st │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1350 │   │   │   raise FwdExecutionNotInStepFnError                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱ 1351 │   │   return self.module(*args, **kwargs)                           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1352 │                                                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1353 │   @property                                                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1354 │   def partitioned(self):                                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1194 in    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ _call_impl                                                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1191 │   │   # this function, and just call forward.                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1192 │   │   if not (self._backward_hooks or self._forward_hooks or self._ │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1193 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱ 1194 │   │   │   return forward_call(*input, **kwargs)                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1195 │   │   # Do not call functions when jit is used                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1196 │   │   full_backward_hooks, non_full_backward_hooks = [], []         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1197 │   │   if self._backward_hooks or _global_backward_hooks:            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ches/execution.py:396 in distributed_forward                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   393 │   # When a module has no parameter, run as no partition. It's to res │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   394 │   # used in a multiple parent modules in which some parent module ha │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   395 │   if state.module_manager.is_executor(module) or num_params == 0:    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱ 396 │   │   outputs = module.execute_locally(*args, **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   397 │   elif state.module_manager.is_parent_executor(module):              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   398 │   │   outputs = execute_as_parent(module, *args, **kwargs)           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   399 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ches/execution.py:338 in execute_module                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   335 │   │   │   preserve_rng_state=config.preserve_rng_state,              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   336 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   337 │   else:                                   [1,mpirank:1,algo-1]<stderr>:                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   273 │   │   │   *tensors,                                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   274 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   275 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱ 276 │   │   outputs = module._orig_forward(*args, **kwargs)                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   277 │   return outputs                                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   278                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   279                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/mod │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ el.py:1351 in forward                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1348 │   def forward(self, *args, **kwargs):                               │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1349 │   │   if state.cfg.pipeline_parallel_degree > 1 and not state.in_st │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1350 │   │   │   raise FwdExecutionNotInStepFnError                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱ 1351 │   │   return self.module(*args, **kwargs)                           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1352 │                                                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1353 │   @property                                                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1354 │   def partitioned(self):                                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1194 in    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ _call_impl                                                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1191 │   │   # this function, and just call forward.                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1192 │   │   if not (self._backward_hooks or self._forward_hooks or self._ │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1193 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱ 1194 │   │   │   return forward_call(*input, **kwargs)                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1195 │   │   # Do not call functions when jit is used                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1196 │   │   full_backward_hooks, non_full_backward_hooks = [], []         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1197 │   │   if self._backward_hooks or _global_backward_hooks:            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ches/execution.py:396 in distributed_forward                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   393 │   # When a module has no parameter, run as no partition. It's to res │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   394 │   # used in a multiple parent modules in which some parent module ha │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   395 │   if state.module_manager.is_executor(module) or num_params == 0:    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱ 396 │   │   outputs = module.execute_locally(*args, **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   397 │   elif state.module_manager.is_parent_executor(module):              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   398 │   │   outputs = execute_as_parent(module, *args, **kwargs)           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   399 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ches/execution.py:338 in execute_module                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   335 │   │   │   preserve_rng_state=config.preserve_rng_state,              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   336 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   337 │   else:                                   [1,mpirank:3,algo-1]<stderr>:                           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱ 338 │   │   outputs = actual_forward(module, *args, **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   339 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   340 │   if not state.is_in_fwd_on_checkpointed_fn:                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   341 │   │   module_name = state.module_manager.get_module_name(module)     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ches/execution.py:276 in actual_forward                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   273 │   │   │   *tensors,                                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   274 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   275 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱ 276 │   │   outputs = module._orig_forward(*args, **kwargs)                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   277 │   return outputs                                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   278                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   279                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/ddp │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ _model.py:440 in forward                                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   437 │   # -----                                                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   438 │   # Redirecting these methods so the extra wrapper is all hidden fro │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   439 │   def forward(self, *args, **kwargs):                                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱ 440 │   │   return self.module(*args, **kwargs)                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   441 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   442 │   def named_parameters(self, *args, **kwargs):                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   443 │   │   return self.module.named_parameters(*args, **kwargs)           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1194 in    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ _call_impl                                                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1191 │   │   # this function, and just call forward.                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1192 │   │   if not (self._backward_hooks or self._forward_hooks or self._ │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1193 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱ 1194 │   │   │   return forward_call(*input, **kwargs)                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1195 │   │   # Do not call functions when jit is used                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1196 │   │   full_backward_hooks, non_full_backward_hooks = [], []         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1197 │   │   if self._backward_hooks or _global_backward_hooks:            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ches/execution.py:396 in distributed_forward                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   393 │   # When a module has no parameter, run as no partition. It's to res │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   394 │   # used in a multiple parent modules in which some parent module ha │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   395 │   if state.module_manager.is_executor(module) or num_params == 0:    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱ 396 │   │   outputs = module.execute_locally(*args, **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   397 │   elif state.module_manager.is_parent_executor(module):              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   398 │   │   outputs = execute_as_parent(m[1,mpirank:2,algo-1]<stderr>:                           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱ 338 │   │   outputs = actual_forward(module, *args, **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   339 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   340 │   if not state.is_in_fwd_on_checkpointed_fn:                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   341 │   │   module_name = state.module_manager.get_module_name(module)     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ches/execution.py:276 in actual_forward                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   273 │   │   │   *tensors,                                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   274 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   275 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱ 276 │   │   outputs = module._orig_forward(*args, **kwargs)                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   277 │   return outputs                                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   278                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   279                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/ddp │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ _model.py:440 in forward                                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   437 │   # -----                                                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   438 │   # Redirecting these methods so the extra wrapper is all hidden fro │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   439 │   def forward(self, *args, **kwargs):                                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱ 440 │   │   return self.module(*args, **kwargs)                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   441 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   442 │   def named_parameters(self, *args, **kwargs):                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   443 │   │   return self.module.named_parameters(*args, **kwargs)           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1194 in    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ _call_impl                                                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1191 │   │   # this function, and just call forward.                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1192 │   │   if not (self._backward_hooks or self._forward_hooks or self._ │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1193 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱ 1194 │   │   │   return forward_call(*input, **kwargs)                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1195 │   │   # Do not call functions when jit is used                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1196 │   │   full_backward_hooks, non_full_backward_hooks = [], []         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1197 │   │   if self._backward_hooks or _global_backward_hooks:            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ches/execution.py:396 in distributed_forward                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   393 │   # When a module has no parameter, run as no partition. It's to res │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   394 │   # used in a multiple parent modules in which some parent module ha │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   395 │   if state.module_manager.is_executor(module) or num_params == 0:    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱ 396 │   │   outputs = module.execute_locally(*args, **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   397 │   elif state.module_manager.is_parent_executor(module):              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   398 │   │   outputs = execute_as_parent(m[1,mpirank:1,algo-1]<stderr>:                           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱ 338 │   │   outputs = actual_forward(module, *args, **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   339 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   340 │   if not state.is_in_fwd_on_checkpointed_fn:                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   341 │   │   module_name = state.module_manager.get_module_name(module)     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ches/execution.py:276 in actual_forward                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   273 │   │   │   *tensors,                                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   274 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   275 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱ 276 │   │   outputs = module._orig_forward(*args, **kwargs)                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   277 │   return outputs                                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   278                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   279                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/ddp │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ _model.py:440 in forward                                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   437 │   # -----                                                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   438 │   # Redirecting these methods so the extra wrapper is all hidden fro │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   439 │   def forward(self, *args, **kwargs):                                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱ 440 │   │   return self.module(*args, **kwargs)                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   441 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   442 │   def named_parameters(self, *args, **kwargs):                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   443 │   │   return self.module.named_parameters(*args, **kwargs)           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1194 in    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ _call_impl                                                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1191 │   │   # this function, and just call forward.                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1192 │   │   if not (self._backward_hooks or self._forward_hooks or self._ │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1193 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱ 1194 │   │   │   return forward_call(*input, **kwargs)                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1195 │   │   # Do not call functions when jit is used                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1196 │   │   full_backward_hooks, non_full_backward_hooks = [], []         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1197 │   │   if self._backward_hooks or _global_backward_hooks:            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ches/execution.py:396 in distributed_forward                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   393 │   # When a module has no parameter, run as no partition. It's to res │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   394 │   # used in a multiple parent modules in which some parent module ha │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   395 │   if state.module_manager.is_executor(module) or num_params == 0:    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱ 396 │   │   outputs = module.execute_locally(*args, **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   397 │   elif state.module_manager.is_parent_executor(module):              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   398 │   │   outputs = execute_as_parent(m[1,mpirank:2,algo-1]<stderr>:odule, *args, **kwargs)           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   399 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ches/execution.py:338 in execute_module                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   335 │   │   │   preserve_rng_state=config.preserve_rng_state,              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   336 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   337 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱ 338 │   │   outputs = actual_forward(module, *args, **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   339 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   340 │   if not state.is_in_fwd_on_checkpointed_fn:                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   341 │   │   module_name = state.module_manager.get_module_name(module)     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ches/execution.py:276 in actual_forward                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   273 │   │   │   *tensors,                                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   274 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   275 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱ 276 │   │   outputs = module._orig_forward(*args, **kwargs)                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   277 │   return outputs                                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   278                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   279                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/peft/peft_model.py:736 in forward     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    733 │   │   return_dict = return_dict if return_dict is not None else sel │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    734 │   │   peft_config = self.active_peft_config                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    735 │   │   if not isinstance(peft_config, PromptLearningConfig):         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱  736 │   │   │   return self.base_model(                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    737 │   │   │   │   input_ids=input_ids,                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    738 │   │   │   │   attention_mask=attention_mask,                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    739 │   │   │   │   inputs_embeds=inputs_embeds,                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1194 in    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ _call_impl                                                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1191 │   │   # this function, and just call forward.                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1192 │   │   if not (self._backward_hooks or self._forward_hooks or self._ │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1193 │   │   │   │   or _global_forward_hooks or _globa\u001b[0m\n",
      "\u001b[34ml_forward_pre_hooks │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱ 1194 │   │   │   return forward_call(*input, **kwargs)                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1195 │   │   # Do not call functions when jit is used                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1196 │   │   full_backward_hooks, non_full_backward_hooks = [], []         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1197 │   │   if self._backward_hooks or _global_backward_hooks:            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeli │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ng_distilbert.py[1,mpirank:3,algo-1]<stderr>:odule, *args, **kwargs)           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   399 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ches/execution.py:338 in execute_module                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   335 │   │   │   preserve_rng_state=config.preserve_rng_state,              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   336 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   337 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱ 338 │   │   outputs = actual_forward(module, *args, **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   339 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   340 │   if not state.is_in_fwd_on_checkpointed_fn:                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   341 │   │   module_name = state.module_manager.get_module_name(module)     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ches/execution.py:276 in actual_forward                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   273 │   │   │   *tensors,                                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   274 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   275 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱ 276 │   │   outputs = module._orig_forward(*args, **kwargs)                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   277 │   return outputs                                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   278                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   279                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/peft/peft_model.py:736 in forward     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    733 │   │   return_dict = return_dict if return_dict is not None else sel │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    734 │   │   peft_config = self.active_peft_config                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    735 │   │   if not isinstance(peft_config, PromptLearningConfig):         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱  736 │   │   │   return self.base_model(                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    737 │   │   │   │   input_ids=input_ids,                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    738 │   │   │   │   attention_mask=attention_mask,                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    739 │   │   │   │   inputs_embeds=inputs_embeds,                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1194 in    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ _call_impl                                                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1191 │   │   # this function, and just call forward.                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1192 │   │   if not (self._backward_hooks or self._forward_hooks or self._ │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1193 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱ 1194 │   │   │   return forward_call(*input, **kwargs)                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1195 │   │   # Do not call functions when jit is used                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1196 │   │   full_backward_hooks, non_full_backward_hooks = [], []         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1197 │   │   if self._backward_hooks or _global_backward_hooks:            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeli │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ng_distilbert.py[1,mpirank:1,algo-1]<stderr>:odule, *args, **kwargs)           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   399 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ches/execution.py:338 in execute_module                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   335 │   │   │   preserve_rng_state=config.preserve_rng_state,              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   336 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   337 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱ 338 │   │   outputs = actual_forward(module, *args, **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   339 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   340 │   if not state.is_in_fwd_on_checkpointed_fn:                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   341 │   │   module_name = state.module_manager.get_module_name(module)     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ches/execution.py:276 in actual_forward                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   273 │   │   │   *tensors,                                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   274 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   275 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱ 276 │   │   outputs = module._orig_forward(*args, **kwargs)                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   277 │   return outputs                                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   278                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   279                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/peft/peft_model.py:736 in forward     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    733 │   │   return_dict = return_dict if return_dict is not None else sel │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    734 │   │   peft_config = self.active_peft_config                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    735 │   │   if not isinstance(peft_config, PromptLearningConfig):         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱  736 │   │   │   return self.base_model(                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    737 │   │   │   │   input_ids=input_ids,                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    738 │   │   │   │   attention_mask=attention_mask,                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    739 │   │   │   │   inputs_embeds=inputs_embeds,                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1194 in    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ _call_impl                                                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1191 │   │   # this function, and just call forward.                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1192 │   │   if not (self._backward_hooks or self._forward_hooks or self._ │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1193 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱ 1194 │   │   │   return forward_call(*input, **kwargs)                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1195 │   │   # Do not call functions when jit is used                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1196 │   │   full_backward_hooks, non_full_backward_hooks = [], []         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1197 │   │   if self._backward_hooks or _global_backward_hooks:            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeli │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ng_distilbert.py[1,mpirank:3,algo-1]<stderr>::759 in forward                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    756 │   │   \"\"\"                                                           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    757 │   │   return_dict = return_dict if return_dict is not None else sel │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    758 │   │                                                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱  759 │   │   distilbert_output = self.distilbert(                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    760 │   │   │   input_ids=input_ids,                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    761 │   │   │   attention_mask=attention_mask,                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    762 │   │   │   head_mask=head_mask,                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1194 in    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ _call_impl                                                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1191 │   │   # this function, and just call forward.                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1192 │   │   if not (self._backward_hooks or self._forward_hooks or self._ │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1193 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱ 1194 │   │   │   return forward_call(*input, **kwargs)                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1195 │   │   # Do not call functions when jit is used                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1196 │   │   full_backward_hooks, non_full_backward_hooks = [], []         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1197 │   │   if self._backward_hooks or _global_backward_hooks:            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ches/execution.py:396 in distributed_forward                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   393 │   # When a module has no parameter, run as no partition. It's to res │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   394 │   # used in a multiple parent modules in which some parent module ha │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   395 │   if state.module_manager.is_executor(module) or num_params == 0:    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱ 396 │   │   outputs = module.execute_locally(*args, **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   397 │   elif state.module_manager.is_parent_executor(module):              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   398 │   │   outputs = execute_as_parent(module, *args, **kwargs)           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   399 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ches/execution.py:338 in execute_module                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   335 │   │   │   preserve_rng_state=config.preserve_rng_state,              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   336 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   337 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱ 338 │   │   outputs = actual_forward(module, *args, **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   339 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   340 │   if not state.is_in_fwd_on_checkpointed_fn:                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   341 │   │   module_name = state.module_manager.get_module_name(module)     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ches/execution.py:276 in actual_forward                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   273 │   │   │   *tensors,                                                  �[1,mpirank:2,algo-1]<stderr>::759 in forward                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    756 │   │   \"\"\"                                                           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    757 │   │   return_dict = return_dict if return_dict is not None else sel │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    758 │   │                                                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱  759 │   │   distilbert_output = self.distilbert(                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    760 │   │   │   input_ids=input_ids,                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    761 │   │   │   attention_mask=attention_mask,                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    762 │   │   │   head_mask=head_mask,                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1194 in    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ _call_impl                                                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1191 │   │   # this function, and just call forward.                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1192 │   │   if not (self._backward_hooks or self._forward_hooks or self._ │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1193 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱ 1194 │   │   │   return forward_call(*input, **kwargs)                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1195 │   │   # Do not call functions when jit is used                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1196 │   │   full_backward_hooks, non_full_backward_hooks = [], []         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1197 │   │   if self._backward_hooks or _global_backward_hooks:            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ches/execution.py:396 in distributed_forward                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   393 │   # When a module has no parameter, run as no partition. It's to res │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   394 │   # used in a multiple parent modules in which some parent module ha │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   395 │   if state.module_manager.is_executor(module) or num_params == 0:    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱ 396 │   │   outputs = module.execute_locally(*args, **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   397 │   elif state.module_manager.is_parent_executor(module):              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   398 │   │   outputs = execute_as_parent(module, *args, **kwargs)           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   399 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ches/execution.py:338 in execute_module                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   335 │   │   │   preserve_rng_state=config.preserve_rng_state,              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   336 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   337 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱ 338 │   │   outputs = actual_forward(module, *args, **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   339 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   340 │   if not state.is_in_fwd_on_checkpointed_fn:                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   341 │   │   module_name = state.module_manager.get_module_name(module)     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ches/execution.py:276 in actual_forward                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   273 │   │   │   *tensors,                                                  �[1,mpirank:1,algo-1]<stderr>::759 in forward                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    756 │   │   \"\"\"                                                           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    757 │   │   return_dict = return_dict if return_dict is not None else sel │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    758 │   │                                                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱  759 │   │   distilbert_output = self.distilbert(                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    760 │   │   │   input_ids=input_ids,                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    761 │   │   │   attention_mask=attention_mask,                            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    762 │   │   │   head_mask=head_mask,                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1194 in    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ _call_impl                                                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1191 │   │   # this function, and just call forward.                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1192 │   │   if not (self._backward_hooks or self._forward_hooks or self._ │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1193 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱ 1194 │   │   │   return forward_call(*input, **kwargs)                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1195 │   │   # Do not call functions when jit is used                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1196 │   │   full_backward_hooks, non_full_backward_hooks = [], []         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1197 │   │   if self._backward_hooks or _global_backward_hooks:            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ches/execution.py:396 in distributed_forward                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   393 │   # When a module has no parameter, run as no partition. It's to res │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   394 │   # used in a multiple parent modules in which some parent module ha │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   395 │   if state.module_manager.is_executor(module) or num_params == 0:    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱ 396 │   │   outputs = module.execute_locally(*args, **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   397 │   elif state.module_manager.is_parent_executor(module):              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   398 │   │   outputs = execute_as_parent(module, *args, **kwargs)           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   399 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ches/execution.py:338 in execute_module                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   335 │   │   │   preserve_rng_state=config.preserve_rng_state,              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   336 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   337 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱ 338 │   │   outputs = actual_forward(module, *args, **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   339 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   340 │   if not state.is_in_fwd_on_checkpointed_fn:                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   341 │   │   module_name = state.module_manager.get_module_name(module)     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ches/execution.py:276 in actual_forward                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   273 │   │   │   *tensors,                                                  �[1,mpirank:2,algo-1]<stderr>:��\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   274 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   275 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱ 276 │   │   outputs = module._orig_forward(*args, **kwargs)                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   277 │   return outputs                                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   278                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   279                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeli │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ng_distilbert.py:579 in forward                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    576 │   │                                                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    577 │   │   if inputs_embeds is None:                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    578 │   │   │   inputs_embeds = self.embeddings(input_ids)  # (bs, seq_le │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱  579 │   │   return self.transformer(                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    580 │   │   │   x=inputs_embeds,                                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    581 │   │   │   attn_mask=attention_mask,                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    582 │   │   │   head_mask=head_mask,                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1194 in    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ _call_impl                                                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1191 │   │   # this function, and just call forward.                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1192 │   │   if not (self._backward_hooks or self._forward_hooks or self._ │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1193 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱ 1194 │   │   │   return forward_call(*input, **kwargs)                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1195 │   │   # Do not call functions when jit is used                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1196 │   │   full_backward_hooks, non_full_backward_hooks = [], []         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1197 │   │   if self._backward_hooks or _global_backward_hooks:            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ches/execution.py:396 in distributed_forward                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   393 │   # When a module has no parameter, run as no partition. It's to res │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   394 │   # used in a multiple parent modules in which some parent module ha │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   395 │   if state.module_manager.is_executor(module) or num_params == 0:    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱ 396 │   │   outputs = module.execute_locally(*args, **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   397 │   elif state.module_manager.is_parent_executor(module):              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   398 │   │   outputs = execute_as_parent(module, *args, **kwargs)           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   399 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ches/execution.py:338 in execute_module                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   335 │   │   │   preserve_rng_state=config.preserve_rng_state,              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   336 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   337 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱ 338 │   │   outputs = actual_forward(module, *args,[1,mpirank:3,algo-1]<stderr>:��\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   274 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   275 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱ 276 │   │   outputs = module._orig_forward(*args, **kwargs)                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   277 │   return outputs                                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   278                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   279                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeli │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ng_distilbert.py:579 in forward                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    576 │   │                                                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    577 │   │   if inputs_embeds is None:                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    578 │   │   │   inputs_embeds = self.embeddings(input_ids)  # (bs, seq_le │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱  579 │   │   return self.transformer(                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    580 │   │   │   x=inputs_embeds,                                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    581 │   │   │   attn_mask=attention_mask,                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    582 │   │   │   head_mask=head_mask,                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1194 in    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ _call_impl                                                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1191 │   │   # this function, and just call forward.                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1192 │   │   if not (self._backward_hooks or self._forward_hooks or self._ │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1193 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱ 1194 │   │   │   return forward_call(*input, **kwargs)                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1195 │   │   # Do not call functions when jit is used                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1196 │   │   full_backward_hooks, non_full_backward_hooks = [], []         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1197 │   │   if self._backward_hooks or _global_backward_hooks:            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ches/execution.py:396 in distributed_forward                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   393 │   # When a module has no parameter, run as no partition. It's to res │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   394 │   # used in a multiple parent modules in which some parent module ha │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   395 │   if state.module_manager.is_executor(module) or num_params == 0:    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱ 396 │   │   outputs = module.execute_locally(*args, **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   397 │   elif state.module_manager.is_parent_executor(module):              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   398 │   │   outputs = execute_as_parent(module, *args, **kwargs)           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   399 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ches/execution.py:338 in execute_module                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   335 │   │   │   preserve_rng_state=config.preserve_rng_state,              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   336 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   337 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱ 338 │   │   outputs = actual_forward(module, *args,[1,mpirank:1,algo-1]<stderr>:��\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   274 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   275 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱ 276 │   │   outputs = module._orig_forward(*args, **kwargs)                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   277 │   return outputs                                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   278                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   279                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeli │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ng_distilbert.py:579 in forward                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    576 │   │                                                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    577 │   │   if inputs_embeds is None:                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    578 │   │   │   inputs_embeds = self.embeddings(input_ids)  # (bs, seq_le │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱  579 │   │   return self.transformer(                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    580 │   │   │   x=inputs_embeds,                                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    581 │   │   │   attn_mask=attention_mask,                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    582 │   │   │   head_mask=head_mask,                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1194 in    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ _call_impl                                                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1191 │   │   # this function, and just call forward.                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1192 │   │   if not (self._backward_hooks or self._forward_hooks or self._ │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1193 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱ 1194 │   │   │   return forward_call(*input, **kwargs)                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1195 │   │   # Do not call functions when jit is used                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1196 │   │   full_backward_hooks, non_full_backward_hooks = [], []         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1197 │   │   if self._backward_hooks or _global_backward_hooks:            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ches/execution.py:396 in distributed_forward                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   393 │   # When a module has no parameter, run as no partition. It's to res │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   394 │   # used in a multiple parent modules in which some parent module ha │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   395 │   if state.module_manager.is_executor(module) or num_params == 0:    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱ 396 │   │   outputs = module.execute_locally(*args, **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   397 │   elif state.module_manager.is_parent_executor(module):              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   398 │   │   outputs = execute_as_parent(module, *args, **kwargs)           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   399 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ches/execution.py:338 in execute_module                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   335 │   │   │   preserve_rng_state=config.preserve_rng_state,              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   336 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   337 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱ 338 │   │   outputs = actual_forward(module, *args,[1,mpirank:3,algo-1]<stderr>: **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   339 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   340 │   if not state.is_in_fwd_on_checkpointed_fn:                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   341 │   │   module_name = state.module_manager.get_module_name(module)     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ches/execution.py:276 in actual_forward                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   273 │   │   │   *tensors,                                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   274 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   275 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱ 276 │   │   outputs = module._orig_forward(*args, **kwargs)                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   277 │   return outputs                                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   278                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   279                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeli │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ng_distilbert.py:355 in forward                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    352 │   │   │   if output_hidden_states:                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    353 │   │   │   │   all_hidden_states = all_hidden_states + (hidden_state │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    354 │   │   │                                                             │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱  355 │   │   │   layer_outputs = layer_module(                             │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    356 │   │   │   │   x=hidden_state, attn_mask=attn_mask, head_mask=head_m │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    357 │   │   │   )                                                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    358 │   │   │   hidden_state = layer_outputs[-1]                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1194 in    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ _call_impl                                                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1191 │   │   # this function, and just call forward.                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1192 │   │   if not (self._backward_hooks or self._forward_hooks or self._ │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1193 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱ 1194 │   │   │   return forward_call(*input, **kwargs)                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1195 │   │   # Do not call functions when jit is used                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1196 │   │   full_backward_hooks, non_full_backward_hooks = [], []         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1197 │   │   if self._backward_hooks or _global_backward_hooks:            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ches/execution.py:396 in distributed_forward                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   393 │   # When a module has no parameter, run as no partition. It's to res │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   394 │   # used in a multiple parent modules in which some parent module ha │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   395 │   if state.module_manager.is_executor(module) or num_params == 0:    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱ 396 │   │   outputs = module.execute_locally(*args, **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   397 │   elif state.module_manager.is_parent_executor(module):              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   398 │   │   outputs = execute_as_parent(module, *args, **kwargs)           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   399 │   else:       [1,mpirank:2,algo-1]<stderr>: **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   339 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   340 │   if not state.is_in_fwd_on_checkpointed_fn:                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   341 │   │   module_name = state.module_manager.get_module_name(module)     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ches/execution.py:276 in actual_forward                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   273 │   │   │   *tensors,                                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   274 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   275 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱ 276 │   │   outputs = module._orig_forward(*args, **kwargs)                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   277 │   return outputs                                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   278                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   279                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeli │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ng_distilbert.py:355 in forward                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    352 │   │   │   if output_hidden_states:                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    353 │   │   │   │   all_hidden_states = all_hidden_states + (hidden_state │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    354 │   │   │                                                             │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱  355 │   │   │   layer_outputs = layer_module(                             │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    356 │   │   │   │   x=hidden_state, attn_mask=attn_mask, head_mask=head_m │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    357 │   │   │   )                                                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    358 │   │   │   hidden_state = layer_outputs[-1]                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1194 in    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ _call_impl                                                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1191 │   │   # this function, and just call forward.                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1192 │   │   if not (self._backward_hooks or self._forward_hooks or self._ │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1193 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱ 1194 │   │   │   return forward_call(*input, **kwargs)                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1195 │   │   # Do not call functions when jit is used                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1196 │   │   full_backward_hooks, non_full_backward_hooks = [], []         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1197 │   │   if self._backward_hooks or _global_backward_hooks:            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ches/execution.py:396 in distributed_forward                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   393 │   # When a module has no parameter, run as no partition. It's to res │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   394 │   # used in a multiple parent modules in which some parent module ha │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   395 │   if state.module_manager.is_executor(module) or num_params == 0:    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱ 396 │   │   outputs = module.execute_locally(*args, **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   397 │   elif state.module_manager.is_parent_executor(module):              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   398 │   │   outputs = execute_as_parent(module, *args, **kwargs)           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   399 │   else:       --------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mmpirun.real detected that one or more processes exited with non-zero status, thus causing\u001b[0m\n",
      "\u001b[34mthe job to be terminated. The first process to do so was:\n",
      "  Process name: [[41117,1],0]\n",
      "  Exit code:    1\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>: **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   339 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   340 │   if not state.is_in_fwd_on_checkpointed_fn:                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   341 │   │   module_name = state.module_manager.get_module_name(module)     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ches/execution.py:276 in actual_forward                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│\u001b[0m\n",
      "\u001b[34m│\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   273 │   │   │   *tensors,                                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   274 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   275 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱ 276 │   │   outputs = module._orig_forward(*args, **kwargs)                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   277 │   return outputs                                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   278                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   279                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeli │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ng_distilbert.py:355 in forward                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    352 │   │   │   if output_hidden_states:                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    353 │   │   │   │   all_hidden_states = all_hidden_states + (hidden_state │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    354 │   │   │                                                             │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱  355 │   │   │   layer_outputs = layer_module(                             │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    356 │   │   │   │   x=hidden_state, attn_mask=attn_mask, head_mask=head_m │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    357 │   │   │   )                                                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    358 │   │   │   hidden_state = layer_outputs[-1]                          │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1194 in    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ _call_impl                                                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1191 │   │   # this function, and just call forward.                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1192 │   │   if not (self._backward_hooks or self._forward_hooks or self._ │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1193 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱ 1194 │   │   │   return forward_call(*input, **kwargs)                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1195 │   │   # Do not call functions when jit is used                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1196 │   │   full_backward_hooks, non_full_backward_hooks = [], []         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1197 │   │   if self._backward_hooks or _global_backward_hooks:            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ches/execution.py:396 in distributed_forward                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   393 │   # When a module has no parameter, run as no partition. It's to res │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   394 │   # used in a multiple parent modules in which some parent module ha │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   395 │   if state.module_manager.is_executor(module) or num_params == 0:    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱ 396 │   │   outputs = module.execute_locally(*args, **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   397 │   elif state.module_manager.is_parent_executor(module):              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   398 │   │   outputs = execute_as_parent(module, *args, **kwargs)           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   399 │   else:       [1,mpirank:2,algo-1]<stderr>:                                                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ches/execution.py:338 in execute_module                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   335 │   │   │   preserve_rng_state=config.preserve_rng_state,              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   336 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   337 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱ 338 │   │   outputs = actual_forward(module, *args, **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   339 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   340 │   if not state.is_in_fwd_on_checkpointed_fn:                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   341 │   │   module_name = state.module_manager.get_module_name(module)     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ches/execution.py:276 in actual_forward                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   273 │   │   │   *tensors,                                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   274 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   275 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱ 276 │   │   outputs = module._orig_forward(*args, **kwargs)                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   277 │   return outputs                                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   278                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   279                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeli │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ng_distilbert.py:290 in forward                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    287 │   │   │   torch.tensor(bs, seq_length, dim) The output of the trans │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    288 │   │   \"\"\"                                                           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    289 │   │   # Self-Attention                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱  290 │   │   sa_output = self.attention(                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    291 │   │   │   query=x,                                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    292 │   │   │   key=x,                                                    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    293 │   │   │   value=x,                                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1194 in    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ _call_impl                                                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1191 │   │   # this function, and just call forward.                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1192 │   │   if not (self._backward_hooks or self._forward_hooks or self._ │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1193 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱ 1194 │   │   │   return forward_call(*input, **kwargs)                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1195 │   │   # Do not call functions when jit is used                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1196 │   │   full_backward_hooks, non_full_backward_hooks = [], []         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   1197 │   │   if self._backward_hooks or _global_backward_hooks:            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ che[1,mpirank:3,algo-1]<stderr>:                                                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ches/execution.py:338 in execute_module                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   335 │   │   │   preserve_rng_state=config.preserve_rng_state,              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   336 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   337 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱ 338 │   │   outputs = actual_forward(module, *args, **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   339 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   340 │   if not state.is_in_fwd_on_checkpointed_fn:                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   341 │   │   module_name = state.module_manager.get_module_name(module)     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ches/execution.py:276 in actual_forward                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   273 │   │   │   *tensors,                                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   274 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   275 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱ 276 │   │   outputs = module._orig_forward(*args, **kwargs)                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   277 │   return outputs                                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   278                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   279                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeli │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ng_distilbert.py:290 in forward                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    287 │   │   │   torch.tensor(bs, seq_length, dim) The output of the trans │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    288 │   │   \"\"\"                                                           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    289 │   │   # Self-Attention                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱  290 │   │   sa_output = self.attention(                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    291 │   │   │   query=x,                                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    292 │   │   │   key=x,                                                    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    293 │   │   │   value=x,                                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1194 in    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ _call_impl                                                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1191 │   │   # this function, and just call forward.                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1192 │   │   if not (self._backward_hooks or self._forward_hooks or self._ │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1193 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱ 1194 │   │   │   return forward_call(*input, **kwargs)                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1195 │   │   # Do not call functions when jit is used                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1196 │   │   full_backward_hooks, non_full_backward_hooks = [], []         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   1197 │   │   if self._backward_hooks or _global_backward_hooks:            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ che[1,mpirank:1,algo-1]<stderr>:                                                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ches/execution.py:338 in execute_module                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   335 │   │   │   preserve_rng_state=config.preserve_rng_state,              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   336 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   337 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱ 338 │   │   outputs = actual_forward(module, *args, **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   339 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   340 │   if not state.is_in_fwd_on_checkpointed_fn:                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   341 │   │   module_name = state.module_manager.get_module_name(module)     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ches/execution.py:276 in actual_forward                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   273 │   │   │   *tensors,                                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   274 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   275 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱ 276 │   │   outputs = module._orig_forward(*args, **kwargs)                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   277 │   return outputs                                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   278                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   279                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeli │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ng_distilbert.py:290 in forward                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    287 │   │   │   torch.tensor(bs, seq_length, dim) The output of the trans │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    288 │   │   \"\"\"                                                           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    289 │   │   # Self-Attention                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱  290 │   │   sa_output = self.attention(                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    291 │   │   │   query=x,                                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    292 │   │   │   key=x,                                                    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    293 │   │   │   value=x,                                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1194 in    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ _call_impl                                                                   │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1191 │   │   # this function, and just call forward.                       │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1192 │   │   if not (self._backward_hooks or self._forward_hooks or self._ │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1193 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱ 1194 │   │   │   return forward_call(*input, **kwargs)                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1195 │   │   # Do not call functions when jit is used                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1196 │   │   full_backward_hooks, non_full_backward_hooks = [], []         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   1197 │   │   if self._backward_hooks or _global_backward_hooks:            │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ che[1,mpirank:3,algo-1]<stderr>:s/execution.py:396 in distributed_forward                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   393 │   # When a module has no parameter, run as no partition. It's to res │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   394 │   # used in a multiple parent modules in which some parent module ha │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   395 │   if state.module_manager.is_executor(module) or num_params == 0:    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱ 396 │   │   outputs = module.execute_locally(*args, **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   397 │   elif state.module_manager.is_parent_executor(module):              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   398 │   │   outputs = execute_as_parent(module, *args, **kwargs)           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   399 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ches/execution.py:338 in execute_module                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   335 │   │   │   preserve_rng_state=config.preserve_rng_state,              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   336 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   337 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱ 338 │   │   outputs = actual_forward(module, *args, **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   339 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   340 │   if not state.is_in_fwd_on_checkpointed_fn:                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   341 │   │   module_name = state.module_manager.get_module_name(module)     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ches/execution.py:276 in actual_forward                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   273 │   │   │   *tensors,                                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   274 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   275 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱ 276 │   │   outputs = module._orig_forward(*args, **kwargs)                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   277 │   return outputs                                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   278                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│   279                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeli │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ng_distilbert.py:215 in forward                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    212 │   │   v = shape(self.v_lin(value))  # (bs, n_heads, k_length, dim_p │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    213 │   │                                                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    214 │   │   q = q / math.sqrt(dim_per_head)  # (bs, n_heads, q_length, di │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│ ❱  215 │   │   scores = torch.matmul(q, k.transpose(2, 3))  # (bs, n_heads,  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    216 │   │   mask = (mask == 0).view(mask_reshp).expand_as(scores)  # (bs, │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    217 │   │   scores = scores.masked_fill(                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:│    218 │   │   │   mask, torch.tensor(torch.finfo(scores.dtype).min)         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:OutOfMemoryError: CUDA out of memory. Tried to allocate 1.50 GiB (GPU 3; 15.77 \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:GiB total capacity; 12.88 GiB already allocated; 1.38 GiB free; 13.02 GiB \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:reserved in total by PyTorch) If reserv[1,mpirank:2,algo-1]<stderr>:s/execution.py:396 in distributed_forward                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   393 │   # When a module has no parameter, run as no partition. It's to res │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   394 │   # used in a multiple parent modules in which some parent module ha │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   395 │   if state.module_manager.is_executor(module) or num_params == 0:    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱ 396 │   │   outputs = module.execute_locally(*args, **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   397 │   elif state.module_manager.is_parent_executor(module):              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   398 │   │   outputs = execute_as_parent(module, *args, **kwargs)           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   399 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ches/execution.py:338 in execute_module                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   335 │   │   │   preserve_rng_state=config.preserve_rng_state,              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   336 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   337 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱ 338 │   │   outputs = actual_forward(module, *args, **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   339 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   340 │   if not state.is_in_fwd_on_checkpointed_fn:                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   341 │   │   module_name = state.module_manager.get_module_name(module)     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ches/execution.py:276 in actual_forward                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   273 │   │   │   *tensors,                                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   274 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   275 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱ 276 │   │   outputs = module._orig_forward(*args, **kwargs)                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   277 │   return outputs                                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   278                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│   279                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeli │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ng_distilbert.py:215 in forward                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    212 │   │   v = shape(self.v_lin(value))  # (bs, n_heads, k_length, dim_p │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    213 │   │                                                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    214 │   │   q = q / math.sqrt(dim_per_head)  # (bs, n_heads, q_length, di │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│ ❱  215 │   │   scores = torch.matmul(q, k.transpose(2, 3))  # (bs, n_heads,  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    216 │   │   mask = (mask == 0).view(mask_reshp).expand_as(scores)  # (bs, │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    217 │   │   scores = scores.masked_fill(                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:│    218 │   │   │   mask, torch.tensor(torch.finfo(scores.dtype).min)         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:OutOfMemoryError: CUDA out of memory. Tried to allocate 1.50 GiB (GPU 2; 15.77 \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:GiB total capacity; 12.88 GiB already allocated; 1.38 GiB free; 13.02 GiB \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:reserved in total by PyTorch) If reserv[1,mpirank:1,algo-1]<stderr>:s/execution.py:396 in distributed_forward                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   393 │   # When a module has no parameter, run as no partition. It's to res │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   394 │   # used in a multiple parent modules in which some parent module ha │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   395 │   if state.module_manager.is_executor(module) or num_params == 0:    │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱ 396 │   │   outputs = module.execute_locally(*args, **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   397 │   elif state.module_manager.is_parent_executor(module):              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   398 │   │   outputs = execute_as_parent(module, *args, **kwargs)           │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   399 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ches/execution.py:338 in execute_module                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   335 │   │   │   preserve_rng_state=config.preserve_rng_state,              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   336 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   337 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱ 338 │   │   outputs = actual_forward(module, *args, **kwargs)              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   339 │                                                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   340 │   if not state.is_in_fwd_on_checkpointed_fn:                         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   341 │   │   module_name = state.module_manager.get_module_name(module)     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/smdistributed/modelparallel/torch/pat │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ches/execution.py:276 in actual_forward                                      │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   273 │   │   │   *tensors,                                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   274 │   │   )                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   275 │   else:                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱ 276 │   │   outputs = module._orig_forward(*args, **kwargs)                │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   277 │   return outputs                                                     │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   278                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│   279                                                                        │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ /opt/conda/lib/python3.9/site-packages/transformers/models/distilbert/modeli │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ng_distilbert.py:215 in forward                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│                                                                              │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    212 │   │   v = shape(self.v_lin(value))  # (bs, n_heads, k_length, dim_p │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    213 │   │                                                                 │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    214 │   │   q = q / math.sqrt(dim_per_head)  # (bs, n_heads, q_length, di │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│ ❱  215 │   │   scores = torch.matmul(q, k.transpose(2, 3))  # (bs, n_heads,  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    216 │   │   mask = (mask == 0).view(mask_reshp).expand_as(scores)  # (bs, │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    217 │   │   scores = scores.masked_fill(                                  │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:│    218 │   │   │   mask, torch.tensor(torch.finfo(scores.dtype).min)         │\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:OutOfMemoryError: CUDA out of memory. Tried to allocate 1.50 GiB (GPU 1; 15.77 \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:GiB total capacity; 12.88 GiB already allocated; 1.45 GiB free; 13.02 GiB \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:reserved in total by PyTorch) If reserv[1,mpirank:2,algo-1]<stderr>:ed memory is >> allocated memory try \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:setting max_split_size_mb to avoid fragmentation.  See documentation for Memory \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:Management and PYTORCH_CUDA_ALLOC_CONF\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:ed memory is >> allocated memory try \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:setting max_split_size_mb to avoid fragmentation.  See documentation for Memory \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:Management and PYTORCH_CUDA_ALLOC_CONF\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:ed memory is >> allocated memory try \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:setting max_split_size_mb to avoid fragmentation.  See documentation for Memory \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:Management and PYTORCH_CUDA_ALLOC_CONF\u001b[0m\n",
      "\u001b[34m2023-09-22 20:35:16,512 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-09-22 20:35:16,512 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 1 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-09-22 20:35:16,514 sagemaker-training-toolkit ERROR    Reporting training FAILURE\u001b[0m\n",
      "\u001b[34m2023-09-22 20:35:16,514 sagemaker-training-toolkit ERROR    Framework Error: \u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sagemaker_training/trainer.py\", line 88, in train\n",
      "    entrypoint()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sagemaker_pytorch_container/training.py\", line 153, in main\n",
      "    train(environment.Environment())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sagemaker_pytorch_container/training.py\", line 100, in train\n",
      "    entry_point.run(uri=training_environment.module_dir,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sagemaker_training/entry_point.py\", line 99, in run\n",
      "    return runner.get(runner_type, user_entry_point, args, env_vars, extra_opts).run(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sagemaker_training/mpi.py\", line 398, in run\n",
      "    process_spawned = process.check_error(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sagemaker_training/process.py\", line 335, in check_error\n",
      "    raise error_class(\u001b[0m\n",
      "\u001b[34mTypeError: SMTrainingCompilerConfigurationError() takes no keyword arguments\u001b[0m\n",
      "\u001b[34mSMTrainingCompilerConfigurationError() takes no keyword arguments\u001b[0m\n",
      "\u001b[34m2023-09-22 20:35:16,514 sagemaker-training-toolkit ERROR    Encountered exit_code 1\u001b[0m\n",
      "\n",
      "2023-09-22 20:35:36 Uploading - Uploading generated training model\n",
      "2023-09-22 20:35:36 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job huggingface-pytorch-training-2023-09-22-20-27-32-127: Failed. Reason: AlgorithmError: Framework Error: \nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.9/site-packages/sagemaker_training/trainer.py\", line 88, in train\n    entrypoint()\n  File \"/opt/conda/lib/python3.9/site-packages/sagemaker_pytorch_container/training.py\", line 153, in main\n    train(environment.Environment())\n  File \"/opt/conda/lib/python3.9/site-packages/sagemaker_pytorch_container/training.py\", line 100, in train\n    entry_point.run(uri=training_environment.module_dir,\n  File \"/opt/conda/lib/python3.9/site-packages/sagemaker_training/entry_point.py\", line 99, in run\n    return runner.get(runner_type, user_entry_point, args, env_vars, extra_opts).run(\n  File \"/opt/conda/lib/python3.9/site-packages/sagemaker_training/mpi.py\", line 398, in run\n    process_spawned = process.check_error(\n  File \"/opt/conda/lib/python3.9/site-packages/sagemaker_training/process.py\", line 335, in check_error\n    raise error_class(\nTypeError: SMTrainingCompilerConfigurationError() takes no keyword arguments\n\nSMTrain",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-e0c48ba62521>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;31m# starting the train job with our uploaded datasets as input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mhuggingface_estimator_DMP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtraining_input_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_input_path\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/workflow/pipeline_context.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_StepArguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretrieve_caller_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_instance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrun_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1310\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   2578\u001b[0m         \u001b[0;31m# If logs are requested, call logs_for_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2579\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2580\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2581\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   4847\u001b[0m             \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnexpectedStatusException\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mwaiting\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mjob\u001b[0m \u001b[0mfails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4848\u001b[0m         \"\"\"\n\u001b[0;32m-> 4849\u001b[0;31m         \u001b[0m_logs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboto_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4851\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlogs_for_processing_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoll\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_logs_for_job\u001b[0;34m(boto_session, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   6758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6759\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6760\u001b[0;31m         \u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TrainingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6761\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6762\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   6814\u001b[0m             \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6815\u001b[0m             \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6816\u001b[0;31m             \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6817\u001b[0m         )\n\u001b[1;32m   6818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job huggingface-pytorch-training-2023-09-22-20-27-32-127: Failed. Reason: AlgorithmError: Framework Error: \nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.9/site-packages/sagemaker_training/trainer.py\", line 88, in train\n    entrypoint()\n  File \"/opt/conda/lib/python3.9/site-packages/sagemaker_pytorch_container/training.py\", line 153, in main\n    train(environment.Environment())\n  File \"/opt/conda/lib/python3.9/site-packages/sagemaker_pytorch_container/training.py\", line 100, in train\n    entry_point.run(uri=training_environment.module_dir,\n  File \"/opt/conda/lib/python3.9/site-packages/sagemaker_training/entry_point.py\", line 99, in run\n    return runner.get(runner_type, user_entry_point, args, env_vars, extra_opts).run(\n  File \"/opt/conda/lib/python3.9/site-packages/sagemaker_training/mpi.py\", line 398, in run\n    process_spawned = process.check_error(\n  File \"/opt/conda/lib/python3.9/site-packages/sagemaker_training/process.py\", line 335, in check_error\n    raise error_class(\nTypeError: SMTrainingCompilerConfigurationError() takes no keyword arguments\n\nSMTrain"
     ]
    }
   ],
   "source": [
    "#####################################################################\n",
    "######################### MODEL PARALLEL ############################\n",
    "#####################################################################\n",
    "\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "\n",
    "instance_type = 'ml.p3.16xlarge' #'ml.p3.16xlarge', 'ml.g5.12xlarge'\n",
    "partitions = 4\n",
    "Adj_BatchSize = 4\n",
    "LR_Adj_LoRA = 8\n",
    "\n",
    "if instance_type == 'ml.g5.12xlarge': #ValueError: Provided instance_type ml.g5.12xlarge is not supported by smdataparallel.\n",
    "    num_GPUs = 4\n",
    "elif instance_type == 'ml.p3.16xlarge':\n",
    "    num_GPUs = 8\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "if dataset_name == 'imdb':\n",
    "    hyperparameters={'epochs': 1,\n",
    "                     'train_batch_size': 32 * Adj_BatchSize, # 32\n",
    "                     'model_name':'distilbert-base-uncased',\n",
    "                     \"eval_batch_size\": 256, # InExample: 64\n",
    "                     \"learning_rate\": 5e-5 * (num_GPUs/partitions) * Adj_BatchSize * LR_Adj_LoRA # LR for DATA PARALLEL training is LR_Single_GPU * #OfGPUs * LoRA Multiplier!!!\n",
    "                    }\n",
    "\n",
    "# configuration for running training on smdistributed model parallel\n",
    "mpi_options = {\"enabled\" : True,\n",
    "               \"processes_per_host\" : 8}\n",
    "\n",
    "#\"\"\" # InExemple: -- but I had to change instance type and it results in GPU MEM ERROR\n",
    "\n",
    "smp_options = {\"enabled\":True,\n",
    "               \"parameters\": {\"microbatches\": 1, # InExample: 4 but changed to address the memory error\n",
    "                              \"placement_strategy\": \"spread\",\n",
    "                              \"pipeline\": \"interleaved\",\n",
    "                              \"optimize\": \"speed\",\n",
    "                              \"partitions\": 2, # InExample: 4 but changed to address the memory error\n",
    "                              \"ddp\": True,}}\n",
    "#\"\"\"\n",
    "\n",
    "distribution={\"smdistributed\": {\"modelparallel\": smp_options},\n",
    "              \"mpi\": mpi_options}\n",
    "\n",
    "#entry_point = 'train.py'\n",
    "entry_point = 'train-LORA.py'\n",
    "#entry_point = 'train-QLORA.py'\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator_DMP = HuggingFace(entry_point=entry_point,\n",
    "                                    source_dir='./',\n",
    "                                    instance_type=instance_type,\n",
    "                                    instance_count=1,\n",
    "                                    role=role,\n",
    "                                    transformers_version='4.26.0',\n",
    "                                    pytorch_version='1.13.1',\n",
    "                                    py_version='py39',\n",
    "                                    hyperparameters = hyperparameters,\n",
    "                                    distribution = distribution)\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator_DMP.fit({'train': training_input_path, 'test': test_input_path})\n",
    "\n",
    "\n",
    "### INSTANCE_TYPE: ml.p3.16xlarge -- ALL MODELs as G5 Instances are not supporte by Distirbuted SageMaker!!!\n",
    "\n",
    "\n",
    "\n",
    "## NO LORA\n",
    "\n",
    "###### \"microbatches\": 4, \"partitions\": 4,\n",
    "# MEM ERROR\n",
    "\n",
    "###### \"microbatches\": 1, \"partitions\": 4,\n",
    "# TRAIN EXEC TIME: 405.6870391368866\n",
    "\n",
    "### LR = 8 * LR_Single_GPU -- very weak possibly because DP is 2 not 8 due to 4 Partitions (PP)...\n",
    "# 'eval_accuracy': 0.8664, \n",
    "# 'eval_f1': 0.8531545394592219, \n",
    "# 'eval_precision': 0.9509924038225925, \n",
    "# 'eval_recall': 0.7735698624676102,\n",
    "\n",
    "### LR = 2 * LR_Single_GPU -- DECENT PERFORMANCE; 2x as DP is 2 due to 4 Partitions (PP)!!!\n",
    "# 'eval_accuracy': 0.9086, \n",
    "# 'eval_f1': 0.9041325781413887, \n",
    "# 'eval_precision': 0.9541731237547044, # VERY HIGH!!! \n",
    "# 'eval_recall': 0.8590791309547539,\n",
    "\n",
    "\n",
    "###### \"microbatches\": 2, \"partitions\": 4,\n",
    "# TRAIN EXEC TIME: 647.028391122818\n",
    "\n",
    "\n",
    "### IMPACT OF BATCH_SIZE (Standard BS = 32):\n",
    "\n",
    "## BS = 32 -- LR = LR_Single_GPU * 4 * 1 * 8 -- # 4x as DP is 4 due to 2 Partitions (PP) * 1 Adj_BatchSize * 8 as LoRA Adjustment\n",
    "\n",
    "# [1,mpirank:0,algo-1]<stderr>:#015100%|██████████| 196/196 [01:46<00:00,  1.89it/s]\n",
    "    \n",
    "# EXEC TIME: 121.97890901565552\n",
    "\n",
    "# 'eval_loss': 0.29470083117485046, \n",
    "# 'eval_accuracy': 0.8781, \n",
    "# 'eval_f1': 0.8800551018400079, \n",
    "# 'eval_precision': 0.8708860759493671, \n",
    "# 'eval_recall': 0.8894192521877486,\n",
    " \n",
    "\n",
    "## BS = 64 -- LR = LR_Single_GPU * 4 * 2 * 8 -- # 4x as DP is 4 due to 2 Partitions (PP) * 2 Adj_BatchSize * 8 as LoRA Adjustment\n",
    "\n",
    "# [1,mpirank:0,algo-1]<stderr>:#015 90%|████████▉ | 88/98 [01:31<00:09,  1.01it/s]\n",
    "    \n",
    "# EXEC TIME: 116.97890901565552\n",
    "\n",
    "# 'eval_loss': 0.26501214504241943, \n",
    "# 'eval_accuracy': 0.8889, \n",
    "# 'eval_f1': 0.8889999000899191, \n",
    "# 'eval_precision': 0.8931941377233488, \n",
    "# 'eval_recall': 0.8848448687350835,\n",
    "\n",
    "# BS = 256, 128 -- OOM Error!\n",
    "\n",
    "\n",
    "## BS = 64 -- LR = LR_Single_GPU * 2 * 2 * 8 -- # 2x as DP is 2 due to 4 Partitions (PP) * 2 Adj_BatchSize * 8 as LoRA Adjustment\n",
    "\n",
    "# [1,mpirank:0,algo-1]<stderr>:#015100%|██████████| 98/98 [01:57<00:00,  1.01it/s]\n",
    "\n",
    "# EXEC TIME: 117.58248496055603\n",
    "\n",
    "# 'eval_loss': 0.2440081536769867, \n",
    "# 'eval_accuracy': 0.8955, \n",
    "# 'eval_f1': 0.8903807825448442, \n",
    "# 'eval_precision': 0.9420643729189789, \n",
    "# 'eval_recall': 0.8440731901352426,\n",
    " \n",
    "## BS = 128 -- LR = LR_Single_GPU * 2 * 2 * 8 -- # 2x as DP is 2 due to 4 Partitions (PP) * 2 Adj_BatchSize * 8 as LoRA Adjustment\n",
    "\n",
    "# OOM Error!\n",
    "\n",
    "\n",
    "\n",
    "### WITH LORA\n",
    "\n",
    "###### \"microbatches\": 1, \"partitions\": 4,\n",
    "### LR = LR_Single_GPU * 2 * 8 -- # 2x as DP is 2 due to 4 Partitions (PP); * 8 as LoRA Adjustment\n",
    "\n",
    "# EXEC TIME: 364.84994435310364\n",
    "\n",
    "# 'eval_accuracy': 0.9117, \n",
    "# 'eval_f1': 0.910418991579588, \n",
    "# 'eval_precision': 0.9270661157024793, \n",
    "# 'eval_recall': 0.8943591787921068\n",
    "\n",
    "### LR = LR_Single_GPU * 2 * 16 -- # 2x as DP is 2 due to 4 Partitions (PP); * 16 as LoRA Adjsutment\n",
    "# 'eval_accuracy': 0.9162, \n",
    "# 'eval_f1': 0.9155412215279178, \n",
    "# 'eval_precision': 0.9259938837920489, \n",
    "# 'eval_recall': 0.9053219055212278,\n",
    "\n",
    "### LR = LR_Single_GPU * 2 * 32 -- # 2x as DP is 2 due to 4 Partitions (PP); * 32 as LoRA Adjsutment\n",
    "# 'eval_accuracy': 0.8947, \n",
    "# 'eval_f1': 0.8900490759110369, \n",
    "# 'eval_precision': 0.9346491228070175, \n",
    "# 'eval_recall': 0.8495116603547938\n",
    "\n",
    "###### \"microbatches\": 1, \"partitions\": 2,\n",
    "### LR = LR_Single_GPU * 2 * 8 -- # 2x was a mistake -- should be 4 as DP is 4 due to 2 Partitions (PP) -- * 8 as LoRA Adjsutment\n",
    "\n",
    "# EXEC TIME: 102.41368317604065\n",
    "\n",
    "# 'eval_accuracy': 0.9014, \n",
    "# 'eval_f1': 0.8990994678673762, \n",
    "# 'eval_precision': 0.923869610935857, \n",
    "# 'eval_recall': 0.8756228822005182,\n",
    "\n",
    "### LR = LR_Single_GPU * 4 * 8 -- # 4x as DP is 4 due to 2 Partitions (PP) * 8 as LoRA Adjsutment - BEST, MOST BALANCED, FASTEST!!!\n",
    "\n",
    "# EXEC TIME: 102.41368317604065\n",
    "\n",
    "# 'eval_accuracy': 0.9119, \n",
    "# 'eval_f1': 0.9117499749574276, \n",
    "# 'eval_precision': 0.9164317358034636, \n",
    "# 'eval_recall': 0.9071158062587203\n",
    "\n",
    "### LR = LR_Single_GPU * 4 * 8 -- # 4x as DP is 4 due to 2 Partitions (PP) * 16 as LoRA Adjsutment\n",
    "# 'eval_accuracy': 0.9144, \n",
    "# 'eval_f1': 0.9131493506493507, \n",
    "# 'eval_precision': 0.9299442033477991, \n",
    "# 'eval_recall': 0.8969503687462627,\n",
    "\n",
    "\n",
    "\n",
    "### IMPACT OF BATCH_SIZE (Standard BS = 32):\n",
    "# Assumption that increase of Batch_Size requires proportional increase in LR\n",
    "\n",
    "## BS = 64 LR = LR_Single_GPU * 4 * 1 * 8 -- # 4x as DP is 4 due to 2 Partitions (PP) * 1 Adj_BatchSize * 8 as LoRA Adjsutment\n",
    "\n",
    "# EXEC TIME: 99.77318120002747\n",
    "\n",
    "# 'eval_loss': 0.30923572182655334, \n",
    "# 'eval_accuracy': 0.8839, \n",
    "# 'eval_f1': 0.8823112012164217, \n",
    "# 'eval_precision': 0.8960263537162858,\n",
    "\n",
    "## BS = 64 LR = LR_Single_GPU * 4 * 2 * 8 -- # 4x as DP is 4 due to 2 Partitions (PP) * 2 Adj_BatchSize * 8 as LoRA Adjsutment\n",
    "\n",
    "# EXEC TIME: 99.77318120002747\n",
    "\n",
    "# 'eval_loss': 0.25893861055374146,\n",
    "# 'eval_accuracy': 0.8984,\n",
    "# 'eval_f1': 0.9001572327044025,\n",
    "# 'eval_precision': 0.8896658896658897,\n",
    "# 'eval_recall': 0.9108989657915673,\n",
    "\n",
    "# BS = 256, 128 -- OOM Error!\n",
    "\n",
    "## BS = 128 -- LR = LR_Single_GPU * 2 * 2 * 8 -- # 2x as DP is 2 due to 4 Partitions (PP) * 2 Adj_BatchSize * 8 as LoRA Adjustment\n",
    "\n",
    "# OOM Error!\n",
    "\n",
    "\n",
    "\n",
    "### WITH QLoRA -- FAILED EXECUTION!!!\n",
    "\n",
    "# QLORA with SMDMP fails for unknown reason resulting in a series of error statments unseen in the SMDDP mode.\n",
    "# It is expected that SMDPP mode will allow fine-tuning for a wide variaty of applications, thus further exploration of the issue will happen later.\n",
    "# TBC...\n",
    "\n",
    "\n",
    "# ERRORs:\n",
    "\n",
    "# Execution Freezes on:\n",
    "# #011Process OMPI jobid: [41117,1] App: 0 Process rank: 0 Bound: N/A -- IS N/A an issue?\n",
    "\n",
    "# SMDDPCollectivesInitWarning: The system is not compatible or not configured to run SMDDP collectives optimized for AWS infrastructure. \n",
    "# The training job will fall back to NCCL.\n",
    "\n",
    "# torch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED. \n",
    "# In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\n",
    "\n",
    "# W smdistributed/modelparallel/torch/optimizers/optimizer.py:111] parameter base_model.model.classifier.modules_to_save.default.bias \n",
    "# is missing when loading optimizer's state_dict, skip.\n",
    "\n",
    "\n",
    "# SUBSEQUENTLY, the trainig starts but freezes...\n",
    "# [1,mpirank:0,algo-1]<stderr>:#015  0%|          | 0/98 [00:00<?, ?it/s]\n",
    "\n",
    "\n",
    "# SOLUTION ATTEMPTS:\n",
    "\n",
    "# to test if it is a memory error we have worked with different setting of \"partition\" parmeter.\n",
    "\n",
    "# Since LoRA sytax worked in SMDMP mode we have created QLORA-MDP.py syntax\n",
    "# with which we tested rolling back the modifcations form LORA.py file intodcued in the QLORA.py file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "interpreter": {
   "hash": "c281c456f1b8161c8906f4af2c08ed2c40c50136979eaae69688b01f70e9f4a9"
  },
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
