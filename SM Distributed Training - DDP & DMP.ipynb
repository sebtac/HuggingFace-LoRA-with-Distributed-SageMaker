{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d08bb3-7e52-403b-9e81-3a46b469c081",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "OBJECTIVE:\n",
    "    Implement and Test different parameter options for DATA and MODEL PARALLEL DISTRIBUTED training on SageMaker\n",
    "\n",
    "BASED ON:\n",
    "    Introduction to DISRIBUTED TRAINING at:\n",
    "    https://towardsdatascience.com/smart-distributed-training-on-amazon-sagemaker-with-smd-part-1-cd296f87a0ee\n",
    "\n",
    "STEPS & FINDINGS:\n",
    "    1. SetUp\n",
    "    2. Prepare CIFAR-10 dataset\n",
    "    3. Distributed Data Parallel (DDP)\n",
    "        - Implemented with TF\n",
    "        - Not heavily tested as more tests of DDP are in different repository\n",
    "        - There needs to be at least as many input files as instances\n",
    "            - NEXT: Explore the consequences of that fact...\n",
    "        - HOROVOD/MPI requires EFA and specific instance type not availabel in my private account\n",
    "        - Also tested on MINIST but run into issues of internal structure of the data (mixing two projets); tbc...\n",
    "    4. Distirbuted Model Parallel (DMP):\n",
    "        - Main focus to test implmentation of Sharded Data Parallel Distriution (SDPD)\n",
    "            if SDPD == instance_count * \"processes_per_host\" then it is ZeRO\n",
    "            if SDPD < instance_count * \"processes_per_host\" than it is MiCS \n",
    "            MiCS =  ZeRO implementaiton by AWS (incomplete but tailored to Cloud Infrastreucture)\n",
    "\n",
    "        - @SM no mixing of SDPD and Pipline Parallelism\n",
    "        - @SM no mixing of SDPD and Tensor Parallelism\n",
    "        - The lower SDPD the faster (but minimally with the given data)\n",
    "        - The lower Pipeline Parallelism the faster -- possibly due to more efficient comms and parallel data distirbution\n",
    "        - SMD:MP:\"sdpd\": 1, SMD:MP:\"partitions\": 1 might equal to simple DDP\n",
    "        - Other Hyperparameters (Tested Individually):\n",
    "            \"horovod\": DEFAULT: False; TEST True  - 'ddp' and 'horovod' cannot be simultaneously enabled. - Slightly Slower\n",
    "            \"ddp\": DEFAULT: False; TEST True  - 'ddp' and 'horovod' cannot be simultaneously enabled.\n",
    "            \"tensor_parallel_degree\": DEFAULT: 1; TEST 2, 4, 8 - Slight improvment over DDP but increasing level of parallelism does not have impact\n",
    "            \"fp16\": DEFAULT: False; TEST True - \n",
    "            \"fp16_params\": False, # DEFAULT: False; TEST True --- REMOVED, USE \"fp16\"\n",
    "            \"bf16\": False, # DEFAULT: False; TEST True\n",
    "            \"offload_activations\": DEFAULT: False; TEST True - No Impact, but might be required for MEMORY MANAGEMENT\n",
    "            \"shard_optimizer_state\": DEFAULT: False; TEST True - No SDPD allowed, but slight improvment over DDP\n",
    "            \"activation_loading_horizon\": DEFAULT: 4; TEST 1 - Slight Improvement\n",
    "            \"microbaches\"and \"batch_size\": \n",
    "                - setting different values for those parameters had a big impact on the training speed\n",
    "                - results dependent on the level of model parallelism:\n",
    "                    - simetimes running into MEMORY issues!\n",
    "                    - needs to be tested and specified individualy per training job and take into account the max batch size available to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6ef165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET UP\n",
    "! pip install tensorflow\n",
    "\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker.session import TrainingInput\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b43171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA GENERATED WITH SYNTAX AT THE BOTTOM!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9804fbc-1d2c-4fe5-aa35-d537ec164ed1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SET FOR THE TRAINING JOB NAME\n",
    "run = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c486290d-5dde-4f3e-b593-e481c54f732e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "################ RUN Distributed DATA Parallel (DDP) ################\n",
    "#####################################################################\n",
    "\n",
    "s3_input_location_str = 's3://dis-tr-workshop/cifar10-dataset/' # USING ALREADY EXISTING BUILD - might not work or have an optimal performance!!!!\n",
    "#s3_input_location_str = 's3://mnist-tdrecords/train/120/' # USING ALREADY EXISTING BUILD - 120 SHARDS!\n",
    "\n",
    "s3_input = TrainingInput(s3_input_location_str,\n",
    "                         input_mode='FastFile') #'FastFile\n",
    "\n",
    "# Training using SMDataParallel Distributed Training Framework\n",
    "distribution = {'smdistributed':{'dataparallel':{'enabled': True,\n",
    "                                                'processes_per_host': 8 # this does not work -- tested with 4 and still 8 processes run\n",
    "                                                }}}\n",
    "\n",
    "# Training using HOROVOD/MPI -- Possibly runs only with 'ml.p3dn.24xlarge' instances as it needs EFA\n",
    "\"\"\"\n",
    "distribution = {'mpi': {'enabled': True,\n",
    "                        'processes_per_host': 8}}\n",
    "\"\"\"\n",
    "#The supported instance types:('ml.p3.16xlarge', 'ml.p3dn.24xlarge', 'ml.p4d.24xlarge', 'ml.p4de.24xlarge', 'local_gpu')\n",
    "\n",
    "tf_estimator = TensorFlow(entry_point='train_tf_DPD.py',\n",
    "                          role=role,\n",
    "                          instance_type='ml.p3.16xlarge', # InExample: 'ml.p4d.24xlarge' = $32.7726/h\n",
    "                                                          # 'ml.p3.16xlarge' = $24.48/h\n",
    "                                                          # 'ml.p3dn.24xlarge' = NoPrice Provided/h\n",
    "                                                          # 'ml.p4de.24xlarge' = NoPrice Provided/h\n",
    "                          instance_count=1, # InExample: 4\n",
    "                          framework_version='2.9', # InExample: '2.9.1'\n",
    "                          py_version='py39',\n",
    "                          distribution=distribution)\n",
    "\n",
    "run += 1\n",
    "\n",
    "tf_estimator.fit(s3_input,\n",
    "                 job_name = \"train-tf-DPD-\" + str(run),)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534d20d8-5b98-4b58-8f55-8fb686125546",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "########################## SUMMARY OF DDP ###########################\n",
    "#####################################################################\n",
    "\n",
    "# CIFAR-10\n",
    "'ml.p3.16xlarge' x1, smdistributed, no @tf.function\n",
    "EXEC TIME: 149.16670870780945\n",
    "EXEC TIME: 148.32054686546326\n",
    "\n",
    "'ml.p3.16xlarge' x1, smdistributed, WITH @tf.function\n",
    "EXEC TIME: 150.85319781303406\n",
    "\n",
    "'ml.p3.16xlarge' x2, smdistributed, no @tf.function\n",
    "requies > 2 files!!!!\n",
    "EXEC TIME: 175.32054686546326\n",
    "\n",
    "'ml.p3.16xlarge' x1, HOROVOD/MPI, no @tf.function\n",
    "ENDS WITH ERROR: EFA device name not set\n",
    "\n",
    "'ml.p4d.24xlarge' x1, smdistributed, no @tf.function\n",
    "NO INSANCE QUOTA FOR THE INSTANCE TYPE\n",
    "\n",
    "MINIST:\n",
    "    \n",
    "'ml.p3.16xlarge' x1, smdistributed, 120 SHARDS, no @tf.function\n",
    "# POSSIBLY the TFRECORD internal structure does not meet the .py requirments\n",
    "\n",
    "'ml.p3.16xlarge' x2, smdistributed, 120 SHARDS, no @tf.function\n",
    "# it seems it started on both instances although input folder name error killed it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b075bda4-0310-49d0-8b14-eaeb0b41b597",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "############### RUN Distributed MODEL Parallel (DMP) ################\n",
    "#####################################################################\n",
    "\n",
    "# NOTES:\n",
    "# \"processes_per_host\" -- HOW many GPUs per HOST/NODE\n",
    "# instance_count = 1, -- how many instances to provision\n",
    "# \"sharded_data_parallel_degree\" -- level of ZeRO model parallelism; \n",
    "                                  # if it == instance_count * \"processes_per_host\" then it is ZeRO \n",
    "                                  # if it is less than it is MiCS -- ZeRO implementaiton by AWS (incomplete but tailored to Cloud Infrastreucture)\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "# from sagemaker.pytorch.training_compiler.config import TrainingCompilerConfig # ValueError: SageMaker distributed training configuration is currently not compatible with SageMaker Training Compiler.\n",
    "role = get_execution_role()\n",
    "print(role)\n",
    "\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "distribution = {\"mpi\": {\"enabled\": True,\n",
    "                        \"processes_per_host\": 8, # InExample: 4\n",
    "                        #\"custom_mpi_options\": \"--mca btl_vader_single_copy_mechanism none\" #Use this key to pass any custom MPI options you might need. To avoid Docker warnings from contaminating your training logs, we recommend the following flag. \n",
    "                       },\n",
    "                #\"pytorchxla\": {\"enabled\": True},\n",
    "                \"smdistributed\": {#\"dataparallel\": {\"enabled\": True}, #choose one of the following supported strategies:['dataparallel', 'modelparallel']\n",
    "                                  \"modelparallel\": {\"enabled\": True,\n",
    "                                                    \"parameters\": {\"partitions\": 1, # Got an Error '\"partitions\" is a required parameter'!!!; controls pipeline_parallel_degree\n",
    "                                                                   \"ddp\": True, # PyTorch Specifc -- allows mix data+model parallel\n",
    "                                                                   \"sharded_data_parallel_degree\": 2,  # InExample: 32\n",
    "                                                                   \"delayed_parameter_initialization\": True,\n",
    "                                                                   \"microbatches\": 4, # DEFAULT: 1; TEST 4\n",
    "                                                                   \"horovod\": False, # DEFAULT: False; TEST True   ---- ValueError: 'ddp' and 'horovod' cannot be simultaneously enabled.\n",
    "                                                                   \"ddp\": True, # DEFAULT: False; TEST True   ---- ValueError: 'ddp' and 'horovod' cannot be simultaneously enabled.\n",
    "                                                                   \"tensor_parallel_degree\": 1, # DEFAULT: 1; TEST 2, ???\n",
    "                                                                   \"fp16\": False, # DEFAULT: False; TEST True\n",
    "                                                                   \"fp16_params\": False, # DEFAULT: False; TEST True --- DEPReciated\n",
    "                                                                   \"bf16\": True, # DEFAULT: False; TEST True\n",
    "                                                                   \"offload_activations\": True, # DEFAULT: False; TEST True\n",
    "                                                                   \"shard_optimizer_state\": False, # DEFAULT: False; TEST True\n",
    "                                                                   \"activation_loading_horizon\": 1, # DEFAULT: 4; TEST 1, ???  \n",
    "                                                                   \"placement_strategy\": \"cluster\" # DEFAULT: \"cluster\" (\"DPT\"); TEST \"spread\" (\"TPD\"), permutations of \"D,P,T\"\n",
    "                                                                  },}},}\n",
    "                        \n",
    "pytorch = PyTorch(entry_point='train_PT_MP.py',\n",
    "                  role=role,\n",
    "                  instance_type='ml.p3.16xlarge', # InExample: 'ml.g4dn.12xlarge' \n",
    "                                                  # 'ml.p4d.24xlarge' = $32.7726/h\n",
    "                                                  # 'ml.p3.16xlarge' = $24.48/h\n",
    "                                                  # 'ml.p3dn.24xlarge' = NoPrice Provided/h\n",
    "                                                  # 'ml.p4de.24xlarge' = NoPrice Provided/h\n",
    "                  instance_count=1,  # InExample: 8\n",
    "                  framework_version='1.12',\n",
    "                  py_version='py38',\n",
    "                  distribution=distribution)\n",
    "\n",
    "                  #compiler_config = TrainingCompilerConfig()) # ValueError: SageMaker distributed training configuration is currently not compatible with SageMaker Training Compiler.\n",
    "\n",
    "pytorch.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ad61c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38df167-f93b-409e-9cde-36767815b5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "########################## SUMMARY OF DPD ###########################\n",
    "#####################################################################\n",
    "\n",
    "# CIFAR-10\n",
    "'ml.p3.16xlarge' x1, MPI:\"pph\": 8, SMD:MP:\"sdpd\": 8, SMD:MP:\"partitions\": 1, FakeDataset_LEN: 100\n",
    "average/sum step time: 68.43147036700003\n",
    "\n",
    "'ml.p3.16xlarge' x1, MPI:\"pph\": 8, SMD:MP:\"sdpd\": 8, SMD:MP:\"partitions\": 2, FakeDataset_LEN: 100\n",
    "ERROR: Setting config parameter sharded_data_parallel_degree to non-default value 8 requires pipeline_parallel_degree to be set to 1. Found: 2\n",
    "\n",
    "'ml.p3.16xlarge' x1, MPI:\"pph\": 8, SMD:MP:\"sdpd\": 4, SMD:MP:\"partitions\": 2, FakeDataset_LEN: 100\n",
    "ERROR: Setting config parameter sharded_data_parallel_degree to non-default value 4 requires pipeline_parallel_degree to be set to 1. Found: 2\n",
    "\n",
    "'ml.p3.16xlarge' x1, MPI:\"pph\": 8, SMD:MP:\"sdpd\": 2, SMD:MP:\"partitions\": 4, FakeDataset_LEN: 100\n",
    "ERROR: Setting config parameter sharded_data_parallel_degree to non-default value 4 requires pipeline_parallel_degree to be set to 1. Found: 2\n",
    "\n",
    "\n",
    "### PIPELINE PARALLEL SEEMS NOT TO BE SUPPORTEDWITH SDPD!!!\n",
    "'ml.p3.16xlarge' x1, MPI:\"pph\": 8, SMD:MP:\"sdpd\": 1, SMD:MP:\"partitions\": 2, FakeDataset_LEN: 100\n",
    "[1,mpirank:0,algo-1]<stdout>:average/sum step time: 1.4530066869583347, 34.87216048700003\n",
    "\n",
    "'ml.p3.16xlarge' x1, MPI:\"pph\": 8, SMD:MP:\"sdpd\": 1, SMD:MP:\"partitions\": 4, FakeDataset_LEN: 100\n",
    "1,mpirank:0,algo-1]<stdout>:average/sum step time: 1.5864331667500007, 38.074396002000015\n",
    "\n",
    "'ml.p3.16xlarge' x1, MPI:\"pph\": 8, SMD:MP:\"sdpd\": 1, SMD:MP:\"partitions\": 8, FakeDataset_LEN: 100\n",
    "[1,mpirank:0,algo-1]<stdout>:average/sum step time: 2.142736179499998, 51.425668307999956\n",
    "\n",
    "\n",
    "'ml.p3.16xlarge' x1, MPI:\"pph\": 8, SMD:MP:\"sdpd\": 4, SMD:MP:\"partitions\": 1, FakeDataset_LEN: 100\n",
    "[1,mpirank:0,algo-1]<stdout>:average/sum step time: 2.6638444754999995, 63.93226741199999\n",
    "\n",
    "'ml.p3.16xlarge' x1, MPI:\"pph\": 8, SMD:MP:\"sdpd\": 2, SMD:MP:\"partitions\": 1, FakeDataset_LEN: 100 ######## BENCHMARK!!!\n",
    "[1,mpirank:0,algo-1]<stdout>:average/sum step time: 2.6146430564166656, 62.75143335399997\n",
    "\n",
    "\n",
    "# PROBABLY JUST DATA PARALLEL\n",
    "'ml.p3.16xlarge' x1, MPI:\"pph\": 8, SMD:MP:\"sdpd\": 1, SMD:MP:\"partitions\": 1, FakeDataset_LEN: 100\n",
    "[1,mpirank:0,algo-1]<stdout>:average/sum step time: 1.343366788208333, 32.240802916999996\n",
    "\n",
    "# ADDITIONAL PARAMS TESTS\n",
    "# DEFAULT VALUES\n",
    "microbaches: 1\n",
    "batch_size=8*8, # InExample: 4\n",
    "\n",
    "\"horovod\": False, # DEFAULT: False; TEST True  ---- ValueError: 'ddp' and 'horovod' cannot be simultaneously enabled.\n",
    "\"ddp\": True, # DEFAULT: False; TEST True  ---- ValueError: 'ddp' and 'horovod' cannot be simultaneously enabled.\n",
    "\"tensor_parallel_degree\": 1, # DEFAULT: 1; TEST 2, ??? --- Setting config parameter sharded_data_parallel_degree to non-default value 2 requires tensor_parallel_degree to be set to 1. Found: 2\n",
    "\"fp16\": DEFAULT: False; TEST True - no impact\n",
    "\"fp16_params\": DEFAULT: False; TEST True - DEPRECIATED, USE \"fp16\"\n",
    "\"bf16\": DEFAULT: False; TEST True - Slight improvement\n",
    "\"offload_activations\": DEFAULT: False; TEST True\n",
    "\"shard_optimizer_state\": False, # DEFAULT: False; TEST True --- Setting config parameter sharded_data_parallel_degree to non-default value 2 requires shard_optimizer_state to be set to False. Found: True\n",
    "\"activation_loading_horizon\": 4 # DEFAULT: 4; TEST 1, ???   \n",
    "                                    \n",
    "'ml.p3.16xlarge' x1, MPI:\"pph\": 8, SMD:MP:\"sdpd\": 2, SMD:MP:\"partitions\": 1, FakeDataset_LEN: 100, microbaches: 8, batch_size=4*8, # InExample: 4\n",
    "average/sum step time: 14.583918485999996, 43.75175545799999 --- # WARNINIG: 1 pytorch allocator cache flushes since last step. \n",
    "this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption.\n",
    "If you are unable to make the cache flushes go away consider adding torch.cuda.empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
    "\n",
    "'ml.p3.16xlarge' x1, MPI:\"pph\": 8, SMD:MP:\"sdpd\": 2, SMD:MP:\"partitions\": 1, FakeDataset_LEN: 100, \"horovod\": True, \"ddp\": False\n",
    "[1,mpirank:0,algo-1]<stdout>:average/sum step time: 2.7699321410416666, 66.478371385\n",
    "\n",
    "# tensor_parallel_degree REQUIRES SMD:MP:\"sdpd\": 1\n",
    "# Error: Setting config parameter sharded_data_parallel_degree to non-default value 2 requires tensor_parallel_degree to be set to 1. Found: 2\n",
    "'ml.p3.16xlarge' x1, MPI:\"pph\": 8, SMD:MP:\"sdpd\": 1, SMD:MP:\"partitions\": 1, FakeDataset_LEN: 100, \"tensor_parallel_degree\": 2,\n",
    "[1,mpirank:0,algo-1]<stdout>:average/sum step time: 1.2903158405, 30.967580171999998\n",
    "\n",
    "'ml.p3.16xlarge' x1, MPI:\"pph\": 8, SMD:MP:\"sdpd\": 1, SMD:MP:\"partitions\": 1, FakeDataset_LEN: 100, \"tensor_parallel_degree\": 4,\n",
    "[1,mpirank:0,algo-1]<stdout>:average/sum step time: 1.3078571323333321, 31.38857117599997\n",
    "\n",
    "'ml.p3.16xlarge' x1, MPI:\"pph\": 8, SMD:MP:\"sdpd\": 1, SMD:MP:\"partitions\": 1, FakeDataset_LEN: 100, \"tensor_parallel_degree\": 8,\n",
    "[1,mpirank:0,algo-1]<stdout>:average/sum step time: 1.2856828337083333, 30.856388009\n",
    "\n",
    "\n",
    "'ml.p3.16xlarge' x1, MPI:\"pph\": 8, SMD:MP:\"sdpd\": 2, SMD:MP:\"partitions\": 1, FakeDataset_LEN: 100, \"fp16\": True,\n",
    "[1,mpirank:0,algo-1]<stdout>:average/sum step time: 2.761540846416667, 66.27698031400001\n",
    "[1,mpirank:0,algo-1]<stdout>:average/sum step time: 1.123599046541668, 26.966377117000036 # SMD:MP:\"sdpd\": 1\n",
    "\n",
    "'ml.p3.16xlarge' x1, MPI:\"pph\": 8, SMD:MP:\"sdpd\": 2, SMD:MP:\"partitions\": 1, FakeDataset_LEN: 100, \"bf16\": True,\n",
    "[1,mpirank:0,algo-1]<stdout>:average/sum step time: 2.5171378690416666, 60.411308856999995\n",
    "[1,mpirank:0,algo-1]<stdout>:average/sum step time: 1.2476976044583334, 29.944742507 # SMD:MP:\"sdpd\": 1\n",
    "\n",
    "'ml.p3.16xlarge' x1, MPI:\"pph\": 8, SMD:MP:\"sdpd\": 2, SMD:MP:\"partitions\": 1, FakeDataset_LEN: 100, \"offload_activations\": True,\n",
    "[1,mpirank:0,algo-1]<stdout>:average/sum step time: 2.7027348693333337, 64.86563686400001\n",
    "[1,mpirank:0,algo-1]<stdout>:average/sum step time: 1.3232287461249992, 31.75748990699998 # SMD:MP:\"sdpd\": 1\n",
    "\n",
    "#\"shard_optimizer_state\": True, REQUIRES SMD:MP:\"sdpd\": 1  \n",
    "'ml.p3.16xlarge' x1, MPI:\"pph\": 8, SMD:MP:\"sdpd\": 1, SMD:MP:\"partitions\": 1, FakeDataset_LEN: 100, \"shard_optimizer_state\": True,\n",
    "[1,mpirank:0,algo-1]<stdout>:average/sum step time: 1.2148715330416664, 29.156916792999994\n",
    "\n",
    "'ml.p3.16xlarge' x1, MPI:\"pph\": 8, SMD:MP:\"sdpd\": 2, SMD:MP:\"partitions\": 1, FakeDataset_LEN: 100, \"activation_loading_horizon\": 1\n",
    "[1,mpirank:0,algo-1]<stdout>:average/sum step time: 2.6403854495833343, 63.369250790000024\n",
    "\n",
    "\n",
    "'ml.p3.16xlarge' x1, MPI:\"pph\": 8, SMD:MP:\"sdpd\": 2, SMD:MP:\"partitions\": 1, FakeDataset_LEN: 100, microbaches: 8, batch_size=4*8, \"bf16\": True, \"offload_activations\": True\n",
    "\"activation_loading_horizon\": 1\n",
    "[1,mpirank:0,algo-1]<stdout>:average/sum step time: 24.500388405999995, 73.50116521799998\n",
    "\n",
    "'ml.p3.16xlarge' x1, MPI:\"pph\": 8, SMD:MP:\"sdpd\": 2, SMD:MP:\"partitions\": 1, FakeDataset_LEN: 100, microbaches: 4, batch_size=4*8, \"bf16\": True, \"offload_activations\": True\n",
    "\"activation_loading_horizon\": 1\n",
    "[1,mpirank:0,algo-1]<stdout>:average/sum step time: 13.70485049233334, 41.11455147700002\n",
    "\n",
    "'ml.p3.16xlarge' x1, MPI:\"pph\": 8, SMD:MP:\"sdpd\": 2, SMD:MP:\"partitions\": 1, FakeDataset_LEN: 100, microbaches: 4, batch_size=4, \"bf16\": True, \"offload_activations\": True\n",
    "\"activation_loading_horizon\": 1\n",
    "[1,mpirank:0,algo-1]<stdout>:average/sum step time: 9.119615185258063, 282.70807074299995 ?!?!?!?!?!?\n",
    "\n",
    "'ml.p3.16xlarge' x1, MPI:\"pph\": 8, SMD:MP:\"sdpd\": 2, SMD:MP:\"partitions\": 1, FakeDataset_LEN: 100, microbaches: 4, batch_size=8*8, \"bf16\": True, \"offload_activations\": True\n",
    "\"activation_loading_horizon\": 1\n",
    "[1,mpirank:0,algo-1]<stdout>:average/sum step time: 26.91075615799997, 26.91075615799997"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980e1733-d4c6-4097-a32c-c623a3f34aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "########## DATA GENERATOR FOR DATA PARALLEL ##########\n",
    "######################################################\n",
    "\n",
    "# Loads Data from the web and saves in the \"data-cifar10\" folder\n",
    "\n",
    "%tb\n",
    "#     Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "#\n",
    "#     Licensed under the Apache License, Version 2.0 (the \"License\").\n",
    "#     You may not use this file except in compliance with the License.\n",
    "#     A copy of the License is located at\n",
    "#\n",
    "#         https://aws.amazon.com/apache-2-0/\n",
    "#\n",
    "#     or in the \"license\" file accompanying this file. This file is distributed\n",
    "#     on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n",
    "#     express or implied. See the License for the specific language governing\n",
    "#     permissions and limitations under the License.\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "\n",
    "# import tensorflow_datasets as tfds\n",
    "\n",
    "CIFAR_FILENAME = \"cifar-10-python.tar.gz\"\n",
    "CIFAR_DOWNLOAD_URL = \"https://www.cs.toronto.edu/~kriz/\" + CIFAR_FILENAME\n",
    "CIFAR_LOCAL_FOLDER = \"cifar-10-batches-py\"\n",
    "\n",
    "\n",
    "def download_and_extract(data_dir):\n",
    "    import tensorflow_datasets as tfds\n",
    "\n",
    "    dm = tfds.download.DownloadManager(download_dir=data_dir + \"/tmp\")\n",
    "    extract_dir = dm.download_and_extract(CIFAR_DOWNLOAD_URL)\n",
    "\n",
    "    return extract_dir\n",
    "\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def _get_file_names():\n",
    "    \"\"\"Returns the file names expected to exist in the input_dir.\"\"\"\n",
    "    file_names = {}\n",
    "    file_names[\"train\"] = [\"data_batch_%d\" % i for i in xrange(1, 5)]\n",
    "    file_names[\"validation\"] = [\"data_batch_5\"]\n",
    "    file_names[\"eval\"] = [\"test_batch\"]\n",
    "    return file_names\n",
    "\n",
    "\n",
    "def read_pickle_from_file(filename):\n",
    "    # with open(filename, 'rb') as f:\n",
    "    with tf.io.gfile.GFile(filename, \"rb\") as f:\n",
    "        if sys.version_info >= (3, 0):\n",
    "            data_dict = pickle.load(f, encoding=\"bytes\")\n",
    "        else:\n",
    "            data_dict = pickle.load(f)\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "def convert_to_tfrecord(input_files, output_file):\n",
    "    \"\"\"Converts a file to TFRecords.\"\"\"\n",
    "    print(\"Generating %s\" % output_file)\n",
    "    with tf.io.TFRecordWriter(output_file) as record_writer:\n",
    "        for input_file in input_files:\n",
    "            data_dict = read_pickle_from_file(input_file)\n",
    "            data = data_dict[b\"data\"]\n",
    "            labels = data_dict[b\"labels\"]\n",
    "\n",
    "            num_entries_in_batch = len(labels)\n",
    "            for i in range(num_entries_in_batch):\n",
    "                example = tf.train.Example(\n",
    "                    features=tf.train.Features(\n",
    "                        feature={\n",
    "                            \"image\": _bytes_feature(data[i].tobytes()),\n",
    "                            \"label\": _int64_feature(labels[i]),\n",
    "                        }\n",
    "                    )\n",
    "                )\n",
    "                record_writer.write(example.SerializeToString())\n",
    "\n",
    "\n",
    "def install_dependencies():\n",
    "    from subprocess import call\n",
    "\n",
    "    call([\"pip\", \"install\", \"--upgrade\", \"pip\"])\n",
    "    call([\"pip\", \"install\", \"tensorflow_datasets==4.1.0\"])\n",
    "\n",
    "\n",
    "def main(data_dir):\n",
    "    print(\"Download from {} and extract.\".format(CIFAR_DOWNLOAD_URL))\n",
    "\n",
    "    extract_dir = download_and_extract(data_dir)\n",
    "    file_names = _get_file_names()\n",
    "    input_dir = os.path.join(extract_dir, CIFAR_LOCAL_FOLDER)\n",
    "\n",
    "    for mode, files in file_names.items():\n",
    "        input_files = [os.path.join(input_dir, f) for f in files]\n",
    "        output_file = os.path.join(data_dir + \"/\" + mode, mode + \".tfrecords\")\n",
    "        if not os.path.exists(data_dir + \"/\" + mode):\n",
    "            os.makedirs(data_dir + \"/\" + mode)\n",
    "        try:\n",
    "            os.remove(output_file)\n",
    "        except OSError:\n",
    "            pass\n",
    "        # Convert to tf.train.Example and write the to TFRecords.\n",
    "        convert_to_tfrecord(input_files, output_file)\n",
    "    print(\"Done!\")\n",
    "    import shutil\n",
    "\n",
    "    shutil.rmtree(data_dir + \"/tmp\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\"--data-dir\",\n",
    "                        type=str,\n",
    "                        default=\"./data\",\n",
    "                        help=\"Directory to download and extract CIFAR-10 to.\",)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    \"\"\"\n",
    "    data_dir = \"./data-cifar-10\"\n",
    "\n",
    "    install_dependencies()\n",
    "    #\"\"\"\n",
    "    main(data_dir)\n",
    "    \n",
    "    #\"\"\"    "
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
