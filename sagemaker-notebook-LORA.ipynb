{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Distributed LoRA with HuggingFace SageMaker-SDK\n",
    "### Binary Classification with `Trainer` and `imdb` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ST NOTES:\n",
    "\n",
    "# OBJECTIVE:\n",
    "# To implement LoRA training in distributed SageMaker session with PyTorch and PEFT library\n",
    "\n",
    "# BASE SCRIPTs (No LoRA) from: https://huggingface.co/docs/sagemaker/train \n",
    "#                              https://github.com/huggingface/notebooks/blob/main/sagemaker/01_getting_started_pytorch/sagemaker-notebook.ipynb\n",
    "# LoRA uses PEFT library and the implementation is based on the Coursera's Generative AI Certificate\n",
    "# LoRA is a form of transfer learinig where we take pre-traind fundational model and fine-tune it to the specifc task/data\n",
    "# LoRA freezes the parameters of the fundational model and adds new weight matrices to the model that are trainable.\n",
    "# the new weight matrices capture the specificity of the new task/data and apply it on top if the broad language understaning of the fundational model\n",
    "# the new weight metrices are represeted by lower dimensional decompositions of those matrices.\n",
    "# \n",
    "# STEPs & FINDINGs:\n",
    "#    1. Prepare SM Environment\n",
    "#    2. Prepare the data file and load it to S3\n",
    "#    3. Modify .py file to add the PEFT/LoRA functionality\n",
    "#       - Add LoRA specific lines of code to the HuggingFace SageMaker SDK example notebook and .py file\n",
    "#.      - Reduces the trainable parameter set to 2.5% of the original\n",
    "#    4. Test on single GPU instance 'ml.p3.2xlarge' with PyTorch\n",
    "#       - Works without any additional modifications.\n",
    "#    5. Test in DDP mode of the SM-SDK with PyTorch\n",
    "#       - Above implementation results in Error: \"Model includes parameters that are not included in the calculation of a loss\"\n",
    "#       - Solution based on: https://github.com/pytorch/pytorch/issues/43259\n",
    "#       - Issue: SM DDP does not include some of the layers in the LOSS calculations\n",
    "#.      - Solution: specify which layers should have gradients calculated and which do not\n",
    "#                   - while this works, this solution needs to be analyzed further as \n",
    "#                     there is possibility that such modification alters the expected behaviour of the PEFT model\n",
    "#                     and/or we might freeze some layers that PEFT/LoRA sets as trainable\n",
    "#     6. Test on DMP\n",
    "#.       - not performed!\n",
    "#        - the objective PEFT/LoRA is to reduce the size of the trainiable part model to enable it \n",
    "#          to be trained with less hardware (GPUs, Memory, Instances,...)\n",
    "#          and current implementation reduces trainaible parameters to 2.5% of the full model count\n",
    "#.         thus I will workout LoRA with DMP when the need for it materializes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Introduction](#Introduction)  \n",
    "2. [Development Environment and Permissions](#Development-Environment-and-Permissions)\n",
    "    1. [Installation](#Installation)  \n",
    "    2. [Development environment](#Development-environment)  \n",
    "    3. [Permissions](#Permissions)\n",
    "3. [Processing](#Preprocessing)   \n",
    "    1. [Tokenization](#Tokenization)  \n",
    "    2. [Uploading data to sagemaker_session_bucket](#Uploading-data-to-sagemaker_session_bucket)  \n",
    "4. [Fine-tuning & starting Sagemaker Training Job](#Fine-tuning-\\&-starting-Sagemaker-Training-Job)  \n",
    "    1. [Creating an Estimator and start a training job](#Creating-an-Estimator-and-start-a-training-job)  \n",
    "    2. [Estimator Parameters](#Estimator-Parameters)   \n",
    "    3. [Download fine-tuned model from s3](#Download-fine-tuned-model-from-s3)\n",
    "    3. [Attach to old training job to an estimator ](#Attach-to-old-training-job-to-an-estimator)  \n",
    "5. [_Coming soon_:Push model to the Hugging Face hub](#Push-model-to-the-Hugging-Face-hub)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Welcome to our end-to-end binary Text-Classification example. In this demo, we will use the Hugging Faces `transformers` and `datasets` library together with a custom Amazon sagemaker-sdk extension to fine-tune a pre-trained transformer on binary text classification. In particular, the pre-trained model will be fine-tuned using the `imdb` dataset. To get started, we need to set up the environment with a few prerequisite steps, for permissions, configurations, and so on. \n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "_**NOTE: You can run this demo in Sagemaker Studio, your local machine or Sagemaker Notebook Instances**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development Environment and Permissions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "_*Note:* we only install the required libraries from Hugging Face and AWS. You also need PyTorch or Tensorflow, if you havent it installed_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker>=2.140.0 in /opt/conda/lib/python3.7/site-packages (2.165.0)\n",
      "Collecting sagemaker>=2.140.0\n",
      "  Using cached sagemaker-2.173.0-py2.py3-none-any.whl\n",
      "Collecting transformers==4.26.1\n",
      "  Using cached transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
      "Collecting datasets[s3]==2.10.1\n",
      "  Using cached datasets-2.10.1-py3-none-any.whl (469 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==4.26.1) (3.0.12)\n",
      "Collecting huggingface-hub<1.0,>=0.11.0 (from transformers==4.26.1)\n",
      "  Using cached huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.26.1) (1.21.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers==4.26.1) (20.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.26.1) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.26.1) (2023.6.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==4.26.1) (2.31.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.26.1)\n",
      "  Using cached tokenizers-0.13.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.26.1) (4.42.1)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers==4.26.1) (6.6.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets[s3]==2.10.1) (12.0.1)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from datasets[s3]==2.10.1) (0.3.6)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets[s3]==2.10.1) (1.3.5)\n",
      "Collecting tqdm>=4.27 (from transformers==4.26.1)\n",
      "  Using cached tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "Collecting xxhash (from datasets[s3]==2.10.1)\n",
      "  Using cached xxhash-3.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets[s3]==2.10.1) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.7/site-packages (from datasets[s3]==2.10.1) (2023.1.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets[s3]==2.10.1) (3.8.4)\n",
      "Collecting responses<0.19 (from datasets[s3]==2.10.1)\n",
      "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: s3fs in /opt/conda/lib/python3.7/site-packages (from datasets[s3]==2.10.1) (0.4.2)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.140.0) (23.1.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.26.131 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.140.0) (1.26.154)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.140.0) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.140.0) (0.2.0)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.140.0) (3.20.3)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.140.0) (1.0.1)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.140.0) (0.3.0)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.140.0) (0.7.5)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.140.0) (3.2.0)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.140.0) (3.5.3)\n",
      "Requirement already satisfied: tblib==1.7.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.140.0) (1.7.0)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.154 in /opt/conda/lib/python3.7/site-packages (from boto3<2.0,>=1.26.131->sagemaker>=2.140.0) (1.29.154)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3<2.0,>=1.26.131->sagemaker>=2.140.0) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from boto3<2.0,>=1.26.131->sagemaker>=2.140.0) (0.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets[s3]==2.10.1) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets[s3]==2.10.1) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets[s3]==2.10.1) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets[s3]==2.10.1) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets[s3]==2.10.1) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets[s3]==2.10.1) (1.3.1)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets[s3]==2.10.1) (0.13.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets[s3]==2.10.1) (4.6.3)\n",
      "Collecting packaging>=20.0 (from transformers==4.26.1)\n",
      "  Using cached packaging-23.1-py3-none-any.whl (48 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.26.1) (2.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.26.1) (2.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.26.1) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.26.1) (2023.5.7)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from google-pasta->sagemaker>=2.140.0) (1.14.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema->sagemaker>=2.140.0) (0.15.7)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from jsonschema->sagemaker>=2.140.0) (65.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets[s3]==2.10.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets[s3]==2.10.1) (2019.3)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker>=2.140.0) (1.7.6.6)\n",
      "Requirement already satisfied: pox>=0.3.2 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker>=2.140.0) (0.3.2)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.7/site-packages (from schema->sagemaker>=2.140.0) (0.6.0.post1)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers==4.26.1)\n",
      "  Using cached urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
      "Installing collected packages: tokenizers, xxhash, urllib3, tqdm, packaging, responses, huggingface-hub, transformers, sagemaker, datasets\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.0.3\n",
      "    Uninstalling urllib3-2.0.3:\n",
      "      Successfully uninstalled urllib3-2.0.3\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.42.1\n",
      "    Uninstalling tqdm-4.42.1:\n",
      "      Successfully uninstalled tqdm-4.42.1\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 20.1\n",
      "    Uninstalling packaging-20.1:\n",
      "      Successfully uninstalled packaging-20.1\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.165.0\n",
      "    Uninstalling sagemaker-2.165.0:\n",
      "      Successfully uninstalled sagemaker-2.165.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pytest-astropy 0.8.0 requires pytest-cov>=2.0, which is not installed.\n",
      "pytest-astropy 0.8.0 requires pytest-filter-subpackage>=0.1, which is not installed.\n",
      "spyder 4.0.1 requires pyqt5<5.13; python_version >= \"3\", which is not installed.\n",
      "spyder 4.0.1 requires pyqtwebengine<5.13; python_version >= \"3\", which is not installed.\n",
      "sparkmagic 0.20.4 requires nest-asyncio==1.5.5, but you have nest-asyncio 1.5.6 which is incompatible.\n",
      "spyder 4.0.1 requires jedi==0.14.1, but you have jedi 0.18.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-2.10.1 huggingface-hub-0.16.4 packaging-23.1 responses-0.18.0 sagemaker-2.173.0 tokenizers-0.13.3 tqdm-4.65.0 transformers-4.26.1 urllib3-1.26.16 xxhash-3.2.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install \"sagemaker>=2.140.0\" \"transformers==4.26.1\" \"datasets[s3]==2.10.1\" \"torch\" --upgrade\n",
    "!pip install \"sagemaker>=2.140.0\" \"transformers==4.26.1\" \"datasets[s3]==2.10.1\" --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker.huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::578864530451:role/service-role/AmazonSageMaker-ExecutionRole-20210306T201609\n",
      "sagemaker bucket: sagemaker-us-east-1-578864530451\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "We are using the `datasets` library to download and preprocess the `imdb` dataset. After preprocessing, the dataset will be uploaded to our `sagemaker_session_bucket` to be used within our training job. The [imdb](http://ai.stanford.edu/~amaas/data/sentiment/) dataset consists of 25000 training and 25000 testing highly polar movie reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer used in preprocessing\n",
    "tokenizer_name = 'distilbert-base-uncased'\n",
    "\n",
    "# dataset used\n",
    "dataset_name = 'imdb'\n",
    "\n",
    "# s3 key prefix for the data\n",
    "s3_prefix = 'samples/datasets/imdb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df411729fcbb45828c734c4576114ac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53cd30009d044798bdae5467cb9e2c6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-d16bb3b5a60af224.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load dataset\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "# download tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "# tokenizer helper function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding='max_length', truncation=True)\n",
    "\n",
    "# load dataset\n",
    "train_dataset, test_dataset = load_dataset('imdb', split=['train', 'test'])\n",
    "test_dataset = test_dataset.shuffle().select(range(10000)) # smaller the size for test dataset to 10k \n",
    "\n",
    "\n",
    "# tokenize dataset\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "# set format for pytorch\n",
    "train_dataset =  train_dataset.rename_column(\"label\", \"labels\")\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset = test_dataset.rename_column(\"label\", \"labels\")\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading data to `sagemaker_session_bucket`\n",
    "\n",
    "After we processed the `datasets` we are going to use the new `FileSystem` [integration](https://huggingface.co/docs/datasets/filesystems.html) to upload our dataset to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PATHs are also needed for training; SAVING files happens only once when data is generated initially\n",
    "\n",
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/train'\n",
    "#train_dataset.save_to_disk(training_input_path)\n",
    "\n",
    "# save test_dataset to s3\n",
    "test_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/test'\n",
    "#test_dataset.save_to_disk(test_input_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning & starting Sagemaker Training Job\n",
    "\n",
    "In order to create a sagemaker training job we need an `HuggingFace` Estimator. The Estimator handles end-to-end Amazon SageMaker training and deployment tasks. In a Estimator we define, which fine-tuning script should be used as `entry_point`, which `instance_type` should be used, which `hyperparameters` are passed in .....\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "huggingface_estimator = HuggingFace(entry_point='train-LORA.py',\n",
    "                            source_dir='./scripts',\n",
    "                            base_job_name='huggingface-sdk-extension',\n",
    "                            instance_type='ml.p3.2xlarge',\n",
    "                            instance_count=1,\n",
    "                            transformers_version='4.4',\n",
    "                            pytorch_version='1.6',\n",
    "                            py_version='py36',\n",
    "                            role=role,\n",
    "                            hyperparameters = {'epochs': 1,\n",
    "                                               'train_batch_size': 32,\n",
    "                                               'model_name':'distilbert-base-uncased'\n",
    "                                                })\n",
    "```\n",
    "\n",
    "When we create a SageMaker training job, SageMaker takes care of starting and managing all the required ec2 instances for us with the `huggingface` container, uploads the provided fine-tuning script `train.py` and downloads the data from our `sagemaker_session_bucket` into the container at `/opt/ml/input/data`. Then, it starts the training job by running. \n",
    "\n",
    "```python\n",
    "/opt/conda/bin/python train.py --epochs 1 --model_name distilbert-base-uncased --train_batch_size 32\n",
    "```\n",
    "\n",
    "The `hyperparameters` you define in the `HuggingFace` estimator are passed in as named arguments. \n",
    "\n",
    "Sagemaker is providing useful properties about the training environment through various environment variables, including the following:\n",
    "\n",
    "* `SM_MODEL_DIR`: A string that represents the path where the training job writes the model artifacts to. After training, artifacts in this directory are uploaded to S3 for model hosting.\n",
    "\n",
    "* `SM_NUM_GPUS`: An integer representing the number of GPUs available to the host.\n",
    "\n",
    "* `SM_CHANNEL_XXXX:` A string that represents the path to the directory that contains the input data for the specified channel. For example, if you specify two input channels in the HuggingFace estimators fit call, named `train` and `test`, the environment variables `SM_CHANNEL_TRAIN` and `SM_CHANNEL_TEST` are set.\n",
    "\n",
    "\n",
    "To run your training job locally you can define `instance_type='local'` or `instance_type='local_gpu'` for gpu usage. _Note: this does not working within SageMaker Studio_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: cannot read infile: [Errno 2] No such file or directory: './scripts/train.py'\n"
     ]
    }
   ],
   "source": [
    "!pygmentize ./scripts/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an Estimator and start a training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={'epochs': 1,\n",
    "                 'train_batch_size': 32,\n",
    "                 'model_name':'distilbert-base-uncased',\n",
    "                 \"eval_batch_size\": 256 # InExample: 64\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TEST on single instance as distibuted training might (and it does) intorduce its own issue\n",
    "\n",
    "huggingface_estimator = HuggingFace(entry_point='train-LORA.py',\n",
    "                            source_dir='./',\n",
    "                            instance_type='ml.p3.2xlarge',\n",
    "                            instance_count=1,\n",
    "                            role=role,\n",
    "                            transformers_version='4.26',\n",
    "                            pytorch_version='1.13',\n",
    "                            py_version='py39',\n",
    "                            hyperparameters = hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-pytorch-training-2023-07-25-11-40-33-719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-25 11:41:30 Starting - Starting the training job...\n",
      "2023-07-25 11:41:57 Starting - Preparing the instances for training.........\n",
      "2023-07-25 11:43:10 Downloading - Downloading input data...\n",
      "2023-07-25 11:43:35 Training - Downloading the training image.....................\n",
      "2023-07-25 11:47:21 Training - Training image download completed. Training in progress...\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-07-25 11:47:39,602 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-07-25 11:47:39,621 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-07-25 11:47:39,633 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-07-25 11:47:39,636 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-07-25 11:47:47,842 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-07-25 11:47:47,876 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-07-25 11:47:47,910 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-07-25 11:47:47,924 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 1,\n",
      "        \"eval_batch_size\": 256,\n",
      "        \"model_name\": \"distilbert-base-uncased\",\n",
      "        \"train_batch_size\": 32\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p3.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2023-07-25-11-40-33-719\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-578864530451/huggingface-pytorch-training-2023-07-25-11-40-33-719/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train-LORA\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train-LORA.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":1,\"eval_batch_size\":256,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":32}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train-LORA.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p3.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train-LORA\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-578864530451/huggingface-pytorch-training-2023-07-25-11-40-33-719/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p3.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":1,\"eval_batch_size\":256,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":32},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-pytorch-training-2023-07-25-11-40-33-719\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-578864530451/huggingface-pytorch-training-2023-07-25-11-40-33-719/source/sourcedir.tar.gz\",\"module_name\":\"train-LORA\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train-LORA.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"1\",\"--eval_batch_size\",\"256\",\"--model_name\",\"distilbert-base-uncased\",\"--train_batch_size\",\"32\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_BATCH_SIZE=256\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=distilbert-base-uncased\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=32\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 train-LORA.py --epochs 1 --eval_batch_size 256 --model_name distilbert-base-uncased --train_batch_size 32\u001b[0m\n",
      "\u001b[34m[2023-07-25 11:47:50.061: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m2023-07-25 11:47:50,067 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m2023-07-25 11:47:50,098 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mCollecting evaluate\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34m 81.4/81.4 kB 3.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2.28.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.9/site-packages (from evaluate) (4.64.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2023.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from evaluate) (23.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from evaluate) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from evaluate) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from evaluate) (1.5.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2022.12.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (1.26.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->evaluate) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->evaluate) (2022.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: evaluate\u001b[0m\n",
      "\u001b[34mSuccessfully installed evaluate-0.4.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mCollecting rouge_score\u001b[0m\n",
      "\u001b[34mDownloading rouge_score-0.1.2.tar.gz (17 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting absl-py\u001b[0m\n",
      "\u001b[34mDownloading absl_py-1.4.0-py3-none-any.whl (126 kB)\u001b[0m\n",
      "\u001b[34m 126.5/126.5 kB 7.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting nltk\u001b[0m\n",
      "\u001b[34mDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34m 1.5/1.5 MB 81.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (4.64.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (8.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (2022.10.31)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: rouge_score\u001b[0m\n",
      "\u001b[34mBuilding wheel for rouge_score (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for rouge_score (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=726bea613a536fbd835ad5b657e14901f7a4257972df13980daea496de808824\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/9b/3d/39/09558097d3119ca0a4d462df68f22c6f3c1b345ac63a09b86e\u001b[0m\n",
      "\u001b[34mSuccessfully built rouge_score\u001b[0m\n",
      "\u001b[34mInstalling collected packages: nltk, absl-py, rouge_score\u001b[0m\n",
      "\u001b[34mSuccessfully installed absl-py-1.4.0 nltk-3.8.1 rouge_score-0.1.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mCollecting loralib\u001b[0m\n",
      "\u001b[34mDownloading loralib-0.1.1-py3-none-any.whl (8.8 kB)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: loralib\u001b[0m\n",
      "\u001b[34mSuccessfully installed loralib-0.1.1\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mCollecting peft\u001b[0m\n",
      "\u001b[34mDownloading peft-0.4.0-py3-none-any.whl (72 kB)\u001b[0m\n",
      "\u001b[34m 72.9/72.9 kB 3.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: transformers in /opt/conda/lib/python3.9/site-packages (from peft) (4.26.0)\u001b[0m\n",
      "\u001b[34mCollecting safetensors\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.3.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m 1.3/1.3 MB 52.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from peft) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: accelerate in /opt/conda/lib/python3.9/site-packages (from peft) (0.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.9/site-packages (from peft) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from peft) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from peft) (23.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from peft) (5.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.13.0->peft) (4.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (2.28.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (0.13.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (2022.10.31)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (4.64.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (3.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (0.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft) (1.26.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft) (2022.12.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft) (3.4)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: safetensors, peft\u001b[0m\n",
      "\u001b[34mSuccessfully installed peft-0.4.0 safetensors-0.3.1\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34margs.output_data_dir /opt/ml/output/data\u001b[0m\n",
      "\u001b[34margs.model_dir /opt/ml/model\u001b[0m\n",
      "\u001b[34margs.n_gpus 1\u001b[0m\n",
      "\u001b[34margs.training_dir /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34margs.test_dir /opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34m2023-07-25 11:48:09,053 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m2023-07-25 11:48:09,053 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34mDownloading ()lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading ()lve/main/config.json: 100%|| 483/483 [00:00<00:00, 64.8kB/s]\u001b[0m\n",
      "\u001b[34mDownloading ()\"model.safetensors\";:   0%|          | 0.00/268M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading ()\"model.safetensors\";:  12%|        | 31.5M/268M [00:00<00:00, 273MB/s]\u001b[0m\n",
      "\u001b[34mDownloading ()\"model.safetensors\";:  23%|       | 62.9M/268M [00:00<00:00, 286MB/s]\u001b[0m\n",
      "\u001b[34mDownloading ()\"model.safetensors\";:  39%|      | 105M/268M [00:00<00:00, 333MB/s]\u001b[0m\n",
      "\u001b[34mDownloading ()\"model.safetensors\";:  55%|    | 147M/268M [00:00<00:00, 257MB/s]\u001b[0m\n",
      "\u001b[34mDownloading ()\"model.safetensors\";:  70%|   | 189M/268M [00:00<00:00, 287MB/s]\u001b[0m\n",
      "\u001b[34mDownloading ()\"model.safetensors\";:  86%| | 231M/268M [00:00<00:00, 294MB/s]\u001b[0m\n",
      "\u001b[34mDownloading ()\"model.safetensors\";: 100%|| 268M/268M [00:00<00:00, 314MB/s]\u001b[0m\n",
      "\u001b[34mDownloading ()\"model.safetensors\";: 100%|| 268M/268M [00:00<00:00, 298MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mORIGINAL model\u001b[0m\n",
      "\u001b[34mtrainable params: 66955010 || all params: 66955010 || trainable%: 100.00\u001b[0m\n",
      "\u001b[34mPEFT model BEFORE WRAP\u001b[0m\n",
      "\u001b[34mtrainable params: 1825540 || all params: 68136964 || trainable%: 2.68\u001b[0m\n",
      "\u001b[34m Traceback (most recent call last) \u001b[0m\n",
      "\u001b[34m /opt/ml/code/train-LORA.py:118 in <module>                                   \u001b[0m\n",
      "\u001b[34m                                                                              \u001b[0m\n",
      "\u001b[34m   115    print(\"PEFT model BEFORE WRAP\")                                    \u001b[0m\n",
      "\u001b[34m   116    print_trainable_parameters(model)                                  \u001b[0m\n",
      "\u001b[34m   117                                                                       \u001b[0m\n",
      "\u001b[34m  118    peft_model = torch.nn.parallel.DistributedDataParallel(peft_model, \u001b[0m\n",
      "\u001b[34m   119    #print(\"PEFT model\", peft_model)                                   \u001b[0m\n",
      "\u001b[34m   120                                                                       \u001b[0m\n",
      "\u001b[34m   121    print(\"PEFT model AFTER WRAP\")                                     \u001b[0m\n",
      "\u001b[34m                                                                              \u001b[0m\n",
      "\u001b[34m /opt/conda/lib/python3.9/site-packages/torch/nn/parallel/distributed.py:604  \u001b[0m\n",
      "\u001b[34m in __init__                                                                  \u001b[0m\n",
      "\u001b[34m                                                                              \u001b[0m\n",
      "\u001b[34m    601          self.output_device = _get_device_index(output_device, Tru \u001b[0m\n",
      "\u001b[34m    602                                                                     \u001b[0m\n",
      "\u001b[34m    603       if process_group is None:                                     \u001b[0m\n",
      "\u001b[34m   604          self.process_group = _get_default_group()                 \u001b[0m\n",
      "\u001b[34m    605       else:                                                         \u001b[0m\n",
      "\u001b[34m    606          self.process_group = process_group                        \u001b[0m\n",
      "\u001b[34m    607                                                                       \u001b[0m\n",
      "\u001b[34m                                                                              \u001b[0m\n",
      "\u001b[34m /opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py \u001b[0m\n",
      "\u001b[34m :584 in _get_default_group                                                   \u001b[0m\n",
      "\u001b[34m                                                                              \u001b[0m\n",
      "\u001b[34m    581    Getting the default process group created by init_process_group   \u001b[0m\n",
      "\u001b[34m    582    \"\"\"                                                               \u001b[0m\n",
      "\u001b[34m    583    if not is_initialized():                                          \u001b[0m\n",
      "\u001b[34m   584       raise RuntimeError(                                           \u001b[0m\n",
      "\u001b[34m    585          \"Default process group has not been initialized, \"        \u001b[0m\n",
      "\u001b[34m    586          \"please make sure to call init_process_group.\"            \u001b[0m\n",
      "\u001b[34m    587       )                                                             \u001b[0m\n",
      "\u001b[34m\u001b[0m\n",
      "\u001b[34mRuntimeError: Default process group has not been initialized, please make sure \u001b[0m\n",
      "\u001b[34mto call init_process_group.\u001b[0m\n",
      "\u001b[34m2023-07-25 11:48:11,632 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-07-25 11:48:11,632 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 1 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-07-25 11:48:11,633 sagemaker-training-toolkit ERROR    Reporting training FAILURE\u001b[0m\n",
      "\u001b[34m2023-07-25 11:48:11,633 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mExitCode 1\u001b[0m\n",
      "\u001b[34mErrorMessage \"   584       raise RuntimeError(                                           \n",
      "     585          \"Default process group has not been initialized, \"        \n",
      "     586          \"please make sure to call init_process_group.\"            \n",
      "     587       )                                                             \n",
      " \n",
      " RuntimeError: Default process group has not been initialized, please make sure\n",
      " to call init_process_group.\"\u001b[0m\n",
      "\u001b[34mCommand \"/opt/conda/bin/python3.9 train-LORA.py --epochs 1 --eval_batch_size 256 --model_name distilbert-base-uncased --train_batch_size 32\"\u001b[0m\n",
      "\u001b[34m2023-07-25 11:48:11,633 sagemaker-training-toolkit ERROR    Encountered exit_code 1\u001b[0m\n",
      "\n",
      "2023-07-25 11:48:33 Uploading - Uploading generated training model\n",
      "2023-07-25 11:48:33 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job huggingface-pytorch-training-2023-07-25-11-40-33-719: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"   584       raise RuntimeError(                                           \n     585          \"Default process group has not been initialized, \"        \n     586          \"please make sure to call init_process_group.\"            \n     587       )                                                             \n \n RuntimeError: Default process group has not been initialized, please make sure\n to call init_process_group.\"\nCommand \"/opt/conda/bin/python3.9 train-LORA.py --epochs 1 --eval_batch_size 256 --model_name distilbert-base-uncased --train_batch_size 32\", exit code: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-794ad4cbb591>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# starting the train job with our uploaded datasets as input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhuggingface_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtraining_input_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_input_path\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/workflow/pipeline_context.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_StepArguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretrieve_caller_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_instance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrun_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1290\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   2452\u001b[0m         \u001b[0;31m# If logs are requested, call logs_for_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2453\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2454\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2455\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2456\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   4812\u001b[0m             \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnexpectedStatusException\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mwaiting\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mjob\u001b[0m \u001b[0mfails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4813\u001b[0m         \"\"\"\n\u001b[0;32m-> 4814\u001b[0;31m         \u001b[0m_logs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboto_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4816\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlogs_for_processing_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoll\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_logs_for_job\u001b[0;34m(boto_session, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   6681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6682\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6683\u001b[0;31m         \u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TrainingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6684\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6685\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   6737\u001b[0m             \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6738\u001b[0m             \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6739\u001b[0;31m             \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6740\u001b[0m         )\n\u001b[1;32m   6741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job huggingface-pytorch-training-2023-07-25-11-40-33-719: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"   584       raise RuntimeError(                                           \n     585          \"Default process group has not been initialized, \"        \n     586          \"please make sure to call init_process_group.\"            \n     587       )                                                             \n \n RuntimeError: Default process group has not been initialized, please make sure\n to call init_process_group.\"\nCommand \"/opt/conda/bin/python3.9 train-LORA.py --epochs 1 --eval_batch_size 256 --model_name distilbert-base-uncased --train_batch_size 32\", exit code: 1"
     ]
    }
   ],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit({'train': training_input_path, 'test': test_input_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the endpoint - NOT PART OF THE TEST\n",
    "\n",
    "To deploy our endpoint, we call `deploy()` on our HuggingFace estimator object, passing in our desired number of instances and instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: huggingface-pytorch-training-2023-07-24-16-26-46-568\n",
      "INFO:sagemaker:Creating endpoint-config with name huggingface-pytorch-training-2023-07-24-16-26-46-568\n",
      "INFO:sagemaker:Creating endpoint with name huggingface-pytorch-training-2023-07-24-16-26-46-568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------!"
     ]
    }
   ],
   "source": [
    "predictor = huggingface_estimator.deploy(1, \"ml.g4dn.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we use the returned predictor object to call the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_1', 'score': 0.9107698798179626}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_input= {\"inputs\":\"I love using the new Inference DLC.\"}\n",
    "\n",
    "predictor.predict(sentiment_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we delete the endpoint again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting model with name: huggingface-pytorch-training-2023-07-24-16-26-46-568\n",
      "INFO:sagemaker:Deleting endpoint configuration with name: huggingface-pytorch-training-2023-07-24-16-26-46-568\n",
      "INFO:sagemaker:Deleting endpoint with name: huggingface-pytorch-training-2023-07-24-16-26-46-568\n"
     ]
    }
   ],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extras - NOT PART OF THE TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimator Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "container image used for training job: \n",
      "None\n",
      "\n",
      "s3 uri where the trained model is located: \n",
      "s3://sagemaker-us-east-1-578864530451/huggingface-pytorch-training-2023-07-24-16-10-46-427/output/model.tar.gz\n",
      "\n",
      "latest training job name for this estimator: \n",
      "huggingface-pytorch-training-2023-07-24-16-10-46-427\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# container image used for training job\n",
    "print(f\"container image used for training job: \\n{huggingface_estimator.image_uri}\\n\")\n",
    "\n",
    "# s3 uri where the trained model is located\n",
    "print(f\"s3 uri where the trained model is located: \\n{huggingface_estimator.model_data}\\n\")\n",
    "\n",
    "# latest training job name for this estimator\n",
    "print(f\"latest training job name for this estimator: \\n{huggingface_estimator.latest_training_job.name}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-24 16:25:39 Starting - Preparing the instances for training\n",
      "2023-07-24 16:25:39 Downloading - Downloading input data\n",
      "2023-07-24 16:25:39 Training - Training image download completed. Training in progress.\n",
      "2023-07-24 16:25:39 Uploading - Uploading generated training model\n",
      "2023-07-24 16:25:39 Completed - Training job completed\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-07-24 16:16:26,837 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-07-24 16:16:26,857 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-07-24 16:16:26,870 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-07-24 16:16:26,873 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-07-24 16:16:27,210 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-07-24 16:16:27,243 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-07-24 16:16:27,276 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-07-24 16:16:27,289 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 1,\n",
      "        \"model_name\": \"distilbert-base-uncased\",\n",
      "        \"train_batch_size\": 32\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p3.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2023-07-24-16-10-46-427\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-578864530451/huggingface-pytorch-training-2023-07-24-16-10-46-427/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":1,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":32}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p3.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-578864530451/huggingface-pytorch-training-2023-07-24-16-10-46-427/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p3.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":1,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":32},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-pytorch-training-2023-07-24-16-10-46-427\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-578864530451/huggingface-pytorch-training-2023-07-24-16-10-46-427/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"1\",\"--model_name\",\"distilbert-base-uncased\",\"--train_batch_size\",\"32\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=distilbert-base-uncased\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=32\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 train.py --epochs 1 --model_name distilbert-base-uncased --train_batch_size 32\u001b[0m\n",
      "\u001b[34m[2023-07-24 16:16:29.514: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m2023-07-24 16:16:29,521 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m2023-07-24 16:16:29,553 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34m2023-07-24 16:16:33,028 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m2023-07-24 16:16:33,028 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34mDownloading ()lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading ()lve/main/config.json: 100%|| 483/483 [00:00<00:00, 61.7kB/s]\u001b[0m\n",
      "\u001b[34mDownloading ()\"pytorch_model.bin\";:   0%|          | 0.00/268M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading ()\"pytorch_model.bin\";:   8%|         | 21.0M/268M [00:00<00:01, 197MB/s]\u001b[0m\n",
      "\u001b[34mDownloading ()\"pytorch_model.bin\";:  20%|        | 52.4M/268M [00:00<00:00, 245MB/s]\u001b[0m\n",
      "\u001b[34mDownloading ()\"pytorch_model.bin\";:  35%|      | 94.4M/268M [00:00<00:00, 269MB/s]\u001b[0m\n",
      "\u001b[34mDownloading ()\"pytorch_model.bin\";:  51%|     | 136M/268M [00:00<00:00, 280MB/s]\u001b[0m\n",
      "\u001b[34mDownloading ()\"pytorch_model.bin\";:  63%|   | 168M/268M [00:00<00:00, 270MB/s]\u001b[0m\n",
      "\u001b[34mDownloading ()\"pytorch_model.bin\";:  78%|  | 210M/268M [00:00<00:00, 274MB/s]\u001b[0m\n",
      "\u001b[34mDownloading ()\"pytorch_model.bin\";:  90%| | 241M/268M [00:00<00:00, 277MB/s]\u001b[0m\n",
      "\u001b[34mDownloading ()\"pytorch_model.bin\";: 100%|| 268M/268M [00:00<00:00, 277MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mDownloading ()okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading ()okenizer_config.json: 100%|| 28.0/28.0 [00:00<00:00, 10.1kB/s]\u001b[0m\n",
      "\u001b[34mDownloading ()solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading ()solve/main/vocab.txt: 100%|| 232k/232k [00:00<00:00, 60.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading ()/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading ()/main/tokenizer.json: 100%|| 466k/466k [00:00<00:00, 70.4MB/s]\u001b[0m\n",
      "\u001b[34mThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34mThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m***** Running training *****\u001b[0m\n",
      "\u001b[34mNum examples = 25000\u001b[0m\n",
      "\u001b[34m***** Running training *****\n",
      "  Num examples = 25000\u001b[0m\n",
      "\u001b[34mNum Epochs = 1\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\u001b[0m\n",
      "\u001b[34mNum Epochs = 1\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 782\u001b[0m\n",
      "\u001b[34mGradient Accumulation steps = 1\n",
      "  Total optimization steps = 782\u001b[0m\n",
      "\u001b[34mNumber of trainable parameters = 66955010\u001b[0m\n",
      "\u001b[34mNumber of trainable parameters = 66955010\u001b[0m\n",
      "\u001b[34m0%|          | 0/782 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2023-07-24 16:16:38.119: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m2023-07-24 16:16:38,125 - root - INFO - Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[2023-07-24 16:16:38.160 algo-1:49 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-07-24 16:16:38.199 algo-1:49 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-07-24 16:16:38.200 algo-1:49 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-07-24 16:16:38.200 algo-1:49 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-07-24 16:16:38.201 algo-1:49 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-07-24 16:16:38.201 algo-1:49 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34mYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m0%|          | 1/782 [00:01<20:46,  1.60s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 2/782 [00:02<12:01,  1.08it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 3/782 [00:02<09:13,  1.41it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 4/782 [00:02<07:54,  1.64it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 5/782 [00:03<07:11,  1.80it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 6/782 [00:03<06:44,  1.92it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 7/782 [00:04<06:27,  2.00it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 8/782 [00:04<06:16,  2.06it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 9/782 [00:05<06:09,  2.09it/s]\u001b[0m\n",
      "\u001b[34m1%|         | 10/782 [00:05<06:04,  2.12it/s]\u001b[0m\n",
      "\u001b[34m1%|         | 11/782 [00:06<06:00,  2.14it/s]\u001b[0m\n",
      "\u001b[34m2%|         | 12/782 [00:06<05:57,  2.16it/s]\u001b[0m\n",
      "\u001b[34m2%|         | 13/782 [00:07<05:55,  2.16it/s]\u001b[0m\n",
      "\u001b[34m2%|         | 14/782 [00:07<05:54,  2.17it/s]\u001b[0m\n",
      "\u001b[34m2%|         | 15/782 [00:07<05:52,  2.17it/s]\u001b[0m\n",
      "\u001b[34m2%|         | 16/782 [00:08<05:52,  2.17it/s]\u001b[0m\n",
      "\u001b[34m2%|         | 17/782 [00:08<05:52,  2.17it/s]\u001b[0m\n",
      "\u001b[34m2%|         | 18/782 [00:09<05:50,  2.18it/s]\u001b[0m\n",
      "\u001b[34m2%|         | 19/782 [00:09<05:49,  2.18it/s]\u001b[0m\n",
      "\u001b[34m3%|         | 20/782 [00:10<05:49,  2.18it/s]\u001b[0m\n",
      "\u001b[34m3%|         | 21/782 [00:10<05:48,  2.18it/s]\u001b[0m\n",
      "\u001b[34m3%|         | 22/782 [00:11<05:49,  2.18it/s]\u001b[0m\n",
      "\u001b[34m3%|         | 23/782 [00:11<05:48,  2.18it/s]\u001b[0m\n",
      "\u001b[34m3%|         | 24/782 [00:12<05:47,  2.18it/s]\u001b[0m\n",
      "\u001b[34m3%|         | 25/782 [00:12<05:47,  2.18it/s]\u001b[0m\n",
      "\u001b[34m3%|         | 26/782 [00:13<05:47,  2.18it/s]\u001b[0m\n",
      "\u001b[34m3%|         | 27/782 [00:13<05:47,  2.17it/s]\u001b[0m\n",
      "\u001b[34m4%|         | 28/782 [00:13<05:46,  2.18it/s]\u001b[0m\n",
      "\u001b[34m4%|         | 29/782 [00:14<05:45,  2.18it/s]\u001b[0m\n",
      "\u001b[34m4%|         | 30/782 [00:14<05:44,  2.18it/s]\u001b[0m\n",
      "\u001b[34m4%|         | 31/782 [00:15<05:43,  2.19it/s]\u001b[0m\n",
      "\u001b[34m4%|         | 32/782 [00:15<05:43,  2.19it/s]\u001b[0m\n",
      "\u001b[34m4%|         | 33/782 [00:16<05:42,  2.19it/s]\u001b[0m\n",
      "\u001b[34m4%|         | 34/782 [00:16<05:42,  2.19it/s]\u001b[0m\n",
      "\u001b[34m4%|         | 35/782 [00:17<05:41,  2.19it/s]\u001b[0m\n",
      "\u001b[34m5%|         | 36/782 [00:17<05:41,  2.18it/s]\u001b[0m\n",
      "\u001b[34m5%|         | 37/782 [00:18<05:40,  2.19it/s]\u001b[0m\n",
      "\u001b[34m5%|         | 38/782 [00:18<05:39,  2.19it/s]\u001b[0m\n",
      "\u001b[34m5%|         | 39/782 [00:18<05:38,  2.19it/s]\u001b[0m\n",
      "\u001b[34m5%|         | 40/782 [00:19<05:38,  2.19it/s]\u001b[0m\n",
      "\u001b[34m5%|         | 41/782 [00:19<05:38,  2.19it/s]\u001b[0m\n",
      "\u001b[34m5%|         | 42/782 [00:20<05:37,  2.19it/s]\u001b[0m\n",
      "\u001b[34m5%|         | 43/782 [00:20<05:36,  2.19it/s]\u001b[0m\n",
      "\u001b[34m6%|         | 44/782 [00:21<05:36,  2.20it/s]\u001b[0m\n",
      "\u001b[34m6%|         | 45/782 [00:21<05:36,  2.19it/s]\u001b[0m\n",
      "\u001b[34m6%|         | 46/782 [00:22<05:36,  2.19it/s]\u001b[0m\n",
      "\u001b[34m6%|         | 47/782 [00:22<05:35,  2.19it/s]\u001b[0m\n",
      "\u001b[34m6%|         | 48/782 [00:23<05:36,  2.18it/s]\u001b[0m\n",
      "\u001b[34m6%|         | 49/782 [00:23<05:36,  2.18it/s]\u001b[0m\n",
      "\u001b[34m6%|         | 50/782 [00:24<05:35,  2.18it/s]\u001b[0m\n",
      "\u001b[34m7%|         | 51/782 [00:24<05:34,  2.18it/s]\u001b[0m\n",
      "\u001b[34m7%|         | 52/782 [00:24<05:35,  2.18it/s]\u001b[0m\n",
      "\u001b[34m7%|         | 53/782 [00:25<05:34,  2.18it/s]\u001b[0m\n",
      "\u001b[34m7%|         | 54/782 [00:25<05:34,  2.18it/s]\u001b[0m\n",
      "\u001b[34m7%|         | 55/782 [00:26<05:33,  2.18it/s]\u001b[0m\n",
      "\u001b[34m7%|         | 56/782 [00:26<05:33,  2.17it/s]\u001b[0m\n",
      "\u001b[34m7%|         | 57/782 [00:27<05:33,  2.18it/s]\u001b[0m\n",
      "\u001b[34m7%|         | 58/782 [00:27<05:32,  2.18it/s]\u001b[0m\n",
      "\u001b[34m8%|         | 59/782 [00:28<05:31,  2.18it/s]\u001b[0m\n",
      "\u001b[34m8%|         | 60/782 [00:28<05:31,  2.18it/s]\u001b[0m\n",
      "\u001b[34m8%|         | 61/782 [00:29<05:31,  2.18it/s]\u001b[0m\n",
      "\u001b[34m8%|         | 62/782 [00:29<05:31,  2.17it/s]\u001b[0m\n",
      "\u001b[34m8%|         | 63/782 [00:29<05:30,  2.17it/s]\u001b[0m\n",
      "\u001b[34m8%|         | 64/782 [00:30<05:29,  2.18it/s]\u001b[0m\n",
      "\u001b[34m8%|         | 65/782 [00:30<05:29,  2.18it/s]\u001b[0m\n",
      "\u001b[34m8%|         | 66/782 [00:31<05:28,  2.18it/s]\u001b[0m\n",
      "\u001b[34m9%|         | 67/782 [00:31<05:28,  2.18it/s]\u001b[0m\n",
      "\u001b[34m9%|         | 68/782 [00:32<05:28,  2.17it/s]\u001b[0m\n",
      "\u001b[34m9%|         | 69/782 [00:32<05:28,  2.17it/s]\u001b[0m\n",
      "\u001b[34m9%|         | 70/782 [00:33<05:27,  2.17it/s]\u001b[0m\n",
      "\u001b[34m9%|         | 71/782 [00:33<05:26,  2.18it/s]\u001b[0m\n",
      "\u001b[34m9%|         | 72/782 [00:34<05:26,  2.18it/s]\u001b[0m\n",
      "\u001b[34m9%|         | 73/782 [00:34<05:26,  2.17it/s]\u001b[0m\n",
      "\u001b[34m9%|         | 74/782 [00:35<05:26,  2.17it/s]\u001b[0m\n",
      "\u001b[34m10%|         | 75/782 [00:35<05:25,  2.17it/s]\u001b[0m\n",
      "\u001b[34m10%|         | 76/782 [00:35<05:24,  2.18it/s]\u001b[0m\n",
      "\u001b[34m10%|         | 77/782 [00:36<05:24,  2.17it/s]\u001b[0m\n",
      "\u001b[34m10%|         | 78/782 [00:36<05:23,  2.18it/s]\u001b[0m\n",
      "\u001b[34m10%|         | 79/782 [00:37<05:22,  2.18it/s]\u001b[0m\n",
      "\u001b[34m10%|         | 80/782 [00:37<05:23,  2.17it/s]\u001b[0m\n",
      "\u001b[34m10%|         | 81/782 [00:38<05:22,  2.17it/s]\u001b[0m\n",
      "\u001b[34m10%|         | 82/782 [00:38<05:22,  2.17it/s]\u001b[0m\n",
      "\u001b[34m11%|         | 83/782 [00:39<05:22,  2.17it/s]\u001b[0m\n",
      "\u001b[34m11%|         | 84/782 [00:39<05:22,  2.17it/s]\u001b[0m\n",
      "\u001b[34m11%|         | 85/782 [00:40<05:21,  2.17it/s]\u001b[0m\n",
      "\u001b[34m11%|         | 86/782 [00:40<05:20,  2.17it/s]\u001b[0m\n",
      "\u001b[34m11%|         | 87/782 [00:41<05:19,  2.18it/s]\u001b[0m\n",
      "\u001b[34m11%|        | 88/782 [00:41<05:18,  2.18it/s]\u001b[0m\n",
      "\u001b[34m11%|        | 89/782 [00:41<05:17,  2.18it/s]\u001b[0m\n",
      "\u001b[34m12%|        | 90/782 [00:42<05:17,  2.18it/s]\u001b[0m\n",
      "\u001b[34m12%|        | 91/782 [00:42<05:17,  2.17it/s]\u001b[0m\n",
      "\u001b[34m12%|        | 92/782 [00:43<05:17,  2.17it/s]\u001b[0m\n",
      "\u001b[34m12%|        | 93/782 [00:43<05:17,  2.17it/s]\u001b[0m\n",
      "\u001b[34m12%|        | 94/782 [00:44<05:17,  2.17it/s]\u001b[0m\n",
      "\u001b[34m12%|        | 95/782 [00:44<05:16,  2.17it/s]\u001b[0m\n",
      "\u001b[34m12%|        | 96/782 [00:45<05:16,  2.17it/s]\u001b[0m\n",
      "\u001b[34m12%|        | 97/782 [00:45<05:15,  2.17it/s]\u001b[0m\n",
      "\u001b[34m13%|        | 98/782 [00:46<05:15,  2.16it/s]\u001b[0m\n",
      "\u001b[34m13%|        | 99/782 [00:46<05:15,  2.17it/s]\u001b[0m\n",
      "\u001b[34m13%|        | 100/782 [00:47<05:14,  2.17it/s]\u001b[0m\n",
      "\u001b[34m13%|        | 101/782 [00:47<05:14,  2.17it/s]\u001b[0m\n",
      "\u001b[34m13%|        | 102/782 [00:47<05:13,  2.17it/s]\u001b[0m\n",
      "\u001b[34m13%|        | 103/782 [00:48<05:13,  2.17it/s]\u001b[0m\n",
      "\u001b[34m13%|        | 104/782 [00:48<05:12,  2.17it/s]\u001b[0m\n",
      "\u001b[34m13%|        | 105/782 [00:49<05:12,  2.17it/s]\u001b[0m\n",
      "\u001b[34m14%|        | 106/782 [00:49<05:11,  2.17it/s]\u001b[0m\n",
      "\u001b[34m14%|        | 107/782 [00:50<05:11,  2.17it/s]\u001b[0m\n",
      "\u001b[34m14%|        | 108/782 [00:50<05:10,  2.17it/s]\u001b[0m\n",
      "\u001b[34m14%|        | 109/782 [00:51<05:09,  2.17it/s]\u001b[0m\n",
      "\u001b[34m14%|        | 110/782 [00:51<05:09,  2.17it/s]\u001b[0m\n",
      "\u001b[34m14%|        | 111/782 [00:52<05:09,  2.17it/s]\u001b[0m\n",
      "\u001b[34m14%|        | 112/782 [00:52<05:08,  2.17it/s]\u001b[0m\n",
      "\u001b[34m14%|        | 113/782 [00:53<05:07,  2.17it/s]\u001b[0m\n",
      "\u001b[34m15%|        | 114/782 [00:53<05:06,  2.18it/s]\u001b[0m\n",
      "\u001b[34m15%|        | 115/782 [00:53<05:06,  2.18it/s]\u001b[0m\n",
      "\u001b[34m15%|        | 116/782 [00:54<05:05,  2.18it/s]\u001b[0m\n",
      "\u001b[34m15%|        | 117/782 [00:54<05:05,  2.17it/s]\u001b[0m\n",
      "\u001b[34m15%|        | 118/782 [00:55<05:05,  2.17it/s]\u001b[0m\n",
      "\u001b[34m15%|        | 119/782 [00:55<05:04,  2.18it/s]\u001b[0m\n",
      "\u001b[34m15%|        | 120/782 [00:56<05:04,  2.17it/s]\u001b[0m\n",
      "\u001b[34m15%|        | 121/782 [00:56<05:04,  2.17it/s]\u001b[0m\n",
      "\u001b[34m16%|        | 122/782 [00:57<05:03,  2.18it/s]\u001b[0m\n",
      "\u001b[34m16%|        | 123/782 [00:57<05:02,  2.18it/s]\u001b[0m\n",
      "\u001b[34m16%|        | 124/782 [00:58<05:02,  2.18it/s]\u001b[0m\n",
      "\u001b[34m16%|        | 125/782 [00:58<05:01,  2.18it/s]\u001b[0m\n",
      "\u001b[34m16%|        | 126/782 [00:58<05:00,  2.18it/s]\u001b[0m\n",
      "\u001b[34m16%|        | 127/782 [00:59<05:00,  2.18it/s]\u001b[0m\n",
      "\u001b[34m16%|        | 128/782 [00:59<04:59,  2.18it/s]\u001b[0m\n",
      "\u001b[34m16%|        | 129/782 [01:00<04:58,  2.18it/s]\u001b[0m\n",
      "\u001b[34m17%|        | 130/782 [01:00<04:58,  2.18it/s]\u001b[0m\n",
      "\u001b[34m17%|        | 131/782 [01:01<04:58,  2.18it/s]\u001b[0m\n",
      "\u001b[34m17%|        | 132/782 [01:01<04:58,  2.18it/s]\u001b[0m\n",
      "\u001b[34m17%|        | 133/782 [01:02<04:57,  2.18it/s]\u001b[0m\n",
      "\u001b[34m17%|        | 134/782 [01:02<04:58,  2.17it/s]\u001b[0m\n",
      "\u001b[34m17%|        | 135/782 [01:03<04:58,  2.17it/s]\u001b[0m\n",
      "\u001b[34m17%|        | 136/782 [01:03<04:58,  2.17it/s]\u001b[0m\n",
      "\u001b[34m18%|        | 137/782 [01:04<04:57,  2.17it/s]\u001b[0m\n",
      "\u001b[34m18%|        | 138/782 [01:04<04:56,  2.17it/s]\u001b[0m\n",
      "\u001b[34m18%|        | 139/782 [01:04<04:55,  2.17it/s]\u001b[0m\n",
      "\u001b[34m18%|        | 140/782 [01:05<04:55,  2.17it/s]\u001b[0m\n",
      "\u001b[34m18%|        | 141/782 [01:05<04:54,  2.17it/s]\u001b[0m\n",
      "\u001b[34m18%|        | 142/782 [01:06<04:53,  2.18it/s]\u001b[0m\n",
      "\u001b[34m18%|        | 143/782 [01:06<04:52,  2.18it/s]\u001b[0m\n",
      "\u001b[34m18%|        | 144/782 [01:07<04:52,  2.18it/s]\u001b[0m\n",
      "\u001b[34m19%|        | 145/782 [01:07<04:51,  2.18it/s]\u001b[0m\n",
      "\u001b[34m19%|        | 146/782 [01:08<04:51,  2.18it/s]\u001b[0m\n",
      "\u001b[34m19%|        | 147/782 [01:08<04:50,  2.19it/s]\u001b[0m\n",
      "\u001b[34m19%|        | 148/782 [01:09<04:50,  2.19it/s]\u001b[0m\n",
      "\u001b[34m19%|        | 149/782 [01:09<04:49,  2.19it/s]\u001b[0m\n",
      "\u001b[34m19%|        | 150/782 [01:09<04:49,  2.18it/s]\u001b[0m\n",
      "\u001b[34m19%|        | 151/782 [01:10<04:48,  2.19it/s]\u001b[0m\n",
      "\u001b[34m19%|        | 152/782 [01:10<04:48,  2.19it/s]\u001b[0m\n",
      "\u001b[34m20%|        | 153/782 [01:11<04:47,  2.19it/s]\u001b[0m\n",
      "\u001b[34m20%|        | 154/782 [01:11<04:47,  2.18it/s]\u001b[0m\n",
      "\u001b[34m20%|        | 155/782 [01:12<04:47,  2.18it/s]\u001b[0m\n",
      "\u001b[34m20%|        | 156/782 [01:12<04:47,  2.18it/s]\u001b[0m\n",
      "\u001b[34m20%|        | 157/782 [01:13<04:46,  2.18it/s]\u001b[0m\n",
      "\u001b[34m20%|        | 158/782 [01:13<04:46,  2.18it/s]\u001b[0m\n",
      "\u001b[34m20%|        | 159/782 [01:14<04:45,  2.18it/s]\u001b[0m\n",
      "\u001b[34m20%|        | 160/782 [01:14<04:45,  2.18it/s]\u001b[0m\n",
      "\u001b[34m21%|        | 161/782 [01:15<04:45,  2.18it/s]\u001b[0m\n",
      "\u001b[34m21%|        | 162/782 [01:15<04:45,  2.17it/s]\u001b[0m\n",
      "\u001b[34m21%|        | 163/782 [01:15<04:44,  2.18it/s]\u001b[0m\n",
      "\u001b[34m21%|        | 164/782 [01:16<04:44,  2.17it/s]\u001b[0m\n",
      "\u001b[34m21%|        | 165/782 [01:16<04:43,  2.17it/s]\u001b[0m\n",
      "\u001b[34m21%|        | 166/782 [01:17<04:43,  2.17it/s]\u001b[0m\n",
      "\u001b[34m21%|       | 167/782 [01:17<04:42,  2.18it/s]\u001b[0m\n",
      "\u001b[34m21%|       | 168/782 [01:18<04:41,  2.18it/s]\u001b[0m\n",
      "\u001b[34m22%|       | 169/782 [01:18<04:41,  2.18it/s]\u001b[0m\n",
      "\u001b[34m22%|       | 170/782 [01:19<04:41,  2.18it/s]\u001b[0m\n",
      "\u001b[34m22%|       | 171/782 [01:19<04:40,  2.18it/s]\u001b[0m\n",
      "\u001b[34m22%|       | 172/782 [01:20<04:40,  2.17it/s]\u001b[0m\n",
      "\u001b[34m22%|       | 173/782 [01:20<04:39,  2.18it/s]\u001b[0m\n",
      "\u001b[34m22%|       | 174/782 [01:21<04:39,  2.17it/s]\u001b[0m\n",
      "\u001b[34m22%|       | 175/782 [01:21<04:39,  2.17it/s]\u001b[0m\n",
      "\u001b[34m23%|       | 176/782 [01:21<04:38,  2.17it/s]\u001b[0m\n",
      "\u001b[34m23%|       | 177/782 [01:22<04:38,  2.17it/s]\u001b[0m\n",
      "\u001b[34m23%|       | 178/782 [01:22<04:37,  2.18it/s]\u001b[0m\n",
      "\u001b[34m23%|       | 179/782 [01:23<04:37,  2.17it/s]\u001b[0m\n",
      "\u001b[34m23%|       | 180/782 [01:23<04:39,  2.15it/s]\u001b[0m\n",
      "\u001b[34m23%|       | 181/782 [01:24<04:39,  2.15it/s]\u001b[0m\n",
      "\u001b[34m23%|       | 182/782 [01:24<04:38,  2.15it/s]\u001b[0m\n",
      "\u001b[34m23%|       | 183/782 [01:25<04:38,  2.15it/s]\u001b[0m\n",
      "\u001b[34m24%|       | 184/782 [01:25<04:37,  2.15it/s]\u001b[0m\n",
      "\u001b[34m24%|       | 185/782 [01:26<04:37,  2.15it/s]\u001b[0m\n",
      "\u001b[34m24%|       | 186/782 [01:26<04:35,  2.16it/s]\u001b[0m\n",
      "\u001b[34m24%|       | 187/782 [01:27<04:34,  2.16it/s]\u001b[0m\n",
      "\u001b[34m24%|       | 188/782 [01:27<04:34,  2.17it/s]\u001b[0m\n",
      "\u001b[34m24%|       | 189/782 [01:27<04:34,  2.16it/s]\u001b[0m\n",
      "\u001b[34m24%|       | 190/782 [01:28<04:33,  2.16it/s]\u001b[0m\n",
      "\u001b[34m24%|       | 191/782 [01:28<04:33,  2.16it/s]\u001b[0m\n",
      "\u001b[34m25%|       | 192/782 [01:29<04:32,  2.16it/s]\u001b[0m\n",
      "\u001b[34m25%|       | 193/782 [01:29<04:31,  2.17it/s]\u001b[0m\n",
      "\u001b[34m25%|       | 194/782 [01:30<04:31,  2.17it/s]\u001b[0m\n",
      "\u001b[34m25%|       | 195/782 [01:30<04:30,  2.17it/s]\u001b[0m\n",
      "\u001b[34m25%|       | 196/782 [01:31<04:29,  2.17it/s]\u001b[0m\n",
      "\u001b[34m25%|       | 197/782 [01:31<04:28,  2.18it/s]\u001b[0m\n",
      "\u001b[34m25%|       | 198/782 [01:32<04:28,  2.18it/s]\u001b[0m\n",
      "\u001b[34m25%|       | 199/782 [01:32<04:28,  2.17it/s]\u001b[0m\n",
      "\u001b[34m26%|       | 200/782 [01:33<04:28,  2.17it/s]\u001b[0m\n",
      "\u001b[34m26%|       | 201/782 [01:33<04:28,  2.17it/s]\u001b[0m\n",
      "\u001b[34m26%|       | 202/782 [01:33<04:27,  2.17it/s]\u001b[0m\n",
      "\u001b[34m26%|       | 203/782 [01:34<04:27,  2.17it/s]\u001b[0m\n",
      "\u001b[34m26%|       | 204/782 [01:34<04:27,  2.16it/s]\u001b[0m\n",
      "\u001b[34m26%|       | 205/782 [01:35<04:27,  2.16it/s]\u001b[0m\n",
      "\u001b[34m26%|       | 206/782 [01:35<04:26,  2.16it/s]\u001b[0m\n",
      "\u001b[34m26%|       | 207/782 [01:36<04:26,  2.16it/s]\u001b[0m\n",
      "\u001b[34m27%|       | 208/782 [01:36<04:25,  2.16it/s]\u001b[0m\n",
      "\u001b[34m27%|       | 209/782 [01:37<04:25,  2.16it/s]\u001b[0m\n",
      "\u001b[34m27%|       | 210/782 [01:37<04:25,  2.16it/s]\u001b[0m\n",
      "\u001b[34m27%|       | 211/782 [01:38<04:25,  2.15it/s]\u001b[0m\n",
      "\u001b[34m27%|       | 212/782 [01:38<04:24,  2.15it/s]\u001b[0m\n",
      "\u001b[34m27%|       | 213/782 [01:39<04:24,  2.15it/s]\u001b[0m\n",
      "\u001b[34m27%|       | 214/782 [01:39<04:23,  2.15it/s]\u001b[0m\n",
      "\u001b[34m27%|       | 215/782 [01:39<04:22,  2.16it/s]\u001b[0m\n",
      "\u001b[34m28%|       | 216/782 [01:40<04:21,  2.17it/s]\u001b[0m\n",
      "\u001b[34m28%|       | 217/782 [01:40<04:20,  2.17it/s]\u001b[0m\n",
      "\u001b[34m28%|       | 218/782 [01:41<04:19,  2.17it/s]\u001b[0m\n",
      "\u001b[34m28%|       | 219/782 [01:41<04:19,  2.17it/s]\u001b[0m\n",
      "\u001b[34m28%|       | 220/782 [01:42<04:19,  2.16it/s]\u001b[0m\n",
      "\u001b[34m28%|       | 221/782 [01:42<04:18,  2.17it/s]\u001b[0m\n",
      "\u001b[34m28%|       | 222/782 [01:43<04:18,  2.17it/s]\u001b[0m\n",
      "\u001b[34m29%|       | 223/782 [01:43<04:17,  2.17it/s]\u001b[0m\n",
      "\u001b[34m29%|       | 224/782 [01:44<04:17,  2.17it/s]\u001b[0m\n",
      "\u001b[34m29%|       | 225/782 [01:44<04:16,  2.17it/s]\u001b[0m\n",
      "\u001b[34m29%|       | 226/782 [01:45<04:17,  2.16it/s]\u001b[0m\n",
      "\u001b[34m29%|       | 227/782 [01:45<04:16,  2.17it/s]\u001b[0m\n",
      "\u001b[34m29%|       | 228/782 [01:45<04:15,  2.17it/s]\u001b[0m\n",
      "\u001b[34m29%|       | 229/782 [01:46<04:14,  2.17it/s]\u001b[0m\n",
      "\u001b[34m29%|       | 230/782 [01:46<04:15,  2.16it/s]\u001b[0m\n",
      "\u001b[34m30%|       | 231/782 [01:47<04:14,  2.17it/s]\u001b[0m\n",
      "\u001b[34m30%|       | 232/782 [01:47<04:13,  2.17it/s]\u001b[0m\n",
      "\u001b[34m30%|       | 233/782 [01:48<04:13,  2.17it/s]\u001b[0m\n",
      "\u001b[34m30%|       | 234/782 [01:48<04:13,  2.16it/s]\u001b[0m\n",
      "\u001b[34m30%|       | 235/782 [01:49<04:12,  2.16it/s]\u001b[0m\n",
      "\u001b[34m30%|       | 236/782 [01:49<04:12,  2.16it/s]\u001b[0m\n",
      "\u001b[34m30%|       | 237/782 [01:50<04:12,  2.16it/s]\u001b[0m\n",
      "\u001b[34m30%|       | 238/782 [01:50<04:11,  2.16it/s]\u001b[0m\n",
      "\u001b[34m31%|       | 239/782 [01:51<04:11,  2.16it/s]\u001b[0m\n",
      "\u001b[34m31%|       | 240/782 [01:51<04:10,  2.16it/s]\u001b[0m\n",
      "\u001b[34m31%|       | 241/782 [01:51<04:09,  2.17it/s]\u001b[0m\n",
      "\u001b[34m31%|       | 242/782 [01:52<04:08,  2.17it/s]\u001b[0m\n",
      "\u001b[34m31%|       | 243/782 [01:52<04:08,  2.17it/s]\u001b[0m\n",
      "\u001b[34m31%|       | 244/782 [01:53<04:08,  2.17it/s]\u001b[0m\n",
      "\u001b[34m31%|      | 245/782 [01:53<04:07,  2.17it/s]\u001b[0m\n",
      "\u001b[34m31%|      | 246/782 [01:54<04:07,  2.17it/s]\u001b[0m\n",
      "\u001b[34m32%|      | 247/782 [01:54<04:06,  2.17it/s]\u001b[0m\n",
      "\u001b[34m32%|      | 248/782 [01:55<04:05,  2.17it/s]\u001b[0m\n",
      "\u001b[34m32%|      | 249/782 [01:55<04:05,  2.17it/s]\u001b[0m\n",
      "\u001b[34m32%|      | 250/782 [01:56<04:05,  2.17it/s]\u001b[0m\n",
      "\u001b[34m32%|      | 251/782 [01:56<04:04,  2.17it/s]\u001b[0m\n",
      "\u001b[34m32%|      | 252/782 [01:57<04:04,  2.17it/s]\u001b[0m\n",
      "\u001b[34m32%|      | 253/782 [01:57<04:03,  2.17it/s]\u001b[0m\n",
      "\u001b[34m32%|      | 254/782 [01:57<04:03,  2.17it/s]\u001b[0m\n",
      "\u001b[34m33%|      | 255/782 [01:58<04:03,  2.16it/s]\u001b[0m\n",
      "\u001b[34m33%|      | 256/782 [01:58<04:02,  2.17it/s]\u001b[0m\n",
      "\u001b[34m33%|      | 257/782 [01:59<04:02,  2.17it/s]\u001b[0m\n",
      "\u001b[34m33%|      | 258/782 [01:59<04:01,  2.17it/s]\u001b[0m\n",
      "\u001b[34m33%|      | 259/782 [02:00<04:01,  2.17it/s]\u001b[0m\n",
      "\u001b[34m33%|      | 260/782 [02:00<04:00,  2.17it/s]\u001b[0m\n",
      "\u001b[34m33%|      | 261/782 [02:01<04:00,  2.17it/s]\u001b[0m\n",
      "\u001b[34m34%|      | 262/782 [02:01<04:00,  2.17it/s]\u001b[0m\n",
      "\u001b[34m34%|      | 263/782 [02:02<03:59,  2.17it/s]\u001b[0m\n",
      "\u001b[34m34%|      | 264/782 [02:02<03:58,  2.17it/s]\u001b[0m\n",
      "\u001b[34m34%|      | 265/782 [02:03<03:57,  2.17it/s]\u001b[0m\n",
      "\u001b[34m34%|      | 266/782 [02:03<03:56,  2.18it/s]\u001b[0m\n",
      "\u001b[34m34%|      | 267/782 [02:03<03:56,  2.18it/s]\u001b[0m\n",
      "\u001b[34m34%|      | 268/782 [02:04<03:56,  2.18it/s]\u001b[0m\n",
      "\u001b[34m34%|      | 269/782 [02:04<03:55,  2.18it/s]\u001b[0m\n",
      "\u001b[34m35%|      | 270/782 [02:05<03:55,  2.18it/s]\u001b[0m\n",
      "\u001b[34m35%|      | 271/782 [02:05<03:54,  2.18it/s]\u001b[0m\n",
      "\u001b[34m35%|      | 272/782 [02:06<03:54,  2.18it/s]\u001b[0m\n",
      "\u001b[34m35%|      | 273/782 [02:06<03:54,  2.17it/s]\u001b[0m\n",
      "\u001b[34m35%|      | 274/782 [02:07<03:54,  2.17it/s]\u001b[0m\n",
      "\u001b[34m35%|      | 275/782 [02:07<03:54,  2.16it/s]\u001b[0m\n",
      "\u001b[34m35%|      | 276/782 [02:08<03:54,  2.16it/s]\u001b[0m\n",
      "\u001b[34m35%|      | 277/782 [02:08<03:54,  2.16it/s]\u001b[0m\n",
      "\u001b[34m36%|      | 278/782 [02:09<03:53,  2.16it/s]\u001b[0m\n",
      "\u001b[34m36%|      | 279/782 [02:09<03:53,  2.16it/s]\u001b[0m\n",
      "\u001b[34m36%|      | 280/782 [02:09<03:52,  2.16it/s]\u001b[0m\n",
      "\u001b[34m36%|      | 281/782 [02:10<03:52,  2.16it/s]\u001b[0m\n",
      "\u001b[34m36%|      | 282/782 [02:10<03:51,  2.16it/s]\u001b[0m\n",
      "\u001b[34m36%|      | 283/782 [02:11<03:50,  2.17it/s]\u001b[0m\n",
      "\u001b[34m36%|      | 284/782 [02:11<03:50,  2.16it/s]\u001b[0m\n",
      "\u001b[34m36%|      | 285/782 [02:12<03:49,  2.16it/s]\u001b[0m\n",
      "\u001b[34m37%|      | 286/782 [02:12<03:48,  2.17it/s]\u001b[0m\n",
      "\u001b[34m37%|      | 287/782 [02:13<03:47,  2.17it/s]\u001b[0m\n",
      "\u001b[34m37%|      | 288/782 [02:13<03:47,  2.17it/s]\u001b[0m\n",
      "\u001b[34m37%|      | 289/782 [02:14<03:47,  2.17it/s]\u001b[0m\n",
      "\u001b[34m37%|      | 290/782 [02:14<03:46,  2.17it/s]\u001b[0m\n",
      "\u001b[34m37%|      | 291/782 [02:15<03:46,  2.17it/s]\u001b[0m\n",
      "\u001b[34m37%|      | 292/782 [02:15<03:45,  2.17it/s]\u001b[0m\n",
      "\u001b[34m37%|      | 293/782 [02:15<03:45,  2.17it/s]\u001b[0m\n",
      "\u001b[34m38%|      | 294/782 [02:16<03:44,  2.17it/s]\u001b[0m\n",
      "\u001b[34m38%|      | 295/782 [02:16<03:44,  2.17it/s]\u001b[0m\n",
      "\u001b[34m38%|      | 296/782 [02:17<03:44,  2.17it/s]\u001b[0m\n",
      "\u001b[34m38%|      | 297/782 [02:17<03:43,  2.17it/s]\u001b[0m\n",
      "\u001b[34m38%|      | 298/782 [02:18<03:43,  2.17it/s]\u001b[0m\n",
      "\u001b[34m38%|      | 299/782 [02:18<03:42,  2.17it/s]\u001b[0m\n",
      "\u001b[34m38%|      | 300/782 [02:19<03:42,  2.17it/s]\u001b[0m\n",
      "\u001b[34m38%|      | 301/782 [02:19<03:41,  2.17it/s]\u001b[0m\n",
      "\u001b[34m39%|      | 302/782 [02:20<03:41,  2.17it/s]\u001b[0m\n",
      "\u001b[34m39%|      | 303/782 [02:20<03:40,  2.17it/s]\u001b[0m\n",
      "\u001b[34m39%|      | 304/782 [02:21<03:40,  2.17it/s]\u001b[0m\n",
      "\u001b[34m39%|      | 305/782 [02:21<03:39,  2.17it/s]\u001b[0m\n",
      "\u001b[34m39%|      | 306/782 [02:21<03:39,  2.17it/s]\u001b[0m\n",
      "\u001b[34m39%|      | 307/782 [02:22<03:39,  2.17it/s]\u001b[0m\n",
      "\u001b[34m39%|      | 308/782 [02:22<03:38,  2.17it/s]\u001b[0m\n",
      "\u001b[34m40%|      | 309/782 [02:23<03:38,  2.16it/s]\u001b[0m\n",
      "\u001b[34m40%|      | 310/782 [02:23<03:38,  2.16it/s]\u001b[0m\n",
      "\u001b[34m40%|      | 311/782 [02:24<03:38,  2.16it/s]\u001b[0m\n",
      "\u001b[34m40%|      | 312/782 [02:24<03:37,  2.16it/s]\u001b[0m\n",
      "\u001b[34m40%|      | 313/782 [02:25<03:37,  2.16it/s]\u001b[0m\n",
      "\u001b[34m40%|      | 314/782 [02:25<03:37,  2.15it/s]\u001b[0m\n",
      "\u001b[34m40%|      | 315/782 [02:26<03:36,  2.15it/s]\u001b[0m\n",
      "\u001b[34m40%|      | 316/782 [02:26<03:35,  2.16it/s]\u001b[0m\n",
      "\u001b[34m41%|      | 317/782 [02:27<03:34,  2.16it/s]\u001b[0m\n",
      "\u001b[34m41%|      | 318/782 [02:27<03:34,  2.17it/s]\u001b[0m\n",
      "\u001b[34m41%|      | 319/782 [02:27<03:33,  2.17it/s]\u001b[0m\n",
      "\u001b[34m41%|      | 320/782 [02:28<03:32,  2.17it/s]\u001b[0m\n",
      "\u001b[34m41%|      | 321/782 [02:28<03:32,  2.17it/s]\u001b[0m\n",
      "\u001b[34m41%|      | 322/782 [02:29<03:31,  2.17it/s]\u001b[0m\n",
      "\u001b[34m41%|     | 323/782 [02:29<03:31,  2.17it/s]\u001b[0m\n",
      "\u001b[34m41%|     | 324/782 [02:30<03:30,  2.17it/s]\u001b[0m\n",
      "\u001b[34m42%|     | 325/782 [02:30<03:30,  2.17it/s]\u001b[0m\n",
      "\u001b[34m42%|     | 326/782 [02:31<03:29,  2.18it/s]\u001b[0m\n",
      "\u001b[34m42%|     | 327/782 [02:31<03:29,  2.17it/s]\u001b[0m\n",
      "\u001b[34m42%|     | 328/782 [02:32<03:29,  2.17it/s]\u001b[0m\n",
      "\u001b[34m42%|     | 329/782 [02:32<03:28,  2.17it/s]\u001b[0m\n",
      "\u001b[34m42%|     | 330/782 [02:33<03:28,  2.17it/s]\u001b[0m\n",
      "\u001b[34m42%|     | 331/782 [02:33<03:28,  2.17it/s]\u001b[0m\n",
      "\u001b[34m42%|     | 332/782 [02:33<03:27,  2.17it/s]\u001b[0m\n",
      "\u001b[34m43%|     | 333/782 [02:34<03:27,  2.17it/s]\u001b[0m\n",
      "\u001b[34m43%|     | 334/782 [02:34<03:26,  2.17it/s]\u001b[0m\n",
      "\u001b[34m43%|     | 335/782 [02:35<03:26,  2.17it/s]\u001b[0m\n",
      "\u001b[34m43%|     | 336/782 [02:35<03:25,  2.17it/s]\u001b[0m\n",
      "\u001b[34m43%|     | 337/782 [02:36<03:25,  2.17it/s]\u001b[0m\n",
      "\u001b[34m43%|     | 338/782 [02:36<03:25,  2.16it/s]\u001b[0m\n",
      "\u001b[34m43%|     | 339/782 [02:37<03:25,  2.16it/s]\u001b[0m\n",
      "\u001b[34m43%|     | 340/782 [02:37<03:25,  2.15it/s]\u001b[0m\n",
      "\u001b[34m44%|     | 341/782 [02:38<03:25,  2.15it/s]\u001b[0m\n",
      "\u001b[34m44%|     | 342/782 [02:38<03:24,  2.15it/s]\u001b[0m\n",
      "\u001b[34m44%|     | 343/782 [02:39<03:23,  2.16it/s]\u001b[0m\n",
      "\u001b[34m44%|     | 344/782 [02:39<03:22,  2.16it/s]\u001b[0m\n",
      "\u001b[34m44%|     | 345/782 [02:39<03:22,  2.16it/s]\u001b[0m\n",
      "\u001b[34m44%|     | 346/782 [02:40<03:22,  2.16it/s]\u001b[0m\n",
      "\u001b[34m44%|     | 347/782 [02:40<03:21,  2.16it/s]\u001b[0m\n",
      "\u001b[34m45%|     | 348/782 [02:41<03:21,  2.16it/s]\u001b[0m\n",
      "\u001b[34m45%|     | 349/782 [02:41<03:20,  2.16it/s]\u001b[0m\n",
      "\u001b[34m45%|     | 350/782 [02:42<03:19,  2.16it/s]\u001b[0m\n",
      "\u001b[34m45%|     | 351/782 [02:42<03:18,  2.17it/s]\u001b[0m\n",
      "\u001b[34m45%|     | 352/782 [02:43<03:18,  2.16it/s]\u001b[0m\n",
      "\u001b[34m45%|     | 353/782 [02:43<03:19,  2.16it/s]\u001b[0m\n",
      "\u001b[34m45%|     | 354/782 [02:44<03:18,  2.16it/s]\u001b[0m\n",
      "\u001b[34m45%|     | 355/782 [02:44<03:17,  2.16it/s]\u001b[0m\n",
      "\u001b[34m46%|     | 356/782 [02:45<03:17,  2.16it/s]\u001b[0m\n",
      "\u001b[34m46%|     | 357/782 [02:45<03:17,  2.15it/s]\u001b[0m\n",
      "\u001b[34m46%|     | 358/782 [02:45<03:16,  2.16it/s]\u001b[0m\n",
      "\u001b[34m46%|     | 359/782 [02:46<03:16,  2.16it/s]\u001b[0m\n",
      "\u001b[34m46%|     | 360/782 [02:46<03:15,  2.16it/s]\u001b[0m\n",
      "\u001b[34m46%|     | 361/782 [02:47<03:15,  2.16it/s]\u001b[0m\n",
      "\u001b[34m46%|     | 362/782 [02:47<03:14,  2.16it/s]\u001b[0m\n",
      "\u001b[34m46%|     | 363/782 [02:48<03:14,  2.15it/s]\u001b[0m\n",
      "\u001b[34m47%|     | 364/782 [02:48<03:14,  2.15it/s]\u001b[0m\n",
      "\u001b[34m47%|     | 365/782 [02:49<03:13,  2.15it/s]\u001b[0m\n",
      "\u001b[34m47%|     | 366/782 [02:49<03:12,  2.16it/s]\u001b[0m\n",
      "\u001b[34m47%|     | 367/782 [02:50<03:12,  2.16it/s]\u001b[0m\n",
      "\u001b[34m47%|     | 368/782 [02:50<03:11,  2.16it/s]\u001b[0m\n",
      "\u001b[34m47%|     | 369/782 [02:51<03:11,  2.16it/s]\u001b[0m\n",
      "\u001b[34m47%|     | 370/782 [02:51<03:10,  2.16it/s]\u001b[0m\n",
      "\u001b[34m47%|     | 371/782 [02:52<03:10,  2.16it/s]\u001b[0m\n",
      "\u001b[34m48%|     | 372/782 [02:52<03:14,  2.10it/s]\u001b[0m\n",
      "\u001b[34m48%|     | 373/782 [02:53<03:21,  2.03it/s]\u001b[0m\n",
      "\u001b[34m48%|     | 374/782 [02:53<03:23,  2.01it/s]\u001b[0m\n",
      "\u001b[34m48%|     | 375/782 [02:54<03:27,  1.96it/s]\u001b[0m\n",
      "\u001b[34m48%|     | 376/782 [02:54<03:25,  1.97it/s]\u001b[0m\n",
      "\u001b[34m48%|     | 377/782 [02:55<03:19,  2.03it/s]\u001b[0m\n",
      "\u001b[34m48%|     | 378/782 [02:55<03:14,  2.07it/s]\u001b[0m\n",
      "\u001b[34m48%|     | 379/782 [02:55<03:11,  2.10it/s]\u001b[0m\n",
      "\u001b[34m49%|     | 380/782 [02:56<03:09,  2.12it/s]\u001b[0m\n",
      "\u001b[34m49%|     | 381/782 [02:56<03:08,  2.13it/s]\u001b[0m\n",
      "\u001b[34m49%|     | 382/782 [02:57<03:07,  2.14it/s]\u001b[0m\n",
      "\u001b[34m49%|     | 383/782 [02:57<03:05,  2.15it/s]\u001b[0m\n",
      "\u001b[34m49%|     | 384/782 [02:58<03:05,  2.15it/s]\u001b[0m\n",
      "\u001b[34m49%|     | 385/782 [02:58<03:04,  2.15it/s]\u001b[0m\n",
      "\u001b[34m49%|     | 386/782 [02:59<03:04,  2.15it/s]\u001b[0m\n",
      "\u001b[34m49%|     | 387/782 [02:59<03:03,  2.15it/s]\u001b[0m\n",
      "\u001b[34m50%|     | 388/782 [03:00<03:02,  2.15it/s]\u001b[0m\n",
      "\u001b[34m50%|     | 389/782 [03:00<03:02,  2.15it/s]\u001b[0m\n",
      "\u001b[34m50%|     | 390/782 [03:01<03:02,  2.15it/s]\u001b[0m\n",
      "\u001b[34m50%|     | 391/782 [03:01<03:01,  2.15it/s]\u001b[0m\n",
      "\u001b[34m50%|     | 392/782 [03:02<03:01,  2.15it/s]\u001b[0m\n",
      "\u001b[34m50%|     | 393/782 [03:02<03:00,  2.16it/s]\u001b[0m\n",
      "\u001b[34m50%|     | 394/782 [03:02<02:59,  2.16it/s]\u001b[0m\n",
      "\u001b[34m51%|     | 395/782 [03:03<02:59,  2.15it/s]\u001b[0m\n",
      "\u001b[34m51%|     | 396/782 [03:03<02:59,  2.15it/s]\u001b[0m\n",
      "\u001b[34m51%|     | 397/782 [03:04<02:58,  2.15it/s]\u001b[0m\n",
      "\u001b[34m51%|     | 398/782 [03:04<02:58,  2.16it/s]\u001b[0m\n",
      "\u001b[34m51%|     | 399/782 [03:05<02:57,  2.16it/s]\u001b[0m\n",
      "\u001b[34m51%|     | 400/782 [03:05<02:56,  2.16it/s]\u001b[0m\n",
      "\u001b[34m51%|    | 401/782 [03:06<02:56,  2.16it/s]\u001b[0m\n",
      "\u001b[34m51%|    | 402/782 [03:06<02:55,  2.17it/s]\u001b[0m\n",
      "\u001b[34m52%|    | 403/782 [03:07<02:54,  2.17it/s]\u001b[0m\n",
      "\u001b[34m52%|    | 404/782 [03:07<02:54,  2.17it/s]\u001b[0m\n",
      "\u001b[34m52%|    | 405/782 [03:08<02:54,  2.16it/s]\u001b[0m\n",
      "\u001b[34m52%|    | 406/782 [03:08<02:53,  2.17it/s]\u001b[0m\n",
      "\u001b[34m52%|    | 407/782 [03:08<02:53,  2.17it/s]\u001b[0m\n",
      "\u001b[34m52%|    | 408/782 [03:09<02:52,  2.17it/s]\u001b[0m\n",
      "\u001b[34m52%|    | 409/782 [03:09<02:51,  2.17it/s]\u001b[0m\n",
      "\u001b[34m52%|    | 410/782 [03:10<02:51,  2.17it/s]\u001b[0m\n",
      "\u001b[34m53%|    | 411/782 [03:10<02:51,  2.17it/s]\u001b[0m\n",
      "\u001b[34m53%|    | 412/782 [03:11<02:50,  2.17it/s]\u001b[0m\n",
      "\u001b[34m53%|    | 413/782 [03:11<02:50,  2.17it/s]\u001b[0m\n",
      "\u001b[34m53%|    | 414/782 [03:12<02:50,  2.16it/s]\u001b[0m\n",
      "\u001b[34m53%|    | 415/782 [03:12<02:49,  2.16it/s]\u001b[0m\n",
      "\u001b[34m53%|    | 416/782 [03:13<02:49,  2.16it/s]\u001b[0m\n",
      "\u001b[34m53%|    | 417/782 [03:13<02:49,  2.16it/s]\u001b[0m\n",
      "\u001b[34m53%|    | 418/782 [03:14<02:48,  2.16it/s]\u001b[0m\n",
      "\u001b[34m54%|    | 419/782 [03:14<02:47,  2.16it/s]\u001b[0m\n",
      "\u001b[34m54%|    | 420/782 [03:14<02:47,  2.16it/s]\u001b[0m\n",
      "\u001b[34m54%|    | 421/782 [03:15<02:47,  2.16it/s]\u001b[0m\n",
      "\u001b[34m54%|    | 422/782 [03:15<02:46,  2.16it/s]\u001b[0m\n",
      "\u001b[34m54%|    | 423/782 [03:16<02:46,  2.15it/s]\u001b[0m\n",
      "\u001b[34m54%|    | 424/782 [03:16<02:46,  2.15it/s]\u001b[0m\n",
      "\u001b[34m54%|    | 425/782 [03:17<02:45,  2.15it/s]\u001b[0m\n",
      "\u001b[34m54%|    | 426/782 [03:17<02:45,  2.15it/s]\u001b[0m\n",
      "\u001b[34m55%|    | 427/782 [03:18<02:44,  2.15it/s]\u001b[0m\n",
      "\u001b[34m55%|    | 428/782 [03:18<02:44,  2.16it/s]\u001b[0m\n",
      "\u001b[34m55%|    | 429/782 [03:19<02:43,  2.16it/s]\u001b[0m\n",
      "\u001b[34m55%|    | 430/782 [03:19<02:42,  2.16it/s]\u001b[0m\n",
      "\u001b[34m55%|    | 431/782 [03:20<02:42,  2.16it/s]\u001b[0m\n",
      "\u001b[34m55%|    | 432/782 [03:20<02:42,  2.16it/s]\u001b[0m\n",
      "\u001b[34m55%|    | 433/782 [03:20<02:41,  2.16it/s]\u001b[0m\n",
      "\u001b[34m55%|    | 434/782 [03:21<02:41,  2.16it/s]\u001b[0m\n",
      "\u001b[34m56%|    | 435/782 [03:21<02:41,  2.15it/s]\u001b[0m\n",
      "\u001b[34m56%|    | 436/782 [03:22<02:41,  2.14it/s]\u001b[0m\n",
      "\u001b[34m56%|    | 437/782 [03:22<02:40,  2.15it/s]\u001b[0m\n",
      "\u001b[34m56%|    | 438/782 [03:23<02:39,  2.15it/s]\u001b[0m\n",
      "\u001b[34m56%|    | 439/782 [03:23<02:39,  2.15it/s]\u001b[0m\n",
      "\u001b[34m56%|    | 440/782 [03:24<02:39,  2.15it/s]\u001b[0m\n",
      "\u001b[34m56%|    | 441/782 [03:24<02:38,  2.16it/s]\u001b[0m\n",
      "\u001b[34m57%|    | 442/782 [03:25<02:37,  2.16it/s]\u001b[0m\n",
      "\u001b[34m57%|    | 443/782 [03:25<02:37,  2.16it/s]\u001b[0m\n",
      "\u001b[34m57%|    | 444/782 [03:26<02:36,  2.16it/s]\u001b[0m\n",
      "\u001b[34m57%|    | 445/782 [03:26<02:35,  2.17it/s]\u001b[0m\n",
      "\u001b[34m57%|    | 446/782 [03:27<02:34,  2.17it/s]\u001b[0m\n",
      "\u001b[34m57%|    | 447/782 [03:27<02:34,  2.17it/s]\u001b[0m\n",
      "\u001b[34m57%|    | 448/782 [03:27<02:34,  2.17it/s]\u001b[0m\n",
      "\u001b[34m57%|    | 449/782 [03:28<02:33,  2.16it/s]\u001b[0m\n",
      "\u001b[34m58%|    | 450/782 [03:28<02:33,  2.16it/s]\u001b[0m\n",
      "\u001b[34m58%|    | 451/782 [03:29<02:33,  2.16it/s]\u001b[0m\n",
      "\u001b[34m58%|    | 452/782 [03:29<02:33,  2.15it/s]\u001b[0m\n",
      "\u001b[34m58%|    | 453/782 [03:30<02:32,  2.16it/s]\u001b[0m\n",
      "\u001b[34m58%|    | 454/782 [03:30<02:31,  2.16it/s]\u001b[0m\n",
      "\u001b[34m58%|    | 455/782 [03:31<02:31,  2.16it/s]\u001b[0m\n",
      "\u001b[34m58%|    | 456/782 [03:31<02:31,  2.16it/s]\u001b[0m\n",
      "\u001b[34m58%|    | 457/782 [03:32<02:30,  2.16it/s]\u001b[0m\n",
      "\u001b[34m59%|    | 458/782 [03:32<02:29,  2.16it/s]\u001b[0m\n",
      "\u001b[34m59%|    | 459/782 [03:33<02:29,  2.16it/s]\u001b[0m\n",
      "\u001b[34m59%|    | 460/782 [03:33<02:28,  2.16it/s]\u001b[0m\n",
      "\u001b[34m59%|    | 461/782 [03:33<02:28,  2.16it/s]\u001b[0m\n",
      "\u001b[34m59%|    | 462/782 [03:34<02:28,  2.16it/s]\u001b[0m\n",
      "\u001b[34m59%|    | 463/782 [03:34<02:27,  2.16it/s]\u001b[0m\n",
      "\u001b[34m59%|    | 464/782 [03:35<02:27,  2.16it/s]\u001b[0m\n",
      "\u001b[34m59%|    | 465/782 [03:35<02:26,  2.16it/s]\u001b[0m\n",
      "\u001b[34m60%|    | 466/782 [03:36<02:25,  2.17it/s]\u001b[0m\n",
      "\u001b[34m60%|    | 467/782 [03:36<02:25,  2.17it/s]\u001b[0m\n",
      "\u001b[34m60%|    | 468/782 [03:37<02:24,  2.17it/s]\u001b[0m\n",
      "\u001b[34m60%|    | 469/782 [03:37<02:24,  2.17it/s]\u001b[0m\n",
      "\u001b[34m60%|    | 470/782 [03:38<02:23,  2.17it/s]\u001b[0m\n",
      "\u001b[34m60%|    | 471/782 [03:38<02:23,  2.17it/s]\u001b[0m\n",
      "\u001b[34m60%|    | 472/782 [03:39<02:23,  2.17it/s]\u001b[0m\n",
      "\u001b[34m60%|    | 473/782 [03:39<02:22,  2.16it/s]\u001b[0m\n",
      "\u001b[34m61%|    | 474/782 [03:39<02:22,  2.16it/s]\u001b[0m\n",
      "\u001b[34m61%|    | 475/782 [03:40<02:21,  2.16it/s]\u001b[0m\n",
      "\u001b[34m61%|    | 476/782 [03:40<02:21,  2.16it/s]\u001b[0m\n",
      "\u001b[34m61%|    | 477/782 [03:41<02:20,  2.17it/s]\u001b[0m\n",
      "\u001b[34m61%|    | 478/782 [03:41<02:20,  2.16it/s]\u001b[0m\n",
      "\u001b[34m61%|   | 479/782 [03:42<02:20,  2.16it/s]\u001b[0m\n",
      "\u001b[34m61%|   | 480/782 [03:42<02:19,  2.16it/s]\u001b[0m\n",
      "\u001b[34m62%|   | 481/782 [03:43<02:19,  2.16it/s]\u001b[0m\n",
      "\u001b[34m62%|   | 482/782 [03:43<02:19,  2.16it/s]\u001b[0m\n",
      "\u001b[34m62%|   | 483/782 [03:44<02:18,  2.16it/s]\u001b[0m\n",
      "\u001b[34m62%|   | 484/782 [03:44<02:18,  2.16it/s]\u001b[0m\n",
      "\u001b[34m62%|   | 485/782 [03:45<02:17,  2.16it/s]\u001b[0m\n",
      "\u001b[34m62%|   | 486/782 [03:45<02:17,  2.16it/s]\u001b[0m\n",
      "\u001b[34m62%|   | 487/782 [03:45<02:16,  2.16it/s]\u001b[0m\n",
      "\u001b[34m62%|   | 488/782 [03:46<02:16,  2.16it/s]\u001b[0m\n",
      "\u001b[34m63%|   | 489/782 [03:46<02:15,  2.16it/s]\u001b[0m\n",
      "\u001b[34m63%|   | 490/782 [03:47<02:14,  2.16it/s]\u001b[0m\n",
      "\u001b[34m63%|   | 491/782 [03:47<02:14,  2.16it/s]\u001b[0m\n",
      "\u001b[34m63%|   | 492/782 [03:48<02:14,  2.16it/s]\u001b[0m\n",
      "\u001b[34m63%|   | 493/782 [03:48<02:13,  2.16it/s]\u001b[0m\n",
      "\u001b[34m63%|   | 494/782 [03:49<02:13,  2.16it/s]\u001b[0m\n",
      "\u001b[34m63%|   | 495/782 [03:49<02:13,  2.16it/s]\u001b[0m\n",
      "\u001b[34m63%|   | 496/782 [03:50<02:12,  2.16it/s]\u001b[0m\n",
      "\u001b[34m64%|   | 497/782 [03:50<02:11,  2.16it/s]\u001b[0m\n",
      "\u001b[34m64%|   | 498/782 [03:51<02:11,  2.17it/s]\u001b[0m\n",
      "\u001b[34m64%|   | 499/782 [03:51<02:10,  2.17it/s]\u001b[0m\n",
      "\u001b[34m64%|   | 500/782 [03:51<02:10,  2.16it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 0.3476, 'learning_rate': 5e-05, 'epoch': 0.64}\u001b[0m\n",
      "\u001b[34m64%|   | 500/782 [03:51<02:10,  2.16it/s]\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model/checkpoint-500\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model/checkpoint-500\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-500/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-500/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-500/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-500/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/checkpoint-500/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/checkpoint-500/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/checkpoint-500/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/checkpoint-500/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m64%|   | 501/782 [03:53<03:57,  1.18it/s]\u001b[0m\n",
      "\u001b[34m64%|   | 502/782 [03:54<03:24,  1.37it/s]\u001b[0m\n",
      "\u001b[34m64%|   | 503/782 [03:54<03:01,  1.54it/s]\u001b[0m\n",
      "\u001b[34m64%|   | 504/782 [03:55<02:45,  1.68it/s]\u001b[0m\n",
      "\u001b[34m65%|   | 505/782 [03:55<02:33,  1.80it/s]\u001b[0m\n",
      "\u001b[34m65%|   | 506/782 [03:56<02:25,  1.90it/s]\u001b[0m\n",
      "\u001b[34m65%|   | 507/782 [03:56<02:19,  1.97it/s]\u001b[0m\n",
      "\u001b[34m65%|   | 508/782 [03:56<02:15,  2.02it/s]\u001b[0m\n",
      "\u001b[34m65%|   | 509/782 [03:57<02:12,  2.06it/s]\u001b[0m\n",
      "\u001b[34m65%|   | 510/782 [03:57<02:10,  2.08it/s]\u001b[0m\n",
      "\u001b[34m65%|   | 511/782 [03:58<02:08,  2.11it/s]\u001b[0m\n",
      "\u001b[34m65%|   | 512/782 [03:58<02:07,  2.12it/s]\u001b[0m\n",
      "\u001b[34m66%|   | 513/782 [03:59<02:06,  2.13it/s]\u001b[0m\n",
      "\u001b[34m66%|   | 514/782 [03:59<02:05,  2.14it/s]\u001b[0m\n",
      "\u001b[34m66%|   | 515/782 [04:00<02:04,  2.15it/s]\u001b[0m\n",
      "\u001b[34m66%|   | 516/782 [04:00<02:03,  2.15it/s]\u001b[0m\n",
      "\u001b[34m66%|   | 517/782 [04:01<02:03,  2.15it/s]\u001b[0m\n",
      "\u001b[34m66%|   | 518/782 [04:01<02:02,  2.16it/s]\u001b[0m\n",
      "\u001b[34m66%|   | 519/782 [04:02<02:01,  2.16it/s]\u001b[0m\n",
      "\u001b[34m66%|   | 520/782 [04:02<02:01,  2.16it/s]\u001b[0m\n",
      "\u001b[34m67%|   | 521/782 [04:02<02:00,  2.16it/s]\u001b[0m\n",
      "\u001b[34m67%|   | 522/782 [04:03<02:00,  2.16it/s]\u001b[0m\n",
      "\u001b[34m67%|   | 523/782 [04:03<01:59,  2.16it/s]\u001b[0m\n",
      "\u001b[34m67%|   | 524/782 [04:04<01:59,  2.16it/s]\u001b[0m\n",
      "\u001b[34m67%|   | 525/782 [04:04<01:58,  2.16it/s]\u001b[0m\n",
      "\u001b[34m67%|   | 526/782 [04:05<01:58,  2.17it/s]\u001b[0m\n",
      "\u001b[34m67%|   | 527/782 [04:05<01:57,  2.17it/s]\u001b[0m\n",
      "\u001b[34m68%|   | 528/782 [04:06<01:57,  2.17it/s]\u001b[0m\n",
      "\u001b[34m68%|   | 529/782 [04:06<01:56,  2.17it/s]\u001b[0m\n",
      "\u001b[34m68%|   | 530/782 [04:07<01:56,  2.17it/s]\u001b[0m\n",
      "\u001b[34m68%|   | 531/782 [04:07<01:55,  2.17it/s]\u001b[0m\n",
      "\u001b[34m68%|   | 532/782 [04:08<01:55,  2.17it/s]\u001b[0m\n",
      "\u001b[34m68%|   | 533/782 [04:08<01:54,  2.17it/s]\u001b[0m\n",
      "\u001b[34m68%|   | 534/782 [04:08<01:54,  2.17it/s]\u001b[0m\n",
      "\u001b[34m68%|   | 535/782 [04:09<01:53,  2.17it/s]\u001b[0m\n",
      "\u001b[34m69%|   | 536/782 [04:09<01:53,  2.17it/s]\u001b[0m\n",
      "\u001b[34m69%|   | 537/782 [04:10<01:53,  2.17it/s]\u001b[0m\n",
      "\u001b[34m69%|   | 538/782 [04:10<01:52,  2.17it/s]\u001b[0m\n",
      "\u001b[34m69%|   | 539/782 [04:11<01:52,  2.16it/s]\u001b[0m\n",
      "\u001b[34m69%|   | 540/782 [04:11<01:51,  2.17it/s]\u001b[0m\n",
      "\u001b[34m69%|   | 541/782 [04:12<01:51,  2.16it/s]\u001b[0m\n",
      "\u001b[34m69%|   | 542/782 [04:12<01:50,  2.17it/s]\u001b[0m\n",
      "\u001b[34m69%|   | 543/782 [04:13<01:50,  2.17it/s]\u001b[0m\n",
      "\u001b[34m70%|   | 544/782 [04:13<01:49,  2.17it/s]\u001b[0m\n",
      "\u001b[34m70%|   | 545/782 [04:14<01:49,  2.17it/s]\u001b[0m\n",
      "\u001b[34m70%|   | 546/782 [04:14<01:48,  2.17it/s]\u001b[0m\n",
      "\u001b[34m70%|   | 547/782 [04:14<01:48,  2.17it/s]\u001b[0m\n",
      "\u001b[34m70%|   | 548/782 [04:15<01:48,  2.16it/s]\u001b[0m\n",
      "\u001b[34m70%|   | 549/782 [04:15<01:47,  2.17it/s]\u001b[0m\n",
      "\u001b[34m70%|   | 550/782 [04:16<01:46,  2.17it/s]\u001b[0m\n",
      "\u001b[34m70%|   | 551/782 [04:16<01:46,  2.17it/s]\u001b[0m\n",
      "\u001b[34m71%|   | 552/782 [04:17<01:46,  2.17it/s]\u001b[0m\n",
      "\u001b[34m71%|   | 553/782 [04:17<01:45,  2.16it/s]\u001b[0m\n",
      "\u001b[34m71%|   | 554/782 [04:18<01:45,  2.16it/s]\u001b[0m\n",
      "\u001b[34m71%|   | 555/782 [04:18<01:44,  2.17it/s]\u001b[0m\n",
      "\u001b[34m71%|   | 556/782 [04:19<01:44,  2.17it/s]\u001b[0m\n",
      "\u001b[34m71%|   | 557/782 [04:19<01:43,  2.17it/s]\u001b[0m\n",
      "\u001b[34m71%|  | 558/782 [04:20<01:43,  2.17it/s]\u001b[0m\n",
      "\u001b[34m71%|  | 559/782 [04:20<01:43,  2.16it/s]\u001b[0m\n",
      "\u001b[34m72%|  | 560/782 [04:20<01:42,  2.16it/s]\u001b[0m\n",
      "\u001b[34m72%|  | 561/782 [04:21<01:42,  2.17it/s]\u001b[0m\n",
      "\u001b[34m72%|  | 562/782 [04:21<01:41,  2.17it/s]\u001b[0m\n",
      "\u001b[34m72%|  | 563/782 [04:22<01:40,  2.17it/s]\u001b[0m\n",
      "\u001b[34m72%|  | 564/782 [04:22<01:40,  2.17it/s]\u001b[0m\n",
      "\u001b[34m72%|  | 565/782 [04:23<01:39,  2.17it/s]\u001b[0m\n",
      "\u001b[34m72%|  | 566/782 [04:23<01:39,  2.17it/s]\u001b[0m\n",
      "\u001b[34m73%|  | 567/782 [04:24<01:39,  2.16it/s]\u001b[0m\n",
      "\u001b[34m73%|  | 568/782 [04:24<01:39,  2.16it/s]\u001b[0m\n",
      "\u001b[34m73%|  | 569/782 [04:25<01:38,  2.16it/s]\u001b[0m\n",
      "\u001b[34m73%|  | 570/782 [04:25<01:38,  2.16it/s]\u001b[0m\n",
      "\u001b[34m73%|  | 571/782 [04:26<01:37,  2.16it/s]\u001b[0m\n",
      "\u001b[34m73%|  | 572/782 [04:26<01:37,  2.16it/s]\u001b[0m\n",
      "\u001b[34m73%|  | 573/782 [04:27<01:36,  2.16it/s]\u001b[0m\n",
      "\u001b[34m73%|  | 574/782 [04:27<01:36,  2.16it/s]\u001b[0m\n",
      "\u001b[34m74%|  | 575/782 [04:27<01:35,  2.16it/s]\u001b[0m\n",
      "\u001b[34m74%|  | 576/782 [04:28<01:35,  2.16it/s]\u001b[0m\n",
      "\u001b[34m74%|  | 577/782 [04:28<01:34,  2.17it/s]\u001b[0m\n",
      "\u001b[34m74%|  | 578/782 [04:29<01:34,  2.17it/s]\u001b[0m\n",
      "\u001b[34m74%|  | 579/782 [04:29<01:33,  2.17it/s]\u001b[0m\n",
      "\u001b[34m74%|  | 580/782 [04:30<01:33,  2.17it/s]\u001b[0m\n",
      "\u001b[34m74%|  | 581/782 [04:30<01:32,  2.17it/s]\u001b[0m\n",
      "\u001b[34m74%|  | 582/782 [04:31<01:32,  2.17it/s]\u001b[0m\n",
      "\u001b[34m75%|  | 583/782 [04:31<01:31,  2.17it/s]\u001b[0m\n",
      "\u001b[34m75%|  | 584/782 [04:32<01:31,  2.17it/s]\u001b[0m\n",
      "\u001b[34m75%|  | 585/782 [04:32<01:30,  2.17it/s]\u001b[0m\n",
      "\u001b[34m75%|  | 586/782 [04:33<01:30,  2.16it/s]\u001b[0m\n",
      "\u001b[34m75%|  | 587/782 [04:33<01:30,  2.16it/s]\u001b[0m\n",
      "\u001b[34m75%|  | 588/782 [04:33<01:29,  2.16it/s]\u001b[0m\n",
      "\u001b[34m75%|  | 589/782 [04:34<01:29,  2.16it/s]\u001b[0m\n",
      "\u001b[34m75%|  | 590/782 [04:34<01:28,  2.16it/s]\u001b[0m\n",
      "\u001b[34m76%|  | 591/782 [04:35<01:28,  2.17it/s]\u001b[0m\n",
      "\u001b[34m76%|  | 592/782 [04:35<01:27,  2.17it/s]\u001b[0m\n",
      "\u001b[34m76%|  | 593/782 [04:36<01:27,  2.17it/s]\u001b[0m\n",
      "\u001b[34m76%|  | 594/782 [04:36<01:26,  2.16it/s]\u001b[0m\n",
      "\u001b[34m76%|  | 595/782 [04:37<01:26,  2.16it/s]\u001b[0m\n",
      "\u001b[34m76%|  | 596/782 [04:37<01:26,  2.16it/s]\u001b[0m\n",
      "\u001b[34m76%|  | 597/782 [04:38<01:25,  2.16it/s]\u001b[0m\n",
      "\u001b[34m76%|  | 598/782 [04:38<01:25,  2.16it/s]\u001b[0m\n",
      "\u001b[34m77%|  | 599/782 [04:39<01:24,  2.17it/s]\u001b[0m\n",
      "\u001b[34m77%|  | 600/782 [04:39<01:23,  2.17it/s]\u001b[0m\n",
      "\u001b[34m77%|  | 601/782 [04:39<01:23,  2.17it/s]\u001b[0m\n",
      "\u001b[34m77%|  | 602/782 [04:40<01:22,  2.17it/s]\u001b[0m\n",
      "\u001b[34m77%|  | 603/782 [04:40<01:22,  2.17it/s]\u001b[0m\n",
      "\u001b[34m77%|  | 604/782 [04:41<01:21,  2.17it/s]\u001b[0m\n",
      "\u001b[34m77%|  | 605/782 [04:41<01:21,  2.17it/s]\u001b[0m\n",
      "\u001b[34m77%|  | 606/782 [04:42<01:21,  2.17it/s]\u001b[0m\n",
      "\u001b[34m78%|  | 607/782 [04:42<01:20,  2.17it/s]\u001b[0m\n",
      "\u001b[34m78%|  | 608/782 [04:43<01:20,  2.17it/s]\u001b[0m\n",
      "\u001b[34m78%|  | 609/782 [04:43<01:19,  2.17it/s]\u001b[0m\n",
      "\u001b[34m78%|  | 610/782 [04:44<01:19,  2.17it/s]\u001b[0m\n",
      "\u001b[34m78%|  | 611/782 [04:44<01:18,  2.17it/s]\u001b[0m\n",
      "\u001b[34m78%|  | 612/782 [04:44<01:18,  2.17it/s]\u001b[0m\n",
      "\u001b[34m78%|  | 613/782 [04:45<01:17,  2.17it/s]\u001b[0m\n",
      "\u001b[34m79%|  | 614/782 [04:45<01:17,  2.17it/s]\u001b[0m\n",
      "\u001b[34m79%|  | 615/782 [04:46<01:17,  2.17it/s]\u001b[0m\n",
      "\u001b[34m79%|  | 616/782 [04:46<01:16,  2.17it/s]\u001b[0m\n",
      "\u001b[34m79%|  | 617/782 [04:47<01:15,  2.17it/s]\u001b[0m\n",
      "\u001b[34m79%|  | 618/782 [04:47<01:15,  2.17it/s]\u001b[0m\n",
      "\u001b[34m79%|  | 619/782 [04:48<01:15,  2.17it/s]\u001b[0m\n",
      "\u001b[34m79%|  | 620/782 [04:48<01:14,  2.17it/s]\u001b[0m\n",
      "\u001b[34m79%|  | 621/782 [04:49<01:14,  2.16it/s]\u001b[0m\n",
      "\u001b[34m80%|  | 622/782 [04:49<01:14,  2.16it/s]\u001b[0m\n",
      "\u001b[34m80%|  | 623/782 [04:50<01:13,  2.17it/s]\u001b[0m\n",
      "\u001b[34m80%|  | 624/782 [04:50<01:12,  2.17it/s]\u001b[0m\n",
      "\u001b[34m80%|  | 625/782 [04:50<01:12,  2.17it/s]\u001b[0m\n",
      "\u001b[34m80%|  | 626/782 [04:51<01:12,  2.16it/s]\u001b[0m\n",
      "\u001b[34m80%|  | 627/782 [04:51<01:11,  2.16it/s]\u001b[0m\n",
      "\u001b[34m80%|  | 628/782 [04:52<01:11,  2.16it/s]\u001b[0m\n",
      "\u001b[34m80%|  | 629/782 [04:52<01:11,  2.15it/s]\u001b[0m\n",
      "\u001b[34m81%|  | 630/782 [04:53<01:10,  2.15it/s]\u001b[0m\n",
      "\u001b[34m81%|  | 631/782 [04:53<01:10,  2.15it/s]\u001b[0m\n",
      "\u001b[34m81%|  | 632/782 [04:54<01:09,  2.15it/s]\u001b[0m\n",
      "\u001b[34m81%|  | 633/782 [04:54<01:09,  2.15it/s]\u001b[0m\n",
      "\u001b[34m81%|  | 634/782 [04:55<01:08,  2.15it/s]\u001b[0m\n",
      "\u001b[34m81%|  | 635/782 [04:55<01:08,  2.15it/s]\u001b[0m\n",
      "\u001b[34m81%| | 636/782 [04:56<01:07,  2.16it/s]\u001b[0m\n",
      "\u001b[34m81%| | 637/782 [04:56<01:07,  2.16it/s]\u001b[0m\n",
      "\u001b[34m82%| | 638/782 [04:57<01:06,  2.16it/s]\u001b[0m\n",
      "\u001b[34m82%| | 639/782 [04:57<01:06,  2.16it/s]\u001b[0m\n",
      "\u001b[34m82%| | 640/782 [04:57<01:06,  2.15it/s]\u001b[0m\n",
      "\u001b[34m82%| | 641/782 [04:58<01:05,  2.15it/s]\u001b[0m\n",
      "\u001b[34m82%| | 642/782 [04:58<01:04,  2.16it/s]\u001b[0m\n",
      "\u001b[34m82%| | 643/782 [04:59<01:04,  2.16it/s]\u001b[0m\n",
      "\u001b[34m82%| | 644/782 [04:59<01:03,  2.16it/s]\u001b[0m\n",
      "\u001b[34m82%| | 645/782 [05:00<01:03,  2.16it/s]\u001b[0m\n",
      "\u001b[34m83%| | 646/782 [05:00<01:02,  2.17it/s]\u001b[0m\n",
      "\u001b[34m83%| | 647/782 [05:01<01:02,  2.17it/s]\u001b[0m\n",
      "\u001b[34m83%| | 648/782 [05:01<01:01,  2.17it/s]\u001b[0m\n",
      "\u001b[34m83%| | 649/782 [05:02<01:01,  2.17it/s]\u001b[0m\n",
      "\u001b[34m83%| | 650/782 [05:02<01:00,  2.17it/s]\u001b[0m\n",
      "\u001b[34m83%| | 651/782 [05:03<01:00,  2.17it/s]\u001b[0m\n",
      "\u001b[34m83%| | 652/782 [05:03<00:59,  2.17it/s]\u001b[0m\n",
      "\u001b[34m84%| | 653/782 [05:03<00:59,  2.17it/s]\u001b[0m\n",
      "\u001b[34m84%| | 654/782 [05:04<00:58,  2.17it/s]\u001b[0m\n",
      "\u001b[34m84%| | 655/782 [05:04<00:58,  2.17it/s]\u001b[0m\n",
      "\u001b[34m84%| | 656/782 [05:05<00:58,  2.17it/s]\u001b[0m\n",
      "\u001b[34m84%| | 657/782 [05:05<00:57,  2.17it/s]\u001b[0m\n",
      "\u001b[34m84%| | 658/782 [05:06<00:57,  2.16it/s]\u001b[0m\n",
      "\u001b[34m84%| | 659/782 [05:06<00:56,  2.16it/s]\u001b[0m\n",
      "\u001b[34m84%| | 660/782 [05:07<00:56,  2.16it/s]\u001b[0m\n",
      "\u001b[34m85%| | 661/782 [05:07<00:55,  2.17it/s]\u001b[0m\n",
      "\u001b[34m85%| | 662/782 [05:08<00:55,  2.17it/s]\u001b[0m\n",
      "\u001b[34m85%| | 663/782 [05:08<00:54,  2.17it/s]\u001b[0m\n",
      "\u001b[34m85%| | 664/782 [05:09<00:54,  2.17it/s]\u001b[0m\n",
      "\u001b[34m85%| | 665/782 [05:09<00:53,  2.17it/s]\u001b[0m\n",
      "\u001b[34m85%| | 666/782 [05:09<00:53,  2.17it/s]\u001b[0m\n",
      "\u001b[34m85%| | 667/782 [05:10<00:53,  2.17it/s]\u001b[0m\n",
      "\u001b[34m85%| | 668/782 [05:10<00:52,  2.16it/s]\u001b[0m\n",
      "\u001b[34m86%| | 669/782 [05:11<00:52,  2.16it/s]\u001b[0m\n",
      "\u001b[34m86%| | 670/782 [05:11<00:51,  2.16it/s]\u001b[0m\n",
      "\u001b[34m86%| | 671/782 [05:12<00:51,  2.17it/s]\u001b[0m\n",
      "\u001b[34m86%| | 672/782 [05:12<00:50,  2.17it/s]\u001b[0m\n",
      "\u001b[34m86%| | 673/782 [05:13<00:50,  2.16it/s]\u001b[0m\n",
      "\u001b[34m86%| | 674/782 [05:13<00:49,  2.17it/s]\u001b[0m\n",
      "\u001b[34m86%| | 675/782 [05:14<00:49,  2.16it/s]\u001b[0m\n",
      "\u001b[34m86%| | 676/782 [05:14<00:48,  2.17it/s]\u001b[0m\n",
      "\u001b[34m87%| | 677/782 [05:15<00:48,  2.17it/s]\u001b[0m\n",
      "\u001b[34m87%| | 678/782 [05:15<00:47,  2.17it/s]\u001b[0m\n",
      "\u001b[34m87%| | 679/782 [05:15<00:47,  2.17it/s]\u001b[0m\n",
      "\u001b[34m87%| | 680/782 [05:16<00:47,  2.17it/s]\u001b[0m\n",
      "\u001b[34m87%| | 681/782 [05:16<00:46,  2.17it/s]\u001b[0m\n",
      "\u001b[34m87%| | 682/782 [05:17<00:46,  2.17it/s]\u001b[0m\n",
      "\u001b[34m87%| | 683/782 [05:17<00:45,  2.17it/s]\u001b[0m\n",
      "\u001b[34m87%| | 684/782 [05:18<00:45,  2.17it/s]\u001b[0m\n",
      "\u001b[34m88%| | 685/782 [05:18<00:44,  2.17it/s]\u001b[0m\n",
      "\u001b[34m88%| | 686/782 [05:19<00:44,  2.17it/s]\u001b[0m\n",
      "\u001b[34m88%| | 687/782 [05:19<00:43,  2.17it/s]\u001b[0m\n",
      "\u001b[34m88%| | 688/782 [05:20<00:43,  2.17it/s]\u001b[0m\n",
      "\u001b[34m88%| | 689/782 [05:20<00:42,  2.17it/s]\u001b[0m\n",
      "\u001b[34m88%| | 690/782 [05:21<00:42,  2.17it/s]\u001b[0m\n",
      "\u001b[34m88%| | 691/782 [05:21<00:41,  2.17it/s]\u001b[0m\n",
      "\u001b[34m88%| | 692/782 [05:21<00:41,  2.17it/s]\u001b[0m\n",
      "\u001b[34m89%| | 693/782 [05:22<00:41,  2.16it/s]\u001b[0m\n",
      "\u001b[34m89%| | 694/782 [05:22<00:40,  2.16it/s]\u001b[0m\n",
      "\u001b[34m89%| | 695/782 [05:23<00:40,  2.16it/s]\u001b[0m\n",
      "\u001b[34m89%| | 696/782 [05:23<00:39,  2.16it/s]\u001b[0m\n",
      "\u001b[34m89%| | 697/782 [05:24<00:39,  2.16it/s]\u001b[0m\n",
      "\u001b[34m89%| | 698/782 [05:24<00:38,  2.16it/s]\u001b[0m\n",
      "\u001b[34m89%| | 699/782 [05:25<00:38,  2.16it/s]\u001b[0m\n",
      "\u001b[34m90%| | 700/782 [05:25<00:38,  2.15it/s]\u001b[0m\n",
      "\u001b[34m90%| | 701/782 [05:26<00:37,  2.15it/s]\u001b[0m\n",
      "\u001b[34m90%| | 702/782 [05:26<00:37,  2.16it/s]\u001b[0m\n",
      "\u001b[34m90%| | 703/782 [05:27<00:36,  2.15it/s]\u001b[0m\n",
      "\u001b[34m90%| | 704/782 [05:27<00:36,  2.16it/s]\u001b[0m\n",
      "\u001b[34m90%| | 705/782 [05:27<00:35,  2.16it/s]\u001b[0m\n",
      "\u001b[34m90%| | 706/782 [05:28<00:35,  2.16it/s]\u001b[0m\n",
      "\u001b[34m90%| | 707/782 [05:28<00:34,  2.16it/s]\u001b[0m\n",
      "\u001b[34m91%| | 708/782 [05:29<00:34,  2.16it/s]\u001b[0m\n",
      "\u001b[34m91%| | 709/782 [05:29<00:33,  2.16it/s]\u001b[0m\n",
      "\u001b[34m91%| | 710/782 [05:30<00:33,  2.15it/s]\u001b[0m\n",
      "\u001b[34m91%| | 711/782 [05:30<00:32,  2.15it/s]\u001b[0m\n",
      "\u001b[34m91%| | 712/782 [05:31<00:32,  2.15it/s]\u001b[0m\n",
      "\u001b[34m91%| | 713/782 [05:31<00:32,  2.15it/s]\u001b[0m\n",
      "\u001b[34m91%|| 714/782 [05:32<00:31,  2.15it/s]\u001b[0m\n",
      "\u001b[34m91%|| 715/782 [05:32<00:31,  2.15it/s]\u001b[0m\n",
      "\u001b[34m92%|| 716/782 [05:33<00:30,  2.14it/s]\u001b[0m\n",
      "\u001b[34m92%|| 717/782 [05:33<00:30,  2.15it/s]\u001b[0m\n",
      "\u001b[34m92%|| 718/782 [05:34<00:29,  2.15it/s]\u001b[0m\n",
      "\u001b[34m92%|| 719/782 [05:34<00:29,  2.16it/s]\u001b[0m\n",
      "\u001b[34m92%|| 720/782 [05:34<00:28,  2.16it/s]\u001b[0m\n",
      "\u001b[34m92%|| 721/782 [05:35<00:28,  2.16it/s]\u001b[0m\n",
      "\u001b[34m92%|| 722/782 [05:35<00:27,  2.16it/s]\u001b[0m\n",
      "\u001b[34m92%|| 723/782 [05:36<00:27,  2.16it/s]\u001b[0m\n",
      "\u001b[34m93%|| 724/782 [05:36<00:26,  2.15it/s]\u001b[0m\n",
      "\u001b[34m93%|| 725/782 [05:37<00:26,  2.16it/s]\u001b[0m\n",
      "\u001b[34m93%|| 726/782 [05:37<00:25,  2.16it/s]\u001b[0m\n",
      "\u001b[34m93%|| 727/782 [05:38<00:25,  2.16it/s]\u001b[0m\n",
      "\u001b[34m93%|| 728/782 [05:38<00:24,  2.16it/s]\u001b[0m\n",
      "\u001b[34m93%|| 729/782 [05:39<00:24,  2.16it/s]\u001b[0m\n",
      "\u001b[34m93%|| 730/782 [05:39<00:24,  2.17it/s]\u001b[0m\n",
      "\u001b[34m93%|| 731/782 [05:40<00:23,  2.17it/s]\u001b[0m\n",
      "\u001b[34m94%|| 732/782 [05:40<00:23,  2.17it/s]\u001b[0m\n",
      "\u001b[34m94%|| 733/782 [05:40<00:22,  2.17it/s]\u001b[0m\n",
      "\u001b[34m94%|| 734/782 [05:41<00:22,  2.16it/s]\u001b[0m\n",
      "\u001b[34m94%|| 735/782 [05:41<00:21,  2.15it/s]\u001b[0m\n",
      "\u001b[34m94%|| 736/782 [05:42<00:21,  2.15it/s]\u001b[0m\n",
      "\u001b[34m94%|| 737/782 [05:42<00:20,  2.16it/s]\u001b[0m\n",
      "\u001b[34m94%|| 738/782 [05:43<00:20,  2.16it/s]\u001b[0m\n",
      "\u001b[34m95%|| 739/782 [05:43<00:19,  2.16it/s]\u001b[0m\n",
      "\u001b[34m95%|| 740/782 [05:44<00:19,  2.16it/s]\u001b[0m\n",
      "\u001b[34m95%|| 741/782 [05:44<00:18,  2.16it/s]\u001b[0m\n",
      "\u001b[34m95%|| 742/782 [05:45<00:18,  2.16it/s]\u001b[0m\n",
      "\u001b[34m95%|| 743/782 [05:45<00:18,  2.16it/s]\u001b[0m\n",
      "\u001b[34m95%|| 744/782 [05:46<00:17,  2.16it/s]\u001b[0m\n",
      "\u001b[34m95%|| 745/782 [05:46<00:17,  2.15it/s]\u001b[0m\n",
      "\u001b[34m95%|| 746/782 [05:46<00:16,  2.15it/s]\u001b[0m\n",
      "\u001b[34m96%|| 747/782 [05:47<00:16,  2.15it/s]\u001b[0m\n",
      "\u001b[34m96%|| 748/782 [05:47<00:15,  2.15it/s]\u001b[0m\n",
      "\u001b[34m96%|| 749/782 [05:48<00:15,  2.15it/s]\u001b[0m\n",
      "\u001b[34m96%|| 750/782 [05:48<00:14,  2.15it/s]\u001b[0m\n",
      "\u001b[34m96%|| 751/782 [05:49<00:14,  2.16it/s]\u001b[0m\n",
      "\u001b[34m96%|| 752/782 [05:49<00:13,  2.16it/s]\u001b[0m\n",
      "\u001b[34m96%|| 753/782 [05:50<00:13,  2.16it/s]\u001b[0m\n",
      "\u001b[34m96%|| 754/782 [05:50<00:12,  2.16it/s]\u001b[0m\n",
      "\u001b[34m97%|| 755/782 [05:51<00:12,  2.16it/s]\u001b[0m\n",
      "\u001b[34m97%|| 756/782 [05:51<00:12,  2.16it/s]\u001b[0m\n",
      "\u001b[34m97%|| 757/782 [05:52<00:11,  2.16it/s]\u001b[0m\n",
      "\u001b[34m97%|| 758/782 [05:52<00:11,  2.16it/s]\u001b[0m\n",
      "\u001b[34m97%|| 759/782 [05:53<00:10,  2.16it/s]\u001b[0m\n",
      "\u001b[34m97%|| 760/782 [05:53<00:10,  2.16it/s]\u001b[0m\n",
      "\u001b[34m97%|| 761/782 [05:53<00:09,  2.16it/s]\u001b[0m\n",
      "\u001b[34m97%|| 762/782 [05:54<00:09,  2.16it/s]\u001b[0m\n",
      "\u001b[34m98%|| 763/782 [05:54<00:08,  2.16it/s]\u001b[0m\n",
      "\u001b[34m98%|| 764/782 [05:55<00:08,  2.16it/s]\u001b[0m\n",
      "\u001b[34m98%|| 765/782 [05:55<00:07,  2.16it/s]\u001b[0m\n",
      "\u001b[34m98%|| 766/782 [05:56<00:07,  2.16it/s]\u001b[0m\n",
      "\u001b[34m98%|| 767/782 [05:56<00:06,  2.16it/s]\u001b[0m\n",
      "\u001b[34m98%|| 768/782 [05:57<00:06,  2.16it/s]\u001b[0m\n",
      "\u001b[34m98%|| 769/782 [05:57<00:06,  2.16it/s]\u001b[0m\n",
      "\u001b[34m98%|| 770/782 [05:58<00:05,  2.16it/s]\u001b[0m\n",
      "\u001b[34m99%|| 771/782 [05:58<00:05,  2.16it/s]\u001b[0m\n",
      "\u001b[34m99%|| 772/782 [05:59<00:04,  2.16it/s]\u001b[0m\n",
      "\u001b[34m99%|| 773/782 [05:59<00:04,  2.16it/s]\u001b[0m\n",
      "\u001b[34m99%|| 774/782 [05:59<00:03,  2.16it/s]\u001b[0m\n",
      "\u001b[34m99%|| 775/782 [06:00<00:03,  2.16it/s]\u001b[0m\n",
      "\u001b[34m99%|| 776/782 [06:00<00:02,  2.16it/s]\u001b[0m\n",
      "\u001b[34m99%|| 777/782 [06:01<00:02,  2.17it/s]\u001b[0m\n",
      "\u001b[34m99%|| 778/782 [06:01<00:01,  2.17it/s]\u001b[0m\n",
      "\u001b[34m100%|| 779/782 [06:02<00:01,  2.17it/s]\u001b[0m\n",
      "\u001b[34m100%|| 780/782 [06:02<00:00,  2.17it/s]\u001b[0m\n",
      "\u001b[34m100%|| 781/782 [06:03<00:00,  2.16it/s]\u001b[0m\n",
      "\u001b[34m100%|| 782/782 [06:03<00:00,  2.74it/s]\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 10000\n",
      "  Batch size = 64\u001b[0m\n",
      "\u001b[34mNum examples = 10000\n",
      "  Batch size = 64\u001b[0m\n",
      "\u001b[34m0%|          | 0/157 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m1%|         | 2/157 [00:00<00:23,  6.50it/s]#033[A\u001b[0m\n",
      "\u001b[34m2%|         | 3/157 [00:00<00:33,  4.59it/s]#033[A\u001b[0m\n",
      "\u001b[34m3%|         | 4/157 [00:00<00:38,  3.98it/s]#033[A\u001b[0m\n",
      "\u001b[34m3%|         | 5/157 [00:01<00:41,  3.69it/s]#033[A\u001b[0m\n",
      "\u001b[34m4%|         | 6/157 [00:01<00:42,  3.53it/s]#033[A\u001b[0m\n",
      "\u001b[34m4%|         | 7/157 [00:01<00:43,  3.43it/s]#033[A\u001b[0m\n",
      "\u001b[34m5%|         | 8/157 [00:02<00:44,  3.36it/s]#033[A\u001b[0m\n",
      "\u001b[34m6%|         | 9/157 [00:02<00:44,  3.32it/s]#033[A\u001b[0m\n",
      "\u001b[34m6%|         | 10/157 [00:02<00:44,  3.29it/s]#033[A\u001b[0m\n",
      "\u001b[34m7%|         | 11/157 [00:03<00:44,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m8%|         | 12/157 [00:03<00:44,  3.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m8%|         | 13/157 [00:03<00:44,  3.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m9%|         | 14/157 [00:04<00:44,  3.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m10%|         | 15/157 [00:04<00:43,  3.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m10%|         | 16/157 [00:04<00:43,  3.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m11%|         | 17/157 [00:04<00:43,  3.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m11%|        | 18/157 [00:05<00:43,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m12%|        | 19/157 [00:05<00:42,  3.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m13%|        | 20/157 [00:05<00:42,  3.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m13%|        | 21/157 [00:06<00:42,  3.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m14%|        | 22/157 [00:06<00:41,  3.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m15%|        | 23/157 [00:06<00:41,  3.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m15%|        | 24/157 [00:07<00:41,  3.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m16%|        | 25/157 [00:07<00:40,  3.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m17%|        | 26/157 [00:07<00:40,  3.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m17%|        | 27/157 [00:08<00:40,  3.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m18%|        | 28/157 [00:08<00:39,  3.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m18%|        | 29/157 [00:08<00:39,  3.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m19%|        | 30/157 [00:08<00:39,  3.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m20%|        | 31/157 [00:09<00:38,  3.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m20%|        | 32/157 [00:09<00:38,  3.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m21%|        | 33/157 [00:09<00:38,  3.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m22%|       | 34/157 [00:10<00:37,  3.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m22%|       | 35/157 [00:10<00:37,  3.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m23%|       | 36/157 [00:10<00:37,  3.25it/s]#033[A\u001b[0m\n",
      "\u001b[34m24%|       | 37/157 [00:11<00:37,  3.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m24%|       | 38/157 [00:11<00:36,  3.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m25%|       | 39/157 [00:11<00:36,  3.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m25%|       | 40/157 [00:12<00:36,  3.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m26%|       | 41/157 [00:12<00:35,  3.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m27%|       | 42/157 [00:12<00:35,  3.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m27%|       | 43/157 [00:12<00:35,  3.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m28%|       | 44/157 [00:13<00:34,  3.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m29%|       | 45/157 [00:13<00:34,  3.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m29%|       | 46/157 [00:13<00:34,  3.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m30%|       | 47/157 [00:14<00:34,  3.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m31%|       | 48/157 [00:14<00:33,  3.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m31%|       | 49/157 [00:14<00:33,  3.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m32%|      | 50/157 [00:15<00:33,  3.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m32%|      | 51/157 [00:15<00:32,  3.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m33%|      | 52/157 [00:15<00:32,  3.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m34%|      | 53/157 [00:16<00:32,  3.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m34%|      | 54/157 [00:16<00:31,  3.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m35%|      | 55/157 [00:16<00:31,  3.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m36%|      | 56/157 [00:17<00:31,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m36%|      | 57/157 [00:17<00:31,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m37%|      | 58/157 [00:17<00:30,  3.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m38%|      | 59/157 [00:17<00:30,  3.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m38%|      | 60/157 [00:18<00:30,  3.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m39%|      | 61/157 [00:18<00:29,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m39%|      | 62/157 [00:18<00:29,  3.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m40%|      | 63/157 [00:19<00:29,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m41%|      | 64/157 [00:19<00:29,  3.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m41%|     | 65/157 [00:19<00:28,  3.19it/s]#033[A\u001b[0m\n",
      "\u001b[34m42%|     | 66/157 [00:20<00:28,  3.19it/s]#033[A\u001b[0m\n",
      "\u001b[34m43%|     | 67/157 [00:20<00:28,  3.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m43%|     | 68/157 [00:20<00:27,  3.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m44%|     | 69/157 [00:21<00:27,  3.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m45%|     | 70/157 [00:21<00:27,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m45%|     | 71/157 [00:21<00:26,  3.19it/s]#033[A\u001b[0m\n",
      "\u001b[34m46%|     | 72/157 [00:22<00:26,  3.16it/s]#033[A\u001b[0m\n",
      "\u001b[34m46%|     | 73/157 [00:22<00:26,  3.17it/s]#033[A\u001b[0m\n",
      "\u001b[34m47%|     | 74/157 [00:22<00:26,  3.18it/s]#033[A\u001b[0m\n",
      "\u001b[34m48%|     | 75/157 [00:22<00:25,  3.19it/s]#033[A\u001b[0m\n",
      "\u001b[34m48%|     | 76/157 [00:23<00:25,  3.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m49%|     | 77/157 [00:23<00:25,  3.19it/s]#033[A\u001b[0m\n",
      "\u001b[34m50%|     | 78/157 [00:23<00:24,  3.19it/s]#033[A\u001b[0m\n",
      "\u001b[34m50%|     | 79/157 [00:24<00:24,  3.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m51%|     | 80/157 [00:24<00:23,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m52%|    | 81/157 [00:24<00:23,  3.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m52%|    | 82/157 [00:25<00:23,  3.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m53%|    | 83/157 [00:25<00:23,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m54%|    | 84/157 [00:25<00:22,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m54%|    | 85/157 [00:26<00:22,  3.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m55%|    | 86/157 [00:26<00:22,  3.19it/s]#033[A\u001b[0m\n",
      "\u001b[34m55%|    | 87/157 [00:26<00:21,  3.19it/s]#033[A\u001b[0m\n",
      "\u001b[34m56%|    | 88/157 [00:27<00:21,  3.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m57%|    | 89/157 [00:27<00:21,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m57%|    | 90/157 [00:27<00:20,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m58%|    | 91/157 [00:27<00:20,  3.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m59%|    | 92/157 [00:28<00:20,  3.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m59%|    | 93/157 [00:28<00:19,  3.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m60%|    | 94/157 [00:28<00:19,  3.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m61%|    | 95/157 [00:29<00:19,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m61%|    | 96/157 [00:29<00:19,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m62%|   | 97/157 [00:29<00:18,  3.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m62%|   | 98/157 [00:30<00:18,  3.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m63%|   | 99/157 [00:30<00:18,  3.19it/s]#033[A\u001b[0m\n",
      "\u001b[34m64%|   | 100/157 [00:30<00:17,  3.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m64%|   | 101/157 [00:31<00:17,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m65%|   | 102/157 [00:31<00:17,  3.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m66%|   | 103/157 [00:31<00:16,  3.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m66%|   | 104/157 [00:32<00:16,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m67%|   | 105/157 [00:32<00:16,  3.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m68%|   | 106/157 [00:32<00:15,  3.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m68%|   | 107/157 [00:32<00:15,  3.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m69%|   | 108/157 [00:33<00:15,  3.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m69%|   | 109/157 [00:33<00:14,  3.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m70%|   | 110/157 [00:33<00:14,  3.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m71%|   | 111/157 [00:34<00:14,  3.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m71%|  | 112/157 [00:34<00:14,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m72%|  | 113/157 [00:34<00:13,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m73%|  | 114/157 [00:35<00:13,  3.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m73%|  | 115/157 [00:35<00:13,  3.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m74%|  | 116/157 [00:35<00:12,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m75%|  | 117/157 [00:36<00:12,  3.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m75%|  | 118/157 [00:36<00:12,  3.19it/s]#033[A\u001b[0m\n",
      "\u001b[34m76%|  | 119/157 [00:36<00:11,  3.19it/s]#033[A\u001b[0m\n",
      "\u001b[34m76%|  | 120/157 [00:36<00:11,  3.19it/s]#033[A\u001b[0m\n",
      "\u001b[34m77%|  | 121/157 [00:37<00:11,  3.19it/s]#033[A\u001b[0m\n",
      "\u001b[34m78%|  | 122/157 [00:37<00:10,  3.18it/s]#033[A\u001b[0m\n",
      "\u001b[34m78%|  | 123/157 [00:37<00:10,  3.18it/s]#033[A\u001b[0m\n",
      "\u001b[34m79%|  | 124/157 [00:38<00:10,  3.18it/s]#033[A\u001b[0m\n",
      "\u001b[34m80%|  | 125/157 [00:38<00:10,  3.18it/s]#033[A\u001b[0m\n",
      "\u001b[34m80%|  | 126/157 [00:38<00:09,  3.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m81%|  | 127/157 [00:39<00:09,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m82%| | 128/157 [00:39<00:09,  3.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m82%| | 129/157 [00:39<00:08,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m83%| | 130/157 [00:40<00:08,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m83%| | 131/157 [00:40<00:08,  3.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m84%| | 132/157 [00:40<00:07,  3.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m85%| | 133/157 [00:41<00:07,  3.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m85%| | 134/157 [00:41<00:07,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m86%| | 135/157 [00:41<00:06,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m87%| | 136/157 [00:41<00:06,  3.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m87%| | 137/157 [00:42<00:06,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m88%| | 138/157 [00:42<00:05,  3.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m89%| | 139/157 [00:42<00:05,  3.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m89%| | 140/157 [00:43<00:05,  3.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m90%| | 141/157 [00:43<00:04,  3.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m90%| | 142/157 [00:43<00:04,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m91%| | 143/157 [00:44<00:04,  3.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m92%|| 144/157 [00:44<00:04,  3.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m92%|| 145/157 [00:44<00:03,  3.19it/s]#033[A\u001b[0m\n",
      "\u001b[34m93%|| 146/157 [00:45<00:03,  3.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m94%|| 147/157 [00:45<00:03,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m94%|| 148/157 [00:45<00:02,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m95%|| 149/157 [00:46<00:02,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m96%|| 150/157 [00:46<00:02,  3.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m96%|| 151/157 [00:46<00:01,  3.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m97%|| 152/157 [00:46<00:01,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m97%|| 153/157 [00:47<00:01,  3.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m98%|| 154/157 [00:47<00:00,  3.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m99%|| 155/157 [00:47<00:00,  3.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m99%|| 156/157 [00:48<00:00,  3.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.19367139041423798, 'eval_accuracy': 0.9252, 'eval_f1': 0.926057730328193, 'eval_precision': 0.9291807181114858, 'eval_recall': 0.9229556650246306, 'eval_runtime': 48.6179, 'eval_samples_per_second': 205.685, 'eval_steps_per_second': 3.229, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m100%|| 782/782 [06:51<00:00,  2.74it/s]\u001b[0m\n",
      "\u001b[34m#015100%|| 157/157 [00:48<00:00,  3.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m{'train_runtime': 411.9412, 'train_samples_per_second': 60.688, 'train_steps_per_second': 1.898, 'train_loss': 0.3071785265832301, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m100%|| 782/782 [06:51<00:00,  2.74it/s]\u001b[0m\n",
      "\u001b[34m100%|| 782/782 [06:51<00:00,  1.90it/s]\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 10000\n",
      "  Batch size = 64\u001b[0m\n",
      "\u001b[34mNum examples = 10000\n",
      "  Batch size = 64\u001b[0m\n",
      "\u001b[34m0%|          | 0/157 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m1%|         | 2/157 [00:00<00:23,  6.46it/s]\u001b[0m\n",
      "\u001b[34m2%|         | 3/157 [00:00<00:34,  4.52it/s]\u001b[0m\n",
      "\u001b[34m3%|         | 4/157 [00:00<00:39,  3.91it/s]\u001b[0m\n",
      "\u001b[34m3%|         | 5/157 [00:01<00:41,  3.63it/s]\u001b[0m\n",
      "\u001b[34m4%|         | 6/157 [00:01<00:43,  3.47it/s]\u001b[0m\n",
      "\u001b[34m4%|         | 7/157 [00:01<00:44,  3.39it/s]\u001b[0m\n",
      "\u001b[34m5%|         | 8/157 [00:02<00:44,  3.32it/s]\u001b[0m\n",
      "\u001b[34m6%|         | 9/157 [00:02<00:45,  3.29it/s]\u001b[0m\n",
      "\u001b[34m6%|         | 10/157 [00:02<00:44,  3.27it/s]\u001b[0m\n",
      "\u001b[34m7%|         | 11/157 [00:03<00:44,  3.26it/s]\u001b[0m\n",
      "\u001b[34m8%|         | 12/157 [00:03<00:44,  3.25it/s]\u001b[0m\n",
      "\u001b[34m8%|         | 13/157 [00:03<00:44,  3.24it/s]\u001b[0m\n",
      "\u001b[34m9%|         | 14/157 [00:04<00:44,  3.24it/s]\u001b[0m\n",
      "\u001b[34m10%|         | 15/157 [00:04<00:43,  3.24it/s]\u001b[0m\n",
      "\u001b[34m10%|         | 16/157 [00:04<00:43,  3.24it/s]\u001b[0m\n",
      "\u001b[34m11%|         | 17/157 [00:04<00:43,  3.23it/s]\u001b[0m\n",
      "\u001b[34m11%|        | 18/157 [00:05<00:42,  3.23it/s]\u001b[0m\n",
      "\u001b[34m12%|        | 19/157 [00:05<00:42,  3.22it/s]\u001b[0m\n",
      "\u001b[34m13%|        | 20/157 [00:05<00:42,  3.22it/s]\u001b[0m\n",
      "\u001b[34m13%|        | 21/157 [00:06<00:42,  3.21it/s]\u001b[0m\n",
      "\u001b[34m14%|        | 22/157 [00:06<00:42,  3.21it/s]\u001b[0m\n",
      "\u001b[34m15%|        | 23/157 [00:06<00:41,  3.22it/s]\u001b[0m\n",
      "\u001b[34m15%|        | 24/157 [00:07<00:41,  3.22it/s]\u001b[0m\n",
      "\u001b[34m16%|        | 25/157 [00:07<00:41,  3.21it/s]\u001b[0m\n",
      "\u001b[34m17%|        | 26/157 [00:07<00:40,  3.22it/s]\u001b[0m\n",
      "\u001b[34m17%|        | 27/157 [00:08<00:40,  3.21it/s]\u001b[0m\n",
      "\u001b[34m18%|        | 28/157 [00:08<00:40,  3.22it/s]\u001b[0m\n",
      "\u001b[34m18%|        | 29/157 [00:08<00:39,  3.22it/s]\u001b[0m\n",
      "\u001b[34m19%|        | 30/157 [00:09<00:39,  3.22it/s]\u001b[0m\n",
      "\u001b[34m20%|        | 31/157 [00:09<00:39,  3.21it/s]\u001b[0m\n",
      "\u001b[34m20%|        | 32/157 [00:09<00:39,  3.20it/s]\u001b[0m\n",
      "\u001b[34m21%|        | 33/157 [00:09<00:38,  3.20it/s]\u001b[0m\n",
      "\u001b[34m22%|       | 34/157 [00:10<00:38,  3.19it/s]\u001b[0m\n",
      "\u001b[34m22%|       | 35/157 [00:10<00:38,  3.20it/s]\u001b[0m\n",
      "\u001b[34m23%|       | 36/157 [00:10<00:37,  3.21it/s]\u001b[0m\n",
      "\u001b[34m24%|       | 37/157 [00:11<00:37,  3.22it/s]\u001b[0m\n",
      "\u001b[34m24%|       | 38/157 [00:11<00:37,  3.21it/s]\u001b[0m\n",
      "\u001b[34m25%|       | 39/157 [00:11<00:36,  3.20it/s]\u001b[0m\n",
      "\u001b[34m25%|       | 40/157 [00:12<00:36,  3.20it/s]\u001b[0m\n",
      "\u001b[34m26%|       | 41/157 [00:12<00:36,  3.21it/s]\u001b[0m\n",
      "\u001b[34m27%|       | 42/157 [00:12<00:35,  3.20it/s]\u001b[0m\n",
      "\u001b[34m27%|       | 43/157 [00:13<00:35,  3.21it/s]\u001b[0m\n",
      "\u001b[34m28%|       | 44/157 [00:13<00:35,  3.22it/s]\u001b[0m\n",
      "\u001b[34m29%|       | 45/157 [00:13<00:34,  3.22it/s]\u001b[0m\n",
      "\u001b[34m29%|       | 46/157 [00:14<00:34,  3.22it/s]\u001b[0m\n",
      "\u001b[34m30%|       | 47/157 [00:14<00:34,  3.22it/s]\u001b[0m\n",
      "\u001b[34m31%|       | 48/157 [00:14<00:33,  3.21it/s]\u001b[0m\n",
      "\u001b[34m31%|       | 49/157 [00:14<00:33,  3.22it/s]\u001b[0m\n",
      "\u001b[34m32%|      | 50/157 [00:15<00:33,  3.21it/s]\u001b[0m\n",
      "\u001b[34m32%|      | 51/157 [00:15<00:32,  3.21it/s]\u001b[0m\n",
      "\u001b[34m33%|      | 52/157 [00:15<00:32,  3.22it/s]\u001b[0m\n",
      "\u001b[34m34%|      | 53/157 [00:16<00:32,  3.21it/s]\u001b[0m\n",
      "\u001b[34m34%|      | 54/157 [00:16<00:32,  3.20it/s]\u001b[0m\n",
      "\u001b[34m35%|      | 55/157 [00:16<00:31,  3.21it/s]\u001b[0m\n",
      "\u001b[34m36%|      | 56/157 [00:17<00:31,  3.20it/s]\u001b[0m\n",
      "\u001b[34m36%|      | 57/157 [00:17<00:31,  3.21it/s]\u001b[0m\n",
      "\u001b[34m37%|      | 58/157 [00:17<00:30,  3.22it/s]\u001b[0m\n",
      "\u001b[34m38%|      | 59/157 [00:18<00:30,  3.22it/s]\u001b[0m\n",
      "\u001b[34m38%|      | 60/157 [00:18<00:30,  3.22it/s]\u001b[0m\n",
      "\u001b[34m39%|      | 61/157 [00:18<00:29,  3.22it/s]\u001b[0m\n",
      "\u001b[34m39%|      | 62/157 [00:18<00:29,  3.22it/s]\u001b[0m\n",
      "\u001b[34m40%|      | 63/157 [00:19<00:29,  3.21it/s]\u001b[0m\n",
      "\u001b[34m41%|      | 64/157 [00:19<00:28,  3.22it/s]\u001b[0m\n",
      "\u001b[34m41%|     | 65/157 [00:19<00:28,  3.21it/s]\u001b[0m\n",
      "\u001b[34m42%|     | 66/157 [00:20<00:28,  3.20it/s]\u001b[0m\n",
      "\u001b[34m43%|     | 67/157 [00:20<00:28,  3.20it/s]\u001b[0m\n",
      "\u001b[34m43%|     | 68/157 [00:20<00:27,  3.21it/s]\u001b[0m\n",
      "\u001b[34m44%|     | 69/157 [00:21<00:27,  3.22it/s]\u001b[0m\n",
      "\u001b[34m45%|     | 70/157 [00:21<00:27,  3.22it/s]\u001b[0m\n",
      "\u001b[34m45%|     | 71/157 [00:21<00:26,  3.22it/s]\u001b[0m\n",
      "\u001b[34m46%|     | 72/157 [00:22<00:26,  3.23it/s]\u001b[0m\n",
      "\u001b[34m46%|     | 73/157 [00:22<00:26,  3.23it/s]\u001b[0m\n",
      "\u001b[34m47%|     | 74/157 [00:22<00:25,  3.23it/s]\u001b[0m\n",
      "\u001b[34m48%|     | 75/157 [00:23<00:25,  3.24it/s]\u001b[0m\n",
      "\u001b[34m48%|     | 76/157 [00:23<00:25,  3.22it/s]\u001b[0m\n",
      "\u001b[34m49%|     | 77/157 [00:23<00:24,  3.21it/s]\u001b[0m\n",
      "\u001b[34m50%|     | 78/157 [00:23<00:24,  3.20it/s]\u001b[0m\n",
      "\u001b[34m50%|     | 79/157 [00:24<00:24,  3.20it/s]\u001b[0m\n",
      "\u001b[34m51%|     | 80/157 [00:24<00:24,  3.19it/s]\u001b[0m\n",
      "\u001b[34m52%|    | 81/157 [00:24<00:23,  3.19it/s]\u001b[0m\n",
      "\u001b[34m52%|    | 82/157 [00:25<00:23,  3.19it/s]\u001b[0m\n",
      "\u001b[34m53%|    | 83/157 [00:25<00:23,  3.19it/s]\u001b[0m\n",
      "\u001b[34m54%|    | 84/157 [00:25<00:22,  3.21it/s]\u001b[0m\n",
      "\u001b[34m54%|    | 85/157 [00:26<00:22,  3.22it/s]\u001b[0m\n",
      "\u001b[34m55%|    | 86/157 [00:26<00:22,  3.20it/s]\u001b[0m\n",
      "\u001b[34m55%|    | 87/157 [00:26<00:21,  3.22it/s]\u001b[0m\n",
      "\u001b[34m56%|    | 88/157 [00:27<00:21,  3.22it/s]\u001b[0m\n",
      "\u001b[34m57%|    | 89/157 [00:27<00:21,  3.22it/s]\u001b[0m\n",
      "\u001b[34m57%|    | 90/157 [00:27<00:20,  3.23it/s]\u001b[0m\n",
      "\u001b[34m58%|    | 91/157 [00:28<00:20,  3.23it/s]\u001b[0m\n",
      "\u001b[34m59%|    | 92/157 [00:28<00:20,  3.23it/s]\u001b[0m\n",
      "\u001b[34m59%|    | 93/157 [00:28<00:19,  3.23it/s]\u001b[0m\n",
      "\u001b[34m60%|    | 94/157 [00:28<00:19,  3.23it/s]\u001b[0m\n",
      "\u001b[34m61%|    | 95/157 [00:29<00:19,  3.23it/s]\u001b[0m\n",
      "\u001b[34m61%|    | 96/157 [00:29<00:18,  3.23it/s]\u001b[0m\n",
      "\u001b[34m62%|   | 97/157 [00:29<00:18,  3.23it/s]\u001b[0m\n",
      "\u001b[34m62%|   | 98/157 [00:30<00:18,  3.23it/s]\u001b[0m\n",
      "\u001b[34m63%|   | 99/157 [00:30<00:17,  3.23it/s]\u001b[0m\n",
      "\u001b[34m64%|   | 100/157 [00:30<00:17,  3.23it/s]\u001b[0m\n",
      "\u001b[34m64%|   | 101/157 [00:31<00:17,  3.23it/s]\u001b[0m\n",
      "\u001b[34m65%|   | 102/157 [00:31<00:17,  3.23it/s]\u001b[0m\n",
      "\u001b[34m66%|   | 103/157 [00:31<00:16,  3.23it/s]\u001b[0m\n",
      "\u001b[34m66%|   | 104/157 [00:32<00:16,  3.22it/s]\u001b[0m\n",
      "\u001b[34m67%|   | 105/157 [00:32<00:16,  3.21it/s]\u001b[0m\n",
      "\u001b[34m68%|   | 106/157 [00:32<00:15,  3.22it/s]\u001b[0m\n",
      "\u001b[34m68%|   | 107/157 [00:32<00:15,  3.22it/s]\u001b[0m\n",
      "\u001b[34m69%|   | 108/157 [00:33<00:15,  3.21it/s]\u001b[0m\n",
      "\u001b[34m69%|   | 109/157 [00:33<00:14,  3.21it/s]\u001b[0m\n",
      "\u001b[34m70%|   | 110/157 [00:33<00:14,  3.21it/s]\u001b[0m\n",
      "\u001b[34m71%|   | 111/157 [00:34<00:14,  3.22it/s]\u001b[0m\n",
      "\u001b[34m71%|  | 112/157 [00:34<00:13,  3.22it/s]\u001b[0m\n",
      "\u001b[34m72%|  | 113/157 [00:34<00:13,  3.21it/s]\u001b[0m\n",
      "\u001b[34m73%|  | 114/157 [00:35<00:13,  3.21it/s]\u001b[0m\n",
      "\u001b[34m73%|  | 115/157 [00:35<00:13,  3.20it/s]\u001b[0m\n",
      "\u001b[34m74%|  | 116/157 [00:35<00:12,  3.21it/s]\u001b[0m\n",
      "\u001b[34m75%|  | 117/157 [00:36<00:12,  3.21it/s]\u001b[0m\n",
      "\u001b[34m75%|  | 118/157 [00:36<00:12,  3.21it/s]\u001b[0m\n",
      "\u001b[34m76%|  | 119/157 [00:36<00:11,  3.20it/s]\u001b[0m\n",
      "\u001b[34m76%|  | 120/157 [00:37<00:11,  3.21it/s]\u001b[0m\n",
      "\u001b[34m77%|  | 121/157 [00:37<00:11,  3.22it/s]\u001b[0m\n",
      "\u001b[34m78%|  | 122/157 [00:37<00:10,  3.23it/s]\u001b[0m\n",
      "\u001b[34m78%|  | 123/157 [00:37<00:10,  3.23it/s]\u001b[0m\n",
      "\u001b[34m79%|  | 124/157 [00:38<00:10,  3.22it/s]\u001b[0m\n",
      "\u001b[34m80%|  | 125/157 [00:38<00:09,  3.22it/s]\u001b[0m\n",
      "\u001b[34m80%|  | 126/157 [00:38<00:09,  3.21it/s]\u001b[0m\n",
      "\u001b[34m81%|  | 127/157 [00:39<00:09,  3.22it/s]\u001b[0m\n",
      "\u001b[34m82%| | 128/157 [00:39<00:09,  3.22it/s]\u001b[0m\n",
      "\u001b[34m82%| | 129/157 [00:39<00:08,  3.21it/s]\u001b[0m\n",
      "\u001b[34m83%| | 130/157 [00:40<00:08,  3.21it/s]\u001b[0m\n",
      "\u001b[34m83%| | 131/157 [00:40<00:08,  3.20it/s]\u001b[0m\n",
      "\u001b[34m84%| | 132/157 [00:40<00:07,  3.21it/s]\u001b[0m\n",
      "\u001b[34m85%| | 133/157 [00:41<00:07,  3.22it/s]\u001b[0m\n",
      "\u001b[34m85%| | 134/157 [00:41<00:07,  3.22it/s]\u001b[0m\n",
      "\u001b[34m86%| | 135/157 [00:41<00:06,  3.21it/s]\u001b[0m\n",
      "\u001b[34m87%| | 136/157 [00:41<00:06,  3.20it/s]\u001b[0m\n",
      "\u001b[34m87%| | 137/157 [00:42<00:06,  3.21it/s]\u001b[0m\n",
      "\u001b[34m88%| | 138/157 [00:42<00:05,  3.22it/s]\u001b[0m\n",
      "\u001b[34m89%| | 139/157 [00:42<00:05,  3.22it/s]\u001b[0m\n",
      "\u001b[34m89%| | 140/157 [00:43<00:05,  3.23it/s]\u001b[0m\n",
      "\u001b[34m90%| | 141/157 [00:43<00:04,  3.22it/s]\u001b[0m\n",
      "\u001b[34m90%| | 142/157 [00:43<00:04,  3.21it/s]\u001b[0m\n",
      "\u001b[34m91%| | 143/157 [00:44<00:04,  3.20it/s]\u001b[0m\n",
      "\u001b[34m92%|| 144/157 [00:44<00:04,  3.21it/s]\u001b[0m\n",
      "\u001b[34m92%|| 145/157 [00:44<00:03,  3.22it/s]\u001b[0m\n",
      "\u001b[34m93%|| 146/157 [00:45<00:03,  3.23it/s]\u001b[0m\n",
      "\u001b[34m94%|| 147/157 [00:45<00:03,  3.23it/s]\u001b[0m\n",
      "\u001b[34m94%|| 148/157 [00:45<00:02,  3.23it/s]\u001b[0m\n",
      "\u001b[34m95%|| 149/157 [00:46<00:02,  3.23it/s]\u001b[0m\n",
      "\u001b[34m96%|| 150/157 [00:46<00:02,  3.23it/s]\u001b[0m\n",
      "\u001b[34m96%|| 151/157 [00:46<00:01,  3.23it/s]\u001b[0m\n",
      "\u001b[34m97%|| 152/157 [00:46<00:01,  3.23it/s]\u001b[0m\n",
      "\u001b[34m97%|| 153/157 [00:47<00:01,  3.23it/s]\u001b[0m\n",
      "\u001b[34m98%|| 154/157 [00:47<00:00,  3.22it/s]\u001b[0m\n",
      "\u001b[34m99%|| 155/157 [00:47<00:00,  3.21it/s]\u001b[0m\n",
      "\u001b[34m99%|| 156/157 [00:48<00:00,  3.22it/s]\u001b[0m\n",
      "\u001b[34m100%|| 157/157 [00:48<00:00,  3.25it/s]\u001b[0m\n",
      "\u001b[34m***** Eval results *****\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m2023-07-24 16:24:19,648 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-07-24 16:24:19,648 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-07-24 16:24:19,649 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# access the logs of the training job\n",
    "huggingface_estimator.sagemaker_session.logs_for_job(huggingface_estimator.latest_training_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attach to old training job to an estimator \n",
    "\n",
    "In Sagemaker you can attach an old training job to an estimator to continue training, get results etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "# job which is going to be attached to the estimator\n",
    "old_training_job_name=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach old training job\n",
    "huggingface_estimator_loaded = Estimator.attach(old_training_job_name)\n",
    "\n",
    "# get model output s3 from training job\n",
    "huggingface_estimator_loaded.model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./model.tar.gz']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "S3Downloader.download(\n",
    "    s3_uri=huggingface_estimator.model_data, # S3 URI where the trained model is located\n",
    "    local_path='.',                          # local path where *.targ.gz is saved\n",
    "    sagemaker_session=sess                   # SageMaker session used for training the model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-pytorch-training-2023-07-25-15-39-26-321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-25 15:40:21 Starting - Starting the training job.........\n",
      "2023-07-25 15:41:28 Starting - Preparing the instances for training......\n",
      "2023-07-25 15:42:44 Downloading - Downloading input data...\n",
      "2023-07-25 15:43:06 Training - Downloading the training image...............\n",
      "2023-07-25 15:45:47 Training - Training image download completed. Training in progress....\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-07-25 15:46:13,077 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-07-25 15:46:13,142 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-07-25 15:46:13,155 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-07-25 15:46:13,158 sagemaker_pytorch_container.training INFO     Invoking SMDataParallel\u001b[0m\n",
      "\u001b[34m2023-07-25 15:46:13,158 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-07-25 15:46:19,985 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-07-25 15:46:20,071 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-07-25 15:46:20,085 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2023-07-25 15:46:20,085 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2023-07-25 15:46:20,088 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2023-07-25 15:46:20,088 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[34m2023-07-25 15:46:20,088 sagemaker-training-toolkit INFO     Host: ['algo-1']\u001b[0m\n",
      "\u001b[34m2023-07-25 15:46:20,090 sagemaker-training-toolkit INFO     sagemaker_communication_backend: None\u001b[0m\n",
      "\u001b[34m2023-07-25 15:46:20,090 sagemaker-training-toolkit WARNING  Missing library /opt/conda/lib/libsmddp.so for SMDDP collective\u001b[0m\n",
      "\u001b[34m2023-07-25 15:46:20,090 sagemaker-training-toolkit WARNING  The system is not configured to run SMDDP collectives optimizedfor AWS infrastructure.Please use the latest SageMaker Deep Learning Container (DLC) to enable SMDDP Collectives support.\u001b[0m\n",
      "\u001b[34mContinuing model training with default NCCL communication backend.\u001b[0m\n",
      "\u001b[34m2023-07-25 15:46:20,090 sagemaker-training-toolkit INFO     instance type: ml.p3.16xlarge\u001b[0m\n",
      "\u001b[34m2023-07-25 15:46:20,090 sagemaker-training-toolkit INFO     Env Hosts: ['algo-1'] Hosts: ['algo-1'] process_per_hosts: 8 num_processes: 8\u001b[0m\n",
      "\u001b[34m2023-07-25 15:46:20,162 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-07-25 15:46:20,176 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_distributed_dataparallel_custom_mpi_options\": \"-verbose -mca distributed-backend nccl -mca ddp-backend no_c10d\",\n",
      "        \"sagemaker_distributed_dataparallel_enabled\": true,\n",
      "        \"sagemaker_instance_type\": \"ml.p3.16xlarge\"\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p3.16xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 1,\n",
      "        \"eval_batch_size\": 256,\n",
      "        \"model_name\": \"distilbert-base-uncased\",\n",
      "        \"train_batch_size\": 32\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p3.16xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2023-07-25-15-39-26-321\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-578864530451/huggingface-pytorch-training-2023-07-25-15-39-26-321/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train-LORA\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.16xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.16xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train-LORA.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":1,\"eval_batch_size\":256,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":32}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train-LORA.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_distributed_dataparallel_custom_mpi_options\":\"-verbose -mca distributed-backend nccl -mca ddp-backend no_c10d\",\"sagemaker_distributed_dataparallel_enabled\":true,\"sagemaker_instance_type\":\"ml.p3.16xlarge\"}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.16xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p3.16xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train-LORA\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-578864530451/huggingface-pytorch-training-2023-07-25-15-39-26-321/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_custom_mpi_options\":\"-verbose -mca distributed-backend nccl -mca ddp-backend no_c10d\",\"sagemaker_distributed_dataparallel_enabled\":true,\"sagemaker_instance_type\":\"ml.p3.16xlarge\"},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p3.16xlarge\",\"distribution_hosts\":[\"algo-1\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":1,\"eval_batch_size\":256,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":32},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-pytorch-training-2023-07-25-15-39-26-321\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-578864530451/huggingface-pytorch-training-2023-07-25-15-39-26-321/source/sourcedir.tar.gz\",\"module_name\":\"train-LORA\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.16xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train-LORA.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"1\",\"--eval_batch_size\",\"256\",\"--model_name\",\"distilbert-base-uncased\",\"--train_batch_size\",\"32\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_BATCH_SIZE=256\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=distilbert-base-uncased\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=32\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1 -np 8 --allow-run-as-root --tag-output --oversubscribe -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -mca plm_rsh_num_concurrent 1 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x SMDATAPARALLEL_USE_SINGLENODE=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1 -x LD_PRELOAD=/opt/conda/lib/python3.9/site-packages/gethostname.cpython-39-x86_64-linux-gnu.so -verbose -mca distributed-backend nccl -mca ddp-backend no_c10d -x USE_SMDDP_COLLECTIVES=0 smddprun /opt/conda/bin/python3.9 -m mpi4py train-LORA.py --epochs 1 --eval_batch_size 256 --model_name distilbert-base-uncased --train_batch_size 32\u001b[0m\n",
      "\u001b[34m[2023-07-25 15:46:22.406: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m2023-07-25 15:46:22,412 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Collecting evaluate\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Collecting evaluate\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Collecting evaluate\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Collecting evaluate\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Collecting evaluate\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Collecting evaluate\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Collecting evaluate\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:#033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:     #033[90m#033[0m #033[32m0.0/81.4 kB#033[0m #033[31m?#033[0m eta #033[36m-:--:--#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:#033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:#033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:     #033[90m#033[0m #033[32m0.0/81.4 kB#033[0m #033[31m?#033[0m eta #033[36m-:--:--#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:     #033[90m#033[0m #033[32m0.0/81.4 kB#033[0m #033[31m?#033[0m eta #033[36m-:--:--#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:#033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:#033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:     #033[90m#033[0m #033[32m0.0/81.4 kB#033[0m #033[31m?#033[0m eta #033[36m-:--:--#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:     #033[90m#033[0m #033[32m0.0/81.4 kB#033[0m #033[31m?#033[0m eta #033[36m-:--:--#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:#033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:#015#033[2K     #033[90m#033[0m #033[32m81.4/81.4 kB#033[0m #033[31m4.3 MB/s#033[0m eta #033[36m0:00:00#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:     #033[90m#033[0m #033[32m0.0/81.4 kB#033[0m #033[31m?#033[0m eta #033[36m-:--:--#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:#033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:#015#033[2K     #033[90m#033[0m #033[32m81.4/81.4 kB#033[0m #033[31m5.0 MB/s#033[0m eta #033[36m0:00:00#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:     #033[90m#033[0m #033[32m0.0/81.4 kB#033[0m #033[31m?#033[0m eta #033[36m-:--:--#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:#015#033[2K     #033[90m#033[0m #033[32m81.4/81.4 kB#033[0m #033[31m4.2 MB/s#033[0m eta #033[36m0:00:00#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:#015#033[2K     #033[90m#033[0m #033[32m81.4/81.4 kB#033[0m #033[31m5.0 MB/s#033[0m eta #033[36m0:00:00#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:#015#033[2K     #033[90m#033[0m #033[32m81.4/81.4 kB#033[0m #033[31m4.3 MB/s#033[0m eta #033[36m0:00:00#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:#015#033[2K     #033[90m#033[0m #033[32m81.4/81.4 kB#033[0m #033[31m5.1 MB/s#033[0m eta #033[36m0:00:00#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:#015#033[2K     #033[90m#033[0m #033[32m81.4/81.4 kB#033[0m #033[31m4.5 MB/s#033[0m eta #033[36m0:00:00#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.70.14)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.18.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: dill in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.3.6)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2023.1.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from evaluate) (1.23.5)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from evaluate) (23.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from evaluate) (1.5.3)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.70.14)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.9/site-packages (from evaluate) (4.64.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from evaluate) (3.2.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from evaluate) (1.23.5)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.12.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.12.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from evaluate) (23.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.18.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2.28.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.9/site-packages (from evaluate) (4.64.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2.9.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2.9.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2.28.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: dill in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.3.6)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from evaluate) (23.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from evaluate) (3.2.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from evaluate) (1.5.3)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2.28.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: dill in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.3.6)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from evaluate) (1.5.3)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.70.14)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.12.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2023.1.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from evaluate) (1.23.5)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2023.1.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2023.1.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from evaluate) (1.23.5)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.12.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from evaluate) (23.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from evaluate) (3.2.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2.9.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.18.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from evaluate) (3.2.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2.28.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.9/site-packages (from evaluate) (4.64.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.18.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2.9.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.18.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.70.14)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.9/site-packages (from evaluate) (4.64.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2.28.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.70.14)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.12.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from evaluate) (1.5.3)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: dill in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.3.6)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.18.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: dill in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.3.6)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from evaluate) (1.5.3)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from evaluate) (1.5.3)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: dill in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.3.6)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2.9.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2.9.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from evaluate) (1.23.5)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from evaluate) (1.23.5)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2023.1.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.9/site-packages (from evaluate) (4.64.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from evaluate) (3.2.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2023.1.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2.28.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from evaluate) (23.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from evaluate) (23.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from evaluate) (3.2.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.9/site-packages (from evaluate) (4.64.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.12.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: dill in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.3.6)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.70.14)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2.28.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.70.14)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2.9.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.9/site-packages (from evaluate) (4.64.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from evaluate) (3.2.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from evaluate) (23.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.12.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2023.1.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from evaluate) (1.5.3)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.18.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from evaluate) (1.23.5)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (5.4.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (5.4.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (5.4.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (5.4.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (5.4.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (5.4.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (5.4.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.9.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.4.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.9.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.4.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (3.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2.1.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2022.12.7)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2022.12.7)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (1.26.14)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.9.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2.1.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (3.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.4.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (1.26.14)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.4.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.9.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.9.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.4.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.4.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.9.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2.1.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (1.26.14)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2.1.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2022.12.7)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2022.12.7)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2.1.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (3.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (3.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (1.26.14)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2022.12.7)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (3.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (1.26.14)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.9.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.4.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2022.12.7)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2.1.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (3.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (1.26.14)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2022.12.7)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2.1.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (3.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (1.26.14)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->evaluate) (2.8.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->evaluate) (2022.7.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->evaluate) (2.8.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->evaluate) (2022.7.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->evaluate) (2.8.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->evaluate) (2.8.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->evaluate) (2022.7.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->evaluate) (2022.7.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->evaluate) (2.8.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->evaluate) (2022.7.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.2.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.2.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->evaluate) (2.8.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->evaluate) (2022.7.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->evaluate) (2.8.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->evaluate) (2022.7.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.2.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.2.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.2.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.2.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.2.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Collecting evaluate\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  Using cached evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from evaluate) (3.2.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2023.1.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.18.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.9/site-packages (from evaluate) (4.64.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from evaluate) (1.23.5)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.70.14)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.12.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2.28.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from evaluate) (23.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: dill in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.3.6)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from evaluate) (1.5.3)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2.9.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (5.4.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.9.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.4.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2.1.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2022.12.7)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (1.26.14)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (3.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->evaluate) (2.8.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->evaluate) (2022.7.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.2.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Installing collected packages: evaluate\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Installing collected packages: evaluate\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Installing collected packages: evaluate\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Installing collected packages: evaluate\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Installing collected packages: evaluate\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Installing collected packages: evaluate\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Installing collected packages: evaluate\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Successfully installed evaluate-0.4.0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Successfully installed evaluate-0.4.0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Successfully installed evaluate-0.4.0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Successfully installed evaluate-0.4.0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Successfully installed evaluate-0.4.0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Successfully installed evaluate-0.4.0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Successfully installed evaluate-0.4.0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Installing collected packages: evaluate\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Successfully installed evaluate-0.4.0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Collecting rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Collecting rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Collecting rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Collecting rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  Downloading rouge_score-0.1.2.tar.gz (17 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  Preparing metadata (setup.py) ... #033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  Downloading rouge_score-0.1.2.tar.gz (17 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  Downloading rouge_score-0.1.2.tar.gz (17 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  Preparing metadata (setup.py) ... #033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  Downloading rouge_score-0.1.2.tar.gz (17 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  Preparing metadata (setup.py) ... #033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  Preparing metadata (setup.py) ... #033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Collecting rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  Using cached rouge_score-0.1.2.tar.gz (17 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Collecting rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  Using cached rouge_score-0.1.2.tar.gz (17 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  Preparing metadata (setup.py) ... #033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  Preparing metadata (setup.py) ... #033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Collecting rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  Using cached rouge_score-0.1.2.tar.gz (17 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  Preparing metadata (setup.py) ... #033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Collecting rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  Using cached rouge_score-0.1.2.tar.gz (17 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  Preparing metadata (setup.py) ... #033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:-\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:-\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:-\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:-\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:-\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:-\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:#010 #010done\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:-\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:#010 #010done\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:#010 #010done\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:#010 #010done\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /simple/absl-py/\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:#010 #010done\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Collecting absl-py\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Collecting absl-py\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:#010 #010done\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Collecting absl-py\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:#033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:     #033[90m#033[0m #033[32m0.0/126.5 kB#033[0m #033[31m?#033[0m eta #033[36m-:--:--#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:#033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:#033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:     #033[90m#033[0m #033[32m0.0/126.5 kB#033[0m #033[31m?#033[0m eta #033[36m-:--:--#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:     #033[90m#033[0m #033[32m0.0/126.5 kB#033[0m #033[31m?#033[0m eta #033[36m-:--:--#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Collecting absl-py\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:#015#033[2K     #033[90m#033[0m #033[32m126.5/126.5 kB#033[0m #033[31m7.5 MB/s#033[0m eta #033[36m0:00:00#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:#015#033[2K     #033[90m#033[0m #033[32m126.5/126.5 kB#033[0m #033[31m7.9 MB/s#033[0m eta #033[36m0:00:00#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:#015#033[2K     #033[90m#033[0m #033[32m126.5/126.5 kB#033[0m #033[31m7.2 MB/s#033[0m eta #033[36m0:00:00#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Collecting absl-py\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:-\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:#010 #010done\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Collecting nltk\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Collecting nltk\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Collecting nltk\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:#033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:     #033[90m#033[0m #033[32m0.0/1.5 MB#033[0m #033[31m?#033[0m eta #033[36m-:--:--#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:#033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Collecting absl-py\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Collecting nltk\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Collecting nltk\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:     #033[90m#033[0m #033[32m0.0/1.5 MB#033[0m #033[31m?#033[0m eta #033[36m-:--:--#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:#033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:#015#033[2K     #033[91m#033[0m#033[91m#033[0m#033[90m#033[0m #033[32m0.3/1.5 MB#033[0m #033[31m117.2 MB/s#033[0m eta #033[36m0:00:01#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:     #033[90m#033[0m #033[32m0.0/1.5 MB#033[0m #033[31m?#033[0m eta #033[36m-:--:--#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:ERROR: Exception:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  File \"/opt/conda/lib/python3.9/site-packages/pip/_vendor/urllib3/response.py\", line 438, in _error_catcher\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    yield\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  File \"/opt/conda/lib/python3.9/site-packages/pip/_vendor/urllib3/response.py\", line 561, in read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    data = self._fp_read(amt) if not fp_closed else b\"\"\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  File \"/opt/conda/lib/python3.9/site-packages/pip/_vendor/urllib3/response.py\", line 527, in _fp_read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    return self._fp.read(amt) if amt is not None else self._fp.read()\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  File \"/opt/conda/lib/python3.9/site-packages/pip/_vendor/cachecontrol/filewrapper.py\", line 90, in read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    data = self.__fp.read(amt)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  File \"/opt/conda/lib/python3.9/http/client.py\", line 463, in read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    n = self.readinto(b)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  File \"/opt/conda/lib/python3.9/http/client.py\", line 507, in readinto\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    n = self.fp.readinto(b)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  File \"/opt/conda/lib/python3.9/socket.py\", line 704, in readinto\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    return self._sock.recv_into(b)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  File \"/opt/conda/lib/python3.9/ssl.py\", line 1242, in recv_into\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    return self.read(nbytes, buffer)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  File \"/opt/conda/lib/python3.9/ssl.py\", line 1100, in read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    return self._sslobj.read(len, buffer)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:ConnectionResetError: [Errno 104] Connection reset by peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:During handling of the above exception, another exception occurred:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/cli/base_command.py\", line 160, in exc_logging_wrapper\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    status = run_func(*args)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/cli/req_command.py\", line 247, in wrapper\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    return func(self, options, args)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/commands/install.py\", line 415, in run\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    requirement_set = resolver.resolve(\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 92, in resolve\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    result = self._result = resolver.resolve(\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  File \"/opt/conda/lib/python3.9/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 481, in resolve\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    state = resolution.resolve(requirements, max_rounds=max_rounds)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  File \"/opt/conda/lib/python3.9/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 373, in resolve\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    failure_causes = self._attempt_to_pin_criterion(name)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  File \"/opt/conda/lib/python3.9/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 213, in _attempt_to_pin_criterion\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    criteria = self._get_updated_criteria(candidate)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  File \"/opt/conda/lib/python3.9/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 204, in _get_updated_criteria\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    self._add_to_criteria(criteria, requirement, parent=candidate)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  File \"/opt/conda/lib/python3.9/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 172, in _add_to_criteria\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    if not criterion.candidates:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  File \"/opt/conda/lib/python3.9/site-packages/pip/_vendor/resolvelib/structs.py\", line 151, in __bool__\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    return bool(self._sequence)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 155, in __bool__\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    return any(self)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 143, in <genexpr>\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    return (c for c in iterator if id(c) not in self._incompatible_ids)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 47, in _iter_built\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    candidate = func()\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/factory.py\", line 206, in _make_candidate_from_link\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    self._link_candidate_cache[link] = LinkCandidate(\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 297, in __init__\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    super().__init__(\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 162, in __init__\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    self.dist = self._prepare()\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 231, in _prepare\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    dist = self._prepare_distribution()\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/resolution/r[1,mpirank:4,algo-1]<stderr>:esolvelib/candidates.py\", line 308, in _prepare_distribution\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/operations/prepare.py\", line 491, in prepare_linked_requirement\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    return self._prepare_linked_requirement(req, parallel_builds)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/operations/prepare.py\", line 536, in _prepare_linked_requirement\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    local_file = unpack_url(\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/operations/prepare.py\", line 166, in unpack_url\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    file = get_http_url(\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/operations/prepare.py\", line 107, in get_http_url\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    from_path, content_type = download(link, temp_dir.path)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/network/download.py\", line 147, in __call__\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    for chunk in chunks:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/cli/progress_bars.py\", line 53, in _rich_progress_bar\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    for chunk in iterable:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/network/utils.py\", line 63, in response_chunks\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    for chunk in response.raw.stream(\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  File \"/opt/conda/lib/python3.9/site-packages/pip/_vendor/urllib3/response.py\", line 622, in stream\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    data = self.read(amt=amt, decode_content=decode_content)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  File \"/opt/conda/lib/python3.9/site-packages/pip/_vendor/urllib3/response.py\", line 587, in read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  File \"/opt/conda/lib/python3.9/contextlib.py\", line 137, in __exit__\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    self.gen.throw(typ, value, traceback)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  File \"/opt/conda/lib/python3.9/site-packages/pip/_vendor/urllib3/response.py\", line 455, in _error_catcher\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    raise ProtocolError(\"Connection broken: %r\" % e, e)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:pip._vendor.urllib3.exceptions.ProtocolError: (\"Connection broken: ConnectionResetError(104, 'Connection reset by peer')\", ConnectionResetError(104, 'Connection reset by peer'))\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Collecting absl-py\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:#015#033[2K     #033[90m#033[0m #033[32m1.5/1.5 MB#033[0m #033[31m71.5 MB/s#033[0m eta #033[36m0:00:00#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:#015#033[2K     #033[90m#033[0m #033[32m1.5/1.5 MB#033[0m #033[31m66.2 MB/s#033[0m eta #033[36m0:00:00#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.23.5)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.16.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.23.5)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.16.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:#033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:#033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Collecting nltk\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Collecting nltk\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:     #033[90m#033[0m #033[32m0.0/1.5 MB#033[0m #033[31m?#033[0m eta #033[36m-:--:--#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:     #033[90m#033[0m #033[32m0.0/1.5 MB#033[0m #033[31m?#033[0m eta #033[36m-:--:--#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (4.64.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (8.1.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (2022.10.31)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (1.2.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (2022.10.31)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (4.64.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (1.2.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (8.1.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:#010 #010done\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Building wheels for collected packages: rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.23.5)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.23.5)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  Building wheel for rouge_score (setup.py) ... #033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.16.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.16.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Building wheels for collected packages: rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  Building wheel for rouge_score (setup.py) ... #033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:#015#033[2K     #033[91m#033[0m#033[91m#033[0m #033[32m1.5/1.5 MB#033[0m #033[31m45.9 MB/s#033[0m eta #033[36m0:00:01#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:#015#033[2K     #033[91m#033[0m#033[91m#033[0m #033[32m1.5/1.5 MB#033[0m #033[31m45.5 MB/s#033[0m eta #033[36m0:00:01#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (8.1.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (4.64.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (1.2.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (4.64.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (2022.10.31)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (2022.10.31)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (1.2.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (8.1.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:#015#033[2K     #033[90m#033[0m #033[32m1.5/1.5 MB#033[0m #033[31m32.6 MB/s#033[0m eta #033[36m0:00:00#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:#033[?25h[1,mpirank:0,algo-1]<stdout>:Collecting absl-py\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:#015#033[2K     #033[90m#033[0m #033[32m1.5/1.5 MB#033[0m #033[31m32.2 MB/s#033[0m eta #033[36m0:00:00#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.23.5)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.23.5)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.16.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.16.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Building wheels for collected packages: rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Building wheels for collected packages: rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  Building wheel for rouge_score (setup.py) ... #033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  Building wheel for rouge_score (setup.py) ... #033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (1.2.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (2022.10.31)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (4.64.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (4.64.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (8.1.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (1.2.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (2022.10.31)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (8.1.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Collecting nltk\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Building wheels for collected packages: rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Building wheels for collected packages: rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  Building wheel for rouge_score (setup.py) ... #033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  Building wheel for rouge_score (setup.py) ... #033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.23.5)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.16.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (4.64.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (1.2.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (8.1.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (2022.10.31)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Building wheels for collected packages: rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  Building wheel for rouge_score (setup.py) ... #033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:-\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:-\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:-\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:-\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:-\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:-\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:#010 #010\\\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:-\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:#010 #010\\\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:#010 #010\\\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:#010 #010\\\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Collecting loralib\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:#010 #010\\\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:#010 #010\\\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  Downloading loralib-0.1.1-py3-none-any.whl (8.8 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:#010 #010\\\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:#010 #010done\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=57a07db33006f163c43c92691e69b41b46c74811143e2edb9766fbc787a9e5f6\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  Stored in directory: /root/.cache/pip/wheels/9b/3d/39/09558097d3119ca0a4d462df68f22c6f3c1b345ac63a09b86e\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Successfully built rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:#010 #010|\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:#010 #010done\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:#010 #010|\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=47f091e01f98ea53f324cc5e0901d3a544601241bde6653243967527c12fbb8a\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  Stored in directory: /root/.cache/pip/wheels/9b/3d/39/09558097d3119ca0a4d462df68f22c6f3c1b345ac63a09b86e\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Successfully built rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:#010 #010done\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=b6ffd9b6d01a34daf1250f4811300e832d635d542e310d613b722edc4bb855be\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  Stored in directory: /root/.cache/pip/wheels/9b/3d/39/09558097d3119ca0a4d462df68f22c6f3c1b345ac63a09b86e\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Successfully built rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:#010 #010|\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:#010 #010done\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=b6ffd9b6d01a34daf1250f4811300e832d635d542e310d613b722edc4bb855be\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  Stored in directory: /root/.cache/pip/wheels/9b/3d/39/09558097d3119ca0a4d462df68f22c6f3c1b345ac63a09b86e\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Successfully built rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:#010 #010done\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=b6ffd9b6d01a34daf1250f4811300e832d635d542e310d613b722edc4bb855be\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  Stored in directory: /root/.cache/pip/wheels/9b/3d/39/09558097d3119ca0a4d462df68f22c6f3c1b345ac63a09b86e\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Successfully built rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:#010 #010|\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:#010 #010done\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=b6ffd9b6d01a34daf1250f4811300e832d635d542e310d613b722edc4bb855be\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  Stored in directory: /root/.cache/pip/wheels/9b/3d/39/09558097d3119ca0a4d462df68f22c6f3c1b345ac63a09b86e\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Successfully built rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:#010 #010done\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=b6ffd9b6d01a34daf1250f4811300e832d635d542e310d613b722edc4bb855be\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  Stored in directory: /root/.cache/pip/wheels/9b/3d/39/09558097d3119ca0a4d462df68f22c6f3c1b345ac63a09b86e\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Successfully built rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Installing collected packages: loralib\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Successfully installed loralib-0.1.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Installing collected packages: nltk, absl-py, rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Installing collected packages: nltk, absl-py, rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Installing collected packages: nltk, absl-py, rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Installing collected packages: nltk, absl-py, rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Installing collected packages: nltk, absl-py, rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Installing collected packages: nltk, absl-py, rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/opt/conda/lib/python3.9/site-packages/nltk/app/rdparser_app.py'\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Installing collected packages: nltk, absl-py, rouge_score\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Collecting peft\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  Downloading peft-0.4.0-py3-none-any.whl (72 kB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:#033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:     #033[90m#033[0m #033[32m0.0/72.9 kB#033[0m #033[31m?#033[0m eta #033[36m-:--:--#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:#015#033[2K     #033[90m#033[0m #033[32m72.9/72.9 kB#033[0m #033[31m3.6 MB/s#033[0m eta #033[36m0:00:00#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from peft) (5.4.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: accelerate in /opt/conda/lib/python3.9/site-packages (from peft) (0.16.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from peft) (1.23.5)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: transformers in /opt/conda/lib/python3.9/site-packages (from peft) (4.26.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.9/site-packages (from peft) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from peft) (23.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from peft) (5.9.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: loralib in /opt/conda/lib/python3.9/site-packages (0.1.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Collecting safetensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:  Downloading safetensors-0.3.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:#033[?25l\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:     #033[90m#033[0m #033[32m0.0/1.3 MB#033[0m #033[31m?#033[0m eta #033[36m-:--:--#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:#015#033[2K     #033[91m#033[0m#033[91m#033[0m #033[32m1.3/1.3 MB#033[0m #033[31m51.0 MB/s#033[0m eta #033[36m0:00:01#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:#015#033[2K     #033[90m#033[0m #033[32m1.3/1.3 MB#033[0m #033[31m34.2 MB/s#033[0m eta #033[36m0:00:00#033[0m\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:#033[?25h\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.13.0->peft) (4.4.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Successfully installed absl-py-1.4.0 nltk-3.8.1 rouge_score-0.1.2\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Successfully installed absl-py-1.4.0 nltk-3.8.1 rouge_score-0.1.2\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Successfully installed absl-py-1.4.0 nltk-3.8.1 rouge_score-0.1.2\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Successfully installed absl-py-1.4.0 nltk-3.8.1 rouge_score-0.1.2\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Successfully installed absl-py-1.4.0 nltk-3.8.1 rouge_score-0.1.2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Successfully installed absl-py-1.4.0 nltk-3.8.1 rouge_score-0.1.2\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (3.9.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (2022.10.31)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (4.64.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (2.28.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (0.12.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (0.13.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft) (2022.12.7)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft) (1.26.14)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft) (3.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft) (2.1.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: loralib in /opt/conda/lib/python3.9/site-packages (0.1.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: loralib in /opt/conda/lib/python3.9/site-packages (0.1.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: loralib in /opt/conda/lib/python3.9/site-packages (0.1.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: loralib in /opt/conda/lib/python3.9/site-packages (0.1.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: loralib in /opt/conda/lib/python3.9/site-packages (0.1.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: loralib in /opt/conda/lib/python3.9/site-packages (0.1.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Installing collected packages: safetensors, peft\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Successfully installed peft-0.4.0 safetensors-0.3.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: peft in /opt/conda/lib/python3.9/site-packages (0.4.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.9/site-packages (from peft) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from peft) (5.4.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from peft) (5.9.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: transformers in /opt/conda/lib/python3.9/site-packages (from peft) (4.26.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from peft) (1.23.5)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: safetensors in /opt/conda/lib/python3.9/site-packages (from peft) (0.3.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: accelerate in /opt/conda/lib/python3.9/site-packages (from peft) (0.16.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from peft) (23.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.13.0->peft) (4.4.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (2022.10.31)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (4.64.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (0.12.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (3.9.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (2.28.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (0.13.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft) (2.1.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft) (2022.12.7)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft) (1.26.14)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft) (3.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: peft in /opt/conda/lib/python3.9/site-packages (0.4.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from peft) (5.9.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from peft) (1.23.5)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from peft) (5.4.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.9/site-packages (from peft) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: transformers in /opt/conda/lib/python3.9/site-packages (from peft) (4.26.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: safetensors in /opt/conda/lib/python3.9/site-packages (from peft) (0.3.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from peft) (23.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: accelerate in /opt/conda/lib/python3.9/site-packages (from peft) (0.16.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.13.0->peft) (4.4.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: peft in /opt/conda/lib/python3.9/site-packages (0.4.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: safetensors in /opt/conda/lib/python3.9/site-packages (from peft) (0.3.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.9/site-packages (from peft) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from peft) (5.4.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: accelerate in /opt/conda/lib/python3.9/site-packages (from peft) (0.16.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from peft) (5.9.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from peft) (23.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from peft) (1.23.5)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: transformers in /opt/conda/lib/python3.9/site-packages (from peft) (4.26.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.13.0->peft) (4.4.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: peft in /opt/conda/lib/python3.9/site-packages (0.4.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: safetensors in /opt/conda/lib/python3.9/site-packages (from peft) (0.3.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: accelerate in /opt/conda/lib/python3.9/site-packages (from peft) (0.16.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from peft) (5.9.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: transformers in /opt/conda/lib/python3.9/site-packages (from peft) (4.26.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.9/site-packages (from peft) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from peft) (1.23.5)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from peft) (23.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from peft) (5.4.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.13.0->peft) (4.4.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: peft in /opt/conda/lib/python3.9/site-packages (0.4.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from peft) (23.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: safetensors in /opt/conda/lib/python3.9/site-packages (from peft) (0.3.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from peft) (1.23.5)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: transformers in /opt/conda/lib/python3.9/site-packages (from peft) (4.26.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from peft) (5.4.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: accelerate in /opt/conda/lib/python3.9/site-packages (from peft) (0.16.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.9/site-packages (from peft) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from peft) (5.9.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.13.0->peft) (4.4.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: peft in /opt/conda/lib/python3.9/site-packages (0.4.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from peft) (5.9.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from peft) (1.23.5)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: safetensors in /opt/conda/lib/python3.9/site-packages (from peft) (0.3.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from peft) (5.4.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: transformers in /opt/conda/lib/python3.9/site-packages (from peft) (4.26.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.9/site-packages (from peft) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from peft) (23.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: accelerate in /opt/conda/lib/python3.9/site-packages (from peft) (0.16.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: peft in /opt/conda/lib/python3.9/site-packages (0.4.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.13.0->peft) (4.4.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.9/site-packages (from peft) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from peft) (23.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from peft) (5.4.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from peft) (1.23.5)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: safetensors in /opt/conda/lib/python3.9/site-packages (from peft) (0.3.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from peft) (5.9.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: transformers in /opt/conda/lib/python3.9/site-packages (from peft) (4.26.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: accelerate in /opt/conda/lib/python3.9/site-packages (from peft) (0.16.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.13.0->peft) (4.4.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (0.12.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (0.13.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (3.9.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (4.64.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (2022.10.31)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (2.28.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (3.9.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (0.13.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (4.64.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (2022.10.31)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (0.12.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (2.28.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft) (3.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft) (1.26.14)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft) (2.1.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft) (2022.12.7)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (3.9.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (0.13.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (2.28.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (2022.10.31)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (4.64.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (0.12.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft) (3.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft) (1.26.14)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft) (2022.12.7)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft) (2.1.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (2022.10.31)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (2.28.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (3.9.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (4.64.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (0.12.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (0.13.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft) (1.26.14)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft) (3.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft) (2.1.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft) (2022.12.7)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (3.9.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (0.12.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (2022.10.31)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (4.64.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (2.28.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (0.12.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (0.13.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (2.28.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (4.64.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (3.9.0)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (0.13.2)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (2022.10.31)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft) (3.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft) (1.26.14)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft) (2.1.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft) (2022.12.7)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft) (2.1.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft) (1.26.14)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft) (3.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft) (2022.12.7)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft) (2.1.1)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft) (3.4)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft) (2022.12.7)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->peft) (1.26.14)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:args.output_data_dir /opt/ml/output/data\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:args.model_dir /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:args.n_gpus 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:args.training_dir /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:args.test_dir /opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:2023-07-25 15:46:37,433 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:2023-07-25 15:46:37,433 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading ()lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading ()lve/main/config.json: 100%|| 483/483 [00:00<00:00, 63.4kB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading ()\"model.safetensors\";:   0%|          | 0.00/268M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading ()\"model.safetensors\";:   8%|         | 21.0M/268M [00:00<00:01, 130MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading ()\"model.safetensors\";:  16%|        | 41.9M/268M [00:00<00:01, 152MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading ()\"model.safetensors\";:  27%|       | 73.4M/268M [00:00<00:01, 190MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading ()\"model.safetensors\";:  39%|      | 105M/268M [00:00<00:00, 193MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading ()\"model.safetensors\";:  47%|     | 126M/268M [00:00<00:00, 187MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading ()\"model.safetensors\";:  59%|    | 157M/268M [00:00<00:00, 216MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading ()\"model.safetensors\";:  70%|   | 189M/268M [00:00<00:00, 241MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading ()\"model.safetensors\";:  82%| | 220M/268M [00:01<00:00, 257MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading ()\"model.safetensors\";:  94%|| 252M/268M [00:01<00:00, 247MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading ()\"model.safetensors\";: 100%|| 268M/268M [00:01<00:00, 210MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:ORIGINAL model\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:trainable params: 66955010 || all params: 66955010 || trainable%: 100.00\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:PEFT model BEFORE WRAP\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:trainable params: 1825540 || all params: 68136964 || trainable%: 2.68\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.0.attention.q_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.0.attention.q_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:name [1,mpirank:4,algo-1]<stdout>:base_model.model.distilbert.transformer.layer.0.attention.v_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:name [1,mpirank:4,algo-1]<stdout>:base_model.model.distilbert.transformer.layer.0.attention.v_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.1.attention.q_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:name [1,mpirank:4,algo-1]<stdout>:base_model.model.distilbert.transformer.layer.1.attention.q_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.1.attention.v_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:name\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:base_model.model.distilbert.transformer.layer.1.attention.v_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.2.attention.q_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.2.attention.q_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.2.attention.v_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.2.attention.v_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.3.attention.q_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.3.attention.q_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.3.attention.v_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.3.attention.v_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.4.attention.q_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:name [1,mpirank:4,algo-1]<stdout>:base_model.model.distilbert.transformer.layer.4.attention.q_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:name [1,mpirank:4,algo-1]<stdout>:base_model.model.distilbert.transformer.layer.4.attention.v_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.4.attention.v_lin.lora_B.default.weight[1,mpirank:4,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.5.attention.q_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.5.attention.q_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.5.attention.v_lin.lora_A.default.weight[1,mpirank:4,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.5.attention.v_lin.lora_B.default.weight[1,mpirank:4,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:PEFT model AFTER WRAP\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:trainable params: 1825540 || all params: 68136964 || trainable%: 2.68\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading ()okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading ()okenizer_config.json: 100%|| 28.0/28.0 [00:00<00:00, 2.21kB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading ()solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading ()solve/main/vocab.txt: 100%|| 232k/232k [00:00<00:00, 37.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:args.output_data_dir /opt/ml/output/data\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:args.model_dir /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:args.n_gpus 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:args.training_dir /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:args.test_dir /opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:2023-07-25 15:46:40,685 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:2023-07-25 15:46:40,685 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading ()/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading ()/main/tokenizer.json: 100%|| 466k/466k [00:00<00:00, 43.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:ORIGINAL model\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:trainable params: 66955010 || all params: 66955010 || trainable%: 100.00\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:args.output_data_dir /opt/ml/output/data\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:args.model_dir /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:args.n_gpus 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:args.training_dir /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:args.test_dir /opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:PEFT model BEFORE WRAP\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:trainable params: 1825540 || all params: 68136964 || trainable%: 2.68\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.0.attention.q_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.0.attention.q_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.0.attention.v_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.0.attention.v_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.1.attention.q_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.1.attention.q_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.1.attention.v_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.1.attention.v_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.2.attention.q_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.2.attention.q_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:name [1,mpirank:6,algo-1]<stdout>:base_model.model.distilbert.transformer.layer.2.attention.v_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:name[1,mpirank:6,algo-1]<stdout>: base_model.model.distilbert.transformer.layer.2.attention.v_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.3.attention.q_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.3.attention.q_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:name [1,mpirank:6,algo-1]<stdout>:base_model.model.distilbert.transformer.layer.3.attention.v_lin.lora_A.default.weight[1,mpirank:6,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:name [1,mpirank:6,algo-1]<stdout>:base_model.model.distilbert.transformer.layer.3.attention.v_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.4.attention.q_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.4.attention.q_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:name[1,mpirank:6,algo-1]<stdout>: base_model.model.distilbert.transformer.layer.4.attention.v_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.4.attention.v_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.5.attention.q_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.5.attention.q_lin.lora_B.default.weight[1,mpirank:6,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:name [1,mpirank:6,algo-1]<stdout>:base_model.model.distilbert.transformer.layer.5.attention.v_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:name [1,mpirank:6,algo-1]<stdout>:base_model.model.distilbert.transformer.layer.5.attention.v_lin.lora_B.default.weight[1,mpirank:6,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:PEFT model AFTER WRAP\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:trainable params: 1825540 || all params: 68136964 || trainable%: 2.68\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:2023-07-25 15:46:41,668 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:2023-07-25 15:46:41,668 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:args.output_data_dir /opt/ml/output/data\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:args.model_dir /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:args.n_gpus 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:args.training_dir /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:args.test_dir /opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:2023-07-25 15:46:41,681 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:2023-07-25 15:46:41,681 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:args.output_data_dir /opt/ml/output/data\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:args.model_dir /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:args.n_gpus 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:args.training_dir /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:args.test_dir /opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:2023-07-25 15:46:41,693 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:2023-07-25 15:46:41,693 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:args.output_data_dir /opt/ml/output/data\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:args.model_dir /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:args.n_gpus 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:args.training_dir /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:args.test_dir /opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:2023-07-25 15:46:41,817 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:2023-07-25 15:46:41,817 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:args.output_data_dir /opt/ml/output/data\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:args.model_dir /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:args.n_gpus 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:args.training_dir /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:args.test_dir /opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:2023-07-25 15:46:41,945 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:2023-07-25 15:46:41,945 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:args.output_data_dir /opt/ml/output/data\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:args.model_dir /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:args.n_gpus 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:args.training_dir /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:args.test_dir /opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:2023-07-25 15:46:42,031 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:2023-07-25 15:46:42,031 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight', 'classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight', 'classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:ORIGINAL model\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:trainable params: 66955010 || all params: 66955010 || trainable%: 100.00\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:ORIGINAL model\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:trainable params: 66955010 || all params: 66955010 || trainable%: 100.00\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:ORIGINAL model\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:trainable params: 66955010 || all params: 66955010 || trainable%: 100.00\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:PEFT model BEFORE WRAP\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:trainable params: 1825540 || all params: 68136964 || trainable%: 2.68\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.0.attention.q_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.0.attention.q_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.0.attention.v_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.0.attention.v_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.1.attention.q_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.1.attention.q_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.1.attention.v_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:name [1,mpirank:0,algo-1]<stdout>:base_model.model.distilbert.transformer.layer.1.attention.v_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.2.attention.q_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.2.attention.q_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.2.attention.v_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.2.attention.v_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.3.attention.q_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.3.attention.q_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.3.attention.v_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.3.attention.v_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:name [1,mpirank:0,algo-1]<stdout>:base_model.model.distilbert.transformer.layer.4.attention.q_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.4.attention.q_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.4.attention.v_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.4.attention.v_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.5.attention.q_lin.lora_A.default.weight[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.5.attention.q_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.5.attention.v_lin.lora_A.default.weight[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:name [1,mpirank:0,algo-1]<stdout>:base_model.model.distilbert.transformer.layer.5.attention.v_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:PEFT model AFTER WRAP\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:trainable params: 1825540 || all params: 68136964 || trainable%: 2.68\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:PEFT model BEFORE WRAP\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:trainable params: 1825540 || all params: 68136964 || trainable%: 2.68\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.0.attention.q_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.0.attention.q_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.0.attention.v_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.0.attention.v_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.1.attention.q_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.1.attention.q_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.1.attention.v_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.1.attention.v_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.2.attention.q_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.2.attention.q_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.2.attention.v_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.2.attention.v_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.3.attention.q_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.3.attention.q_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.3.attention.v_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:name[1,mpirank:2,algo-1]<stdout>: base_model.model.distilbert.transformer.layer.3.attention.v_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.4.attention.q_lin.lora_A.default.weight[1,mpirank:3,algo-1]<stdout>:PEFT model BEFORE WRAP\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.4.attention.q_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.4.attention.v_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.4.attention.v_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.5.attention.q_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.5.attention.q_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.5.attention.v_lin.lora_A.default.weight[1,mpirank:2,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:name [1,mpirank:2,algo-1]<stdout>:base_model.model.distilbert.transformer.layer.5.attention.v_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:PEFT model AFTER WRAP\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:trainable params: 1825540 || all params: 68136964 || trainable%: 2.68\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.0.attention.q_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.0.attention.q_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:name [1,mpirank:3,algo-1]<stdout>:base_model.model.distilbert.transformer.layer.0.attention.v_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:name [1,mpirank:3,algo-1]<stdout>:base_model.model.distilbert.transformer.layer.0.attention.v_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:trainable params: 1825540 || all params: 68136964 || trainable%: 2.68\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.1.attention.q_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:name [1,mpirank:3,algo-1]<stdout>:base_model.model.distilbert.transformer.layer.1.attention.q_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.1.attention.v_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.1.attention.v_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.2.attention.q_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:name\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>: base_model.model.distilbert.transformer.layer.2.attention.q_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.2.attention.v_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:name [1,mpirank:3,algo-1]<stdout>:base_model.model.distilbert.transformer.layer.2.attention.v_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.3.attention.q_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.3.attention.q_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.3.attention.v_lin.lora_A.default.weight[1,mpirank:3,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.3.attention.v_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.4.attention.q_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.4.attention.q_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.4.attention.v_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.4.attention.v_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.5.attention.q_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.5.attention.q_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.5.attention.v_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:name [1,mpirank:3,algo-1]<stdout>:base_model.model.distilbert.transformer.layer.5.attention.v_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:PEFT model AFTER WRAP\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:ORIGINAL model\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:trainable params: 1825540 || all params: 68136964 || trainable%: 2.68\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:trainable params: 66955010 || all params: 66955010 || trainable%: 100.00\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:ORIGINAL model\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:trainable params: 66955010 || all params: 66955010 || trainable%: 100.00\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:PEFT model BEFORE WRAP\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:trainable params: 1825540 || all params: 68136964 || trainable%: 2.68\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.0.attention.q_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.0.attention.q_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.0.attention.v_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.0.attention.v_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.1.attention.q_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.1.attention.q_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.1.attention.v_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.1.attention.v_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.2.attention.q_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.2.attention.q_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:name [1,mpirank:1,algo-1]<stdout>:base_model.model.distilbert.transformer.layer.2.attention.v_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.2.attention.v_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.3.attention.q_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.3.attention.q_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.3.attention.v_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:name [1,mpirank:1,algo-1]<stdout>:base_model.model.distilbert.transformer.layer.3.attention.v_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.4.attention.q_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:name [1,mpirank:1,algo-1]<stdout>:base_model.model.distilbert.transformer.layer.4.attention.q_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.4.attention.v_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:name [1,mpirank:1,algo-1]<stdout>:base_model.model.distilbert.transformer.layer.4.attention.v_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.5.attention.q_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.5.attention.q_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.5.attention.v_lin.lora_A.default.weight[1,mpirank:1,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.5.attention.v_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:PEFT model AFTER WRAP\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:trainable params: 1825540 || all params: 68136964 || trainable%: 2.68\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight', 'classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight', 'classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:ORIGINAL model\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:trainable params: 66955010 || all params: 66955010 || trainable%: 100.00\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:PEFT model BEFORE WRAP\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:trainable params: 1825540 || all params: 68136964 || trainable%: 2.68\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.0.attention.q_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:name [1,mpirank:5,algo-1]<stdout>:base_model.model.distilbert.transformer.layer.0.attention.q_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.0.attention.v_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.0.attention.v_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.1.attention.q_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.1.attention.q_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.1.attention.v_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.1.attention.v_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.2.attention.q_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.2.attention.q_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.2.attention.v_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.2.attention.v_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.3.attention.q_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.3.attention.q_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.3.attention.v_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.3.attention.v_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.4.attention.q_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.4.attention.q_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.4.attention.v_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.4.attention.v_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:name [1,mpirank:5,algo-1]<stdout>:base_model.model.distilbert.transformer.layer.5.attention.q_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:name [1,mpirank:5,algo-1]<stdout>:base_model.model.distilbert.transformer.layer.5.attention.q_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.5.attention.v_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.5.attention.v_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:PEFT model AFTER WRAP\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:trainable params: 1825540 || all params: 68136964 || trainable%: 2.68\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:PEFT model BEFORE WRAP\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:trainable params: 1825540 || all params: 68136964 || trainable%: 2.68\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.0.attention.q_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.0.attention.q_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.0.attention.v_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.0.attention.v_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.1.attention.q_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.1.attention.q_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.1.attention.v_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.1.attention.v_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.2.attention.q_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.2.attention.q_lin.lora_B.default.weight[1,mpirank:7,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.2.attention.v_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.2.attention.v_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.3.attention.q_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:name [1,mpirank:7,algo-1]<stdout>:base_model.model.distilbert.transformer.layer.3.attention.q_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.3.attention.v_lin.lora_A.default.weight[1,mpirank:7,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:name [1,mpirank:7,algo-1]<stdout>:base_model.model.distilbert.transformer.layer.3.attention.v_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.4.attention.q_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.4.attention.q_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.4.attention.v_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.4.attention.v_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.5.attention.q_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:name base_model.model.distilbert.transformer.layer.5.attention.q_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:name [1,mpirank:7,algo-1]<stdout>:base_model.model.distilbert.transformer.layer.5.attention.v_lin.lora_A.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:name[1,mpirank:7,algo-1]<stdout>: base_model.model.distilbert.transformer.layer.5.attention.v_lin.lora_B.default.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:PEFT model AFTER WRAP\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:trainable params: 1825540 || all params: 68136964 || trainable%: 2.68\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:178:178 [0] NCCL INFO Bootstrap : Using eth0:10.2.122.108<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:178:178 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:178:178 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:178:178 [0] NCCL INFO cudaDriverVersion 12000\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:NCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:137:137 [3] NCCL INFO cudaDriverVersion 12000\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:145:145 [6] NCCL INFO cudaDriverVersion 12000\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:133:133 [2] NCCL INFO cudaDriverVersion 12000\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:146:146 [7] NCCL INFO cudaDriverVersion 12000\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:130:130 [1] NCCL INFO cudaDriverVersion 12000\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:141:141 [4] NCCL INFO cudaDriverVersion 12000\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:143:143 [5] NCCL INFO cudaDriverVersion 12000\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:178:178 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:178:178 [0] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:178:178 [0] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:178:178 [0] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:178:178 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:178:178 [0] NCCL INFO NET/Socket : Using [0]eth0:10.2.122.108<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:178:178 [0] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:141:141 [4] NCCL INFO Bootstrap : Using eth0:10.2.122.108<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:141:141 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:141:141 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:141:141 [4] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:141:141 [4] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:146:146 [7] NCCL INFO Bootstrap : Using eth0:10.2.122.108<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:146:146 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:146:146 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:141:141 [4] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:141:141 [4] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:141:141 [4] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:146:146 [7] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:141:141 [4] NCCL INFO NET/Socket : Using [0]eth0:10.2.122.108<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:141:141 [4] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:146:146 [7] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:146:146 [7] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:146:146 [7] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:146:146 [7] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:146:146 [7] NCCL INFO NET/Socket : Using [0]eth0:10.2.122.108<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:146:146 [7] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:143:143 [5] NCCL INFO Bootstrap : Using eth0:10.2.122.108<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:143:143 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:143:143 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:143:143 [5] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:143:143 [5] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:137:137 [3] NCCL INFO Bootstrap : Using eth0:10.2.122.108<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:145:145 [6] NCCL INFO Bootstrap : Using eth0:10.2.122.108<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:137:137 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:137:137 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:145:145 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:145:145 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:137:137 [3] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:137:137 [3] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:145:145 [6] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:145:145 [6] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:143:143 [5] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:143:143 [5] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:143:143 [5] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:143:143 [5] NCCL INFO NET/Socket : Using [0]eth0:10.2.122.108<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:143:143 [5] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:137:137 [3] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:137:137 [3] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:137:137 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:145:145 [6] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:145:145 [6] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:145:145 [6] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:137:137 [3] NCCL INFO NET/Socket : Using [0]eth0:10.2.122.108<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:137:137 [3] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:145:145 [6] NCCL INFO NET/Socket : Using [0]eth0:10.2.122.108<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:145:145 [6] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:133:133 [2] NCCL INFO Bootstrap : Using eth0:10.2.122.108<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:130:130 [1] NCCL INFO Bootstrap : Using eth0:10.2.122.108<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:133:133 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:133:133 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:130:130 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:130:130 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:133:133 [2] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:133:133 [2] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:130:130 [1] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:130:130 [1] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:133:133 [2] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:133:133 [2] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:133:133 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:133:133 [2] NCCL INFO NET/Socket : Using [0]eth0:10.2.122.108<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:133:133 [2] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:130:130 [1] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:130:130 [1] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:130:130 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:130:130 [1] NCCL INFO NET/Socket : Using [0]eth0:10.2.122.108<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:130:130 [1] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:178:178 [0] NCCL INFO Channel 00/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:178:178 [0] NCCL INFO Channel 01/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:178:178 [0] NCCL INFO Channel 02/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:178:178 [0] NCCL INFO Channel 03/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:178:178 [0] NCCL INFO Channel 04/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:178:178 [0] NCCL INFO Channel 05/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:178:178 [0] NCCL INFO Channel 06/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:178:178 [0] NCCL INFO Channel 07/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:178:178 [0] NCCL INFO Channel 08/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:178:178 [0] NCCL INFO Channel 09/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:178:178 [0] NCCL INFO Channel 10/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:178:178 [0] NCCL INFO Channel 11/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:178:178 [0] NCCL INFO Trees [0] 3/-1/-1->0->-1 [1] 3/-1/-1->0->-1 [2] 4/-1/-1->0->-1 [3] 4/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 2/-1/-1->0->-1 [6] 3/-1/-1->0->-1 [7] 3/-1/-1->0->-1 [8] 4/-1/-1->0->-1 [9] 4/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 2/-1/-1->0->-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:146:146 [7] NCCL INFO Trees [0] 4/-1/-1->7->6 [1] 4/-1/-1->7->6 [2] 6/-1/-1->7->4 [3] 6/-1/-1->7->4 [4] 5/-1/-1->7->3 [5] 3/-1/-1->7->5 [6] 4/-1/-1->7->6 [7] 4/-1/-1->7->6 [8] 6/-1/-1->7->4 [9] 6/-1/-1->7->4 [10] 5/-1/-1->7->3 [11] 3/-1/-1->7->5\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:137:137 [3] NCCL INFO Trees [0] 2/-1/-1->3->0 [1] 2/-1/-1->3->0 [2] -1/-1/-1->3->2 [3] -1/-1/-1->3->2 [4] 7/-1/-1->3->1 [5] 1/-1/-1->3->7 [6] 2/-1/-1->3->0 [7] 2/-1/-1->3->0 [8] -1/-1/-1->3->2 [9] -1/-1/-1->3->2 [10] 7/-1/-1->3->1 [11] 1/-1/-1->3->7\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:143:143 [5] NCCL INFO Trees [0] 6/-1/-1->5->1 [1] 6/-1/-1->5->1 [2] 1/-1/-1->5->6 [3] 1/-1/-1->5->6 [4] 4/-1/-1->5->7 [5] 7/-1/-1->5->4 [6] 6/-1/-1->5->1 [7] 6/-1/-1->5->1 [8] 1/-1/-1->5->6 [9] 1/-1/-1->5->6 [10] 4/-1/-1->5->7 [11] 7/-1/-1->5->4\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:145:145 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 5/-1/-1->6->7 [3] 5/-1/-1->6->7 [4] 2/-1/-1->6->4 [5] 4/-1/-1->6->2 [6] 7/-1/-1->6->5 [7] 7/-1/-1->6->5 [8] 5/-1/-1->6->7 [9] 5/-1/-1->6->7 [10] 2/-1/-1->6->4 [11] 4/-1/-1->6->2\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:141:141 [4] NCCL INFO Trees [0] -1/-1/-1->4->7 [1] -1/-1/-1->4->7 [2] 7/-1/-1->4->0 [3] 7/-1/-1->4->0 [4] 6/-1/-1->4->5 [5] 5/-1/-1->4->6 [6] -1/-1/-1->4->7 [7] -1/-1/-1->4->7 [8] 7/-1/-1->4->0 [9] 7/-1/-1->4->0 [10] 6/-1/-1->4->5 [11] 5/-1/-1->4->6\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:133:133 [2] NCCL INFO Trees [0] 1/-1/-1->2->3 [1] 1/-1/-1->2->3 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] -1/-1/-1->2->6 [5] 6/-1/-1->2->0 [6] 1/-1/-1->2->3 [7] 1/-1/-1->2->3 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] -1/-1/-1->2->6 [11] 6/-1/-1->2->0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:130:130 [1] NCCL INFO Trees [0] 5/-1/-1->1->2 [1] 5/-1/-1->1->2 [2] 2/-1/-1->1->5 [3] 2/-1/-1->1->5 [4] 3/-1/-1->1->0 [5] -1/-1/-1->1->3 [6] 5/-1/-1->1->2 [7] 5/-1/-1->1->2 [8] 2/-1/-1->1->5 [9] 2/-1/-1->1->5 [10] 3/-1/-1->1->0 [11] -1/-1/-1->1->3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:178:178 [0] NCCL INFO Channel 04/0 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:141:141 [4] NCCL INFO Channel 05/0 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:141:141 [4] NCCL INFO Channel 11/0 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:178:178 [0] NCCL INFO Channel 10/0 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:130:130 [1] NCCL INFO Channel 02/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:143:143 [5] NCCL INFO Channel 00/0 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:130:130 [1] NCCL INFO Channel 03/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:143:143 [5] NCCL INFO Channel 01/0 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:130:130 [1] NCCL INFO Channel 08/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:145:145 [6] NCCL INFO Channel 00/0 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:143:143 [5] NCCL INFO Channel 06/0 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:133:133 [2] NCCL INFO Channel 02/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:130:130 [1] NCCL INFO Channel 09/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:143:143 [5] NCCL INFO Channel 07/0 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:145:145 [6] NCCL INFO Channel 01/0 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:133:133 [2] NCCL INFO Channel 03/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:145:145 [6] NCCL INFO Channel 06/0 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:133:133 [2] NCCL INFO Channel 08/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:145:145 [6] NCCL INFO Channel 07/0 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:133:133 [2] NCCL INFO Channel 09/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:141:141 [4] NCCL INFO Channel 04/0 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:178:178 [0] NCCL INFO Channel 05/0 : 0[170] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:141:141 [4] NCCL INFO Channel 10/0 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:178:178 [0] NCCL INFO Channel 11/0 : 0[170] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:143:143 [5] NCCL INFO Channel 05/0 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:130:130 [1] NCCL INFO Channel 04/0 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:143:143 [5] NCCL INFO Channel 11/0 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:130:130 [1] NCCL INFO Channel 10/0 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:141:141 [4] NCCL INFO Channel 02/0 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:178:178 [0] NCCL INFO Channel 00/0 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:141:141 [4] NCCL INFO Channel 03/0 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:178:178 [0] NCCL INFO Channel 01/0 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:145:145 [6] NCCL INFO Channel 04/0 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:141:141 [4] NCCL INFO Channel 08/0 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:133:133 [2] NCCL INFO Channel 05/0 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:178:178 [0] NCCL INFO Channel 06/0 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:143:143 [5] NCCL INFO Channel 02/0 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:130:130 [1] NCCL INFO Channel 00/0 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:145:145 [6] NCCL INFO Channel 10/0 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:133:133 [2] NCCL INFO Channel 11/0 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:141:141 [4] NCCL INFO Channel 09/0 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:178:178 [0] NCCL INFO Channel 07/0 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:143:143 [5] NCCL INFO Channel 03/0 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:130:130 [1] NCCL INFO Channel 01/0 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:143:143 [5] NCCL INFO Channel 08/0 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:130:130 [1] NCCL INFO Channel 06/0 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:143:143 [5] NCCL INFO Channel 09/0 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:130:130 [1] NCCL INFO Channel 07/0 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:133:133 [2] NCCL INFO Channel 04/0 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:145:145 [6] NCCL INFO Channel 05/0 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:133:133 [2] NCCL INFO Channel 10/0 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:145:145 [6] NCCL INFO Channel 11/0 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:146:146 [7] NCCL INFO Channel 05/0 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:137:137 [3] NCCL INFO Channel 04/0 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:146:146 [7] NCCL INFO Channel 11/0 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:137:137 [3] NCCL INFO Channel 10/0 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:141:141 [4] NCCL INFO Channel 00/0 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:178:178 [0] NCCL INFO Channel 02/0 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:141:141 [4] NCCL INFO Channel 01/0 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:178:178 [0] NCCL INFO Channel 03/0 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:146:146 [7] NCCL INFO Channel 00/0 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:141:141 [4] NCCL INFO Channel 06/0 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:137:137 [3] NCCL INFO Channel 02/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:178:178 [0] NCCL INFO Channel 08/0 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:146:146 [7] NCCL INFO Channel 01/0 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:141:141 [4] NCCL INFO Channel 07/0 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:178:178 [0] NCCL INFO Channel 09/0 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:137:137 [3] NCCL INFO Channel 03/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:146:146 [7] NCCL INFO Channel 06/0 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:137:137 [3] NCCL INFO Channel 08/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:146:146 [7] NCCL INFO Channel 07/0 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:137:137 [3] NCCL INFO Channel 09/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:146:146 [7] NCCL INFO Channel 04/0 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:137:137 [3] NCCL INFO Channel 05/0 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:146:146 [7] NCCL INFO Channel 10/0 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:137:137 [3] NCCL INFO Channel 11/0 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:146:146 [7] NCCL INFO Channel 02/0 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:137:137 [3] NCCL INFO Channel 00/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:146:146 [7] NCCL INFO Channel 03/0 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:137:137 [3] NCCL INFO Channel 01/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:146:146 [7] NCCL INFO Channel 08/0 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:137:137 [3] NCCL INFO Channel 06/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:137:137 [3] NCCL INFO Channel 07/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:146:146 [7] NCCL INFO Channel 09/0 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:145:145 [6] NCCL INFO Channel 02/0 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:143:143 [5] NCCL INFO Channel 04/0 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:130:130 [1] NCCL INFO Channel 05/0 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:133:133 [2] NCCL INFO Channel 00/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:145:145 [6] NCCL INFO Channel 03/0 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:133:133 [2] NCCL INFO Channel 01/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:130:130 [1] NCCL INFO Channel 11/0 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:143:143 [5] NCCL INFO Channel 10/0 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:145:145 [6] NCCL INFO Channel 08/0 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:133:133 [2] NCCL INFO Channel 06/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:145:145 [6] NCCL INFO Channel 09/0 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:133:133 [2] NCCL INFO Channel 07/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:141:141 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:141:141 [4] NCCL INFO Channel 04/0 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:178:178 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:141:141 [4] NCCL INFO Channel 10/0 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:146:146 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:137:137 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:143:143 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:130:130 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:130:130 [1] NCCL INFO Channel 00/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:145:145 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:130:130 [1] NCCL INFO Channel 01/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:133:133 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:143:143 [5] NCCL INFO Channel 02/0 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:130:130 [1] NCCL INFO Channel 06/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:143:143 [5] NCCL INFO Channel 03/0 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:130:130 [1] NCCL INFO Channel 07/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:143:143 [5] NCCL INFO Channel 08/0 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:145:145 [6] NCCL INFO Channel 02/0 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:143:143 [5] NCCL INFO Channel 09/0 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:145:145 [6] NCCL INFO Channel 03/0 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:133:133 [2] NCCL INFO Channel 00/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:145:145 [6] NCCL INFO Channel 08/0 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:133:133 [2] NCCL INFO Channel 01/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:141:141 [4] NCCL INFO Channel 05/0 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:145:145 [6] NCCL INFO Channel 09/0 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:133:133 [2] NCCL INFO Channel 06/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:141:141 [4] NCCL INFO Channel 11/0 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:133:133 [2] NCCL INFO Channel 07/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:143:143 [5] NCCL INFO Channel 04/0 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:130:130 [1] NCCL INFO Channel 05/0 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:143:143 [5] NCCL INFO Channel 10/0 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:130:130 [1] NCCL INFO Channel 11/0 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:133:133 [2] NCCL INFO Channel 04/0 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:141:141 [4] NCCL INFO Channel 00/0 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:133:133 [2] NCCL INFO Channel 10/0 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:137:137 [3] NCCL INFO Channel 05/0 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:141:141 [4] NCCL INFO Channel 01/0 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:137:137 [3] NCCL INFO Channel 11/0 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:145:145 [6] NCCL INFO Channel 05/0 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:141:141 [4] NCCL INFO Channel 06/0 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:143:143 [5] NCCL INFO Channel 00/0 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:130:130 [1] NCCL INFO Channel 02/0 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:145:145 [6] NCCL INFO Channel 11/0 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:141:141 [4] NCCL INFO Channel 07/0 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:143:143 [5] NCCL INFO Channel 01/0 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:130:130 [1] NCCL INFO Channel 03/0 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:130:130 [1] NCCL INFO Channel 08/0 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:143:143 [5] NCCL INFO Channel 06/0 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:130:130 [1] NCCL INFO Channel 09/0 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:143:143 [5] NCCL INFO Channel 07/0 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:145:145 [6] NCCL INFO Channel 04/0 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:133:133 [2] NCCL INFO Channel 05/0 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:141:141 [4] NCCL INFO Channel 02/0 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:145:145 [6] NCCL INFO Channel 10/0 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:133:133 [2] NCCL INFO Channel 11/0 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:141:141 [4] NCCL INFO Channel 03/0 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:146:146 [7] NCCL INFO Channel 04/0 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:141:141 [4] NCCL INFO Channel 08/0 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:146:146 [7] NCCL INFO Channel 10/0 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:141:141 [4] NCCL INFO Channel 09/0 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:146:146 [7] NCCL INFO Channel 02/0 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:137:137 [3] NCCL INFO Channel 00/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:146:146 [7] NCCL INFO Channel 03/0 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:137:137 [3] NCCL INFO Channel 01/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:146:146 [7] NCCL INFO Channel 08/0 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:137:137 [3] NCCL INFO Channel 06/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:146:146 [7] NCCL INFO Channel 09/0 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:137:137 [3] NCCL INFO Channel 07/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:146:146 [7] NCCL INFO Channel 05/0 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:137:137 [3] NCCL INFO Channel 04/0 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:146:146 [7] NCCL INFO Channel 11/0 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:137:137 [3] NCCL INFO Channel 10/0 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:146:146 [7] NCCL INFO Channel 00/0 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:137:137 [3] NCCL INFO Channel 02/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:146:146 [7] NCCL INFO Channel 01/0 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:137:137 [3] NCCL INFO Channel 03/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:146:146 [7] NCCL INFO Channel 06/0 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:137:137 [3] NCCL INFO Channel 08/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:137:137 [3] NCCL INFO Channel 09/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:146:146 [7] NCCL INFO Channel 07/0 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:145:145 [6] NCCL INFO Channel 00/0 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:130:130 [1] NCCL INFO Channel 04/0 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:143:143 [5] NCCL INFO Channel 05/0 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:133:133 [2] NCCL INFO Channel 02/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:145:145 [6] NCCL INFO Channel 01/0 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:130:130 [1] NCCL INFO Channel 10/0 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:145:145 [6] NCCL INFO Channel 06/0 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:143:143 [5] NCCL INFO Channel 11/0 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:133:133 [2] NCCL INFO Channel 03/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:133:133 [2] NCCL INFO Channel 08/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:145:145 [6] NCCL INFO Channel 07/0 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:178:178 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:178:178 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:133:133 [2] NCCL INFO Channel 09/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:178:178 [0] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:141:141 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:141:141 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:141:141 [4] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:146:146 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:146:146 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:146:146 [7] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:146:146 [7] NCCL INFO Channel 08/1 : 7[1e0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:137:137 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:137:137 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:137:137 [3] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:137:137 [3] NCCL INFO Channel 08/1 : 3[1a0] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:143:143 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:143:143 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:143:143 [5] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:130:130 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:130:130 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:130:130 [1] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:145:145 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:145:145 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:145:145 [6] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:145:145 [6] NCCL INFO Channel 04/1 : 6[1d0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:133:133 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:133:133 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:133:133 [2] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:133:133 [2] NCCL INFO Channel 04/1 : 2[190] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:146:146 [7] NCCL INFO Channel 09/1 : 7[1e0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:137:137 [3] NCCL INFO Channel 09/1 : 3[1a0] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:133:133 [2] NCCL INFO Channel 05/1 : 2[190] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:145:145 [6] NCCL INFO Channel 05/1 : 6[1d0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:146:146 [7] NCCL INFO Channel 04/1 : 7[1e0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:137:137 [3] NCCL INFO Channel 04/1 : 3[1a0] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:146:146 [7] NCCL INFO Channel 05/1 : 7[1e0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:146:146 [7] NCCL INFO Channel 12/1 : 7[1e0] -> 2[190] via P2P/indirect/3[1a0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:137:137 [3] NCCL INFO Channel 05/1 : 3[1a0] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:137:137 [3] NCCL INFO Channel 12/1 : 3[1a0] -> 6[1d0] via P2P/indirect/7[1e0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:146:146 [7] NCCL INFO Channel 13/1 : 7[1e0] -> 2[190] via P2P/indirect/3[1a0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:145:145 [6] NCCL INFO Channel 12/1 : 6[1d0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:137:137 [3] NCCL INFO Channel 13/1 : 3[1a0] -> 6[1d0] via P2P/indirect/7[1e0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:130:130 [1] NCCL INFO Channel 12/1 : 1[180] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:133:133 [2] NCCL INFO Channel 12/1 : 2[190] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:143:143 [5] NCCL INFO Channel 12/1 : 5[1c0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:145:145 [6] NCCL INFO Channel 13/1 : 6[1d0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:130:130 [1] NCCL INFO Channel 13/1 : 1[180] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:143:143 [5] NCCL INFO Channel 13/1 : 5[1c0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:133:133 [2] NCCL INFO Channel 13/1 : 2[190] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:141:141 [4] NCCL INFO Channel 10/1 : 4[1b0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:141:141 [4] NCCL INFO Channel 11/1 : 4[1b0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:178:178 [0] NCCL INFO Channel 10/1 : 0[170] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:178:178 [0] NCCL INFO Channel 11/1 : 0[170] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:130:130 [1] NCCL INFO Channel 10/1 : 1[180] -> 6[1d0] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:145:145 [6] NCCL INFO Channel 10/1 : 6[1d0] -> 3[1a0] via P2P/indirect/2[190]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:130:130 [1] NCCL INFO Channel 11/1 : 1[180] -> 6[1d0] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:133:133 [2] NCCL INFO Channel 10/1 : 2[190] -> 7[1e0] via P2P/indirect/6[1d0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:143:143 [5] NCCL INFO Channel 10/1 : 5[1c0] -> 2[190] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:145:145 [6] NCCL INFO Channel 11/1 : 6[1d0] -> 3[1a0] via P2P/indirect/2[190]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:133:133 [2] NCCL INFO Channel 11/1 : 2[190] -> 7[1e0] via P2P/indirect/6[1d0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:143:143 [5] NCCL INFO Channel 11/1 : 5[1c0] -> 2[190] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:141:141 [4] NCCL INFO Channel 06/1 : 4[1b0] -> 2[190] via P2P/indirect/6[1d0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:130:130 [1] NCCL INFO Channel 06/1 : 1[180] -> 7[1e0] via P2P/indirect/3[1a0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:141:141 [4] NCCL INFO Channel 07/1 : 4[1b0] -> 2[190] via P2P/indirect/6[1d0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:130:130 [1] NCCL INFO Channel 07/1 : 1[180] -> 7[1e0] via P2P/indirect/3[1a0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:178:178 [0] NCCL INFO Channel 06/1 : 0[170] -> 6[1d0] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:143:143 [5] NCCL INFO Channel 06/1 : 5[1c0] -> 3[1a0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:178:178 [0] NCCL INFO Channel 07/1 : 0[170] -> 6[1d0] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:143:143 [5] NCCL INFO Channel 07/1 : 5[1c0] -> 3[1a0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:178:178 [0] NCCL INFO Channel 14/1 : 0[170] -> 7[1e0] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:141:141 [4] NCCL INFO Channel 14/1 : 4[1b0] -> 3[1a0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:178:178 [0] NCCL INFO Channel 15/1 : 0[170] -> 7[1e0] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:141:141 [4] NCCL INFO Channel 15/1 : 4[1b0] -> 3[1a0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:143:143 [5] NCCL INFO comm 0x56272723cfd0 rank 5 nranks 8 cudaDev 5 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:130:130 [1] NCCL INFO comm 0x55c20671f4b0 rank 1 nranks 8 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:178:178 [0] NCCL INFO comm 0x564b4c00d220 rank 0 nranks 8 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Running smdistributed.dataparallel v1.7.0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:SMDDP: Single node mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:146:146 [7] NCCL INFO comm 0x561ad3d2c470 rank 7 nranks 8 cudaDev 7 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:145:145 [6] NCCL INFO comm 0x5598cc70ba60 rank 6 nranks 8 cudaDev 6 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:133:133 [2] NCCL INFO comm 0x56311fa21fa0 rank 2 nranks 8 cudaDev 2 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:137:137 [3] NCCL INFO comm 0x55740200f3d0 rank 3 nranks 8 cudaDev 3 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:141:141 [4] NCCL INFO comm 0x564130b8d880 rank 4 nranks 8 cudaDev 4 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:2023-07-25 15:46:49,566 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:1 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:2023-07-25 15:46:49,566 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:1 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:2023-07-25 15:46:49,566 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:1 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:2023-07-25 15:46:49,566 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:1 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:2023-07-25 15:46:49,566 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:1 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:2023-07-25 15:46:49,566 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:1 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:2023-07-25 15:46:49,566 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:1 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:2023-07-25 15:46:49,566 - torch.distributed.distributed_c10d - INFO - Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:2023-07-25 15:46:49,566 - torch.distributed.distributed_c10d - INFO - Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:2023-07-25 15:46:49,567 - torch.distributed.distributed_c10d - INFO - Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:2023-07-25 15:46:49,567 - torch.distributed.distributed_c10d - INFO - Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:2023-07-25 15:46:49,567 - torch.distributed.distributed_c10d - INFO - Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:2023-07-25 15:46:49,567 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:1 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:2023-07-25 15:46:49,567 - torch.distributed.distributed_c10d - INFO - Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:2023-07-25 15:46:49,567 - torch.distributed.distributed_c10d - INFO - Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:2023-07-25 15:46:49,577 - torch.distributed.distributed_c10d - INFO - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:The following columns in the training set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:The following columns in the training set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:***** Running training *****\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  Num examples = 25000\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:***** Running training *****\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  Num examples = 25000\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  Instantaneous batch size per device = 32\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  Total train batch size (w. parallel, distributed & accumulation) = 256\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  Total optimization steps = 98\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  Instantaneous batch size per device = 32\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  Total train batch size (w. parallel, distributed & accumulation) = 256\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  Total optimization steps = 98\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  Number of trainable parameters = 1825540\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  Number of trainable parameters = 1825540\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  0%|          | 0/98 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-07-25 15:46:49.869: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-07-25 15:46:49.869: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-07-25 15:46:49.869: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-07-25 15:46:49.869: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-07-25 15:46:49.869: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-07-25 15:46:49.871: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-07-25 15:46:49.870: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:2023-07-25 15:46:49,876 - root - INFO - Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:2023-07-25 15:46:49,876 - root - INFO - Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:2023-07-25 15:46:49,876 - root - INFO - Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:2023-07-25 15:46:49,876 - root - INFO - Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:2023-07-25 15:46:49,876 - root - INFO - Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:2023-07-25 15:46:49,876 - root - INFO - Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:2023-07-25 15:46:49,877 - root - INFO - Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-07-25 15:46:49.879: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:2023-07-25 15:46:49,886 - root - INFO - Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-07-25 15:46:49.912 algo-1:178 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-07-25 15:46:49.912 algo-1:146 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-07-25 15:46:49.912 algo-1:130 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-07-25 15:46:49.912 algo-1:145 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-07-25 15:46:49.912 algo-1:133 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-07-25 15:46:49.912 algo-1:143 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-07-25 15:46:49.913 algo-1:137 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-07-25 15:46:49.924 algo-1:141 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-07-25 15:46:49.958 algo-1:137 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-07-25 15:46:49.958 algo-1:178 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-07-25 15:46:49.958 algo-1:146 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-07-25 15:46:49.958 algo-1:130 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-07-25 15:46:49.958 algo-1:143 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-07-25 15:46:49.958 algo-1:145 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-07-25 15:46:49.958 algo-1:133 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-07-25 15:46:49.959 algo-1:178 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-07-25 15:46:49.959 algo-1:146 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-07-25 15:46:49.959 algo-1:130 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-07-25 15:46:49.959 algo-1:143 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-07-25 15:46:49.959 algo-1:145 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-07-25 15:46:49.959 algo-1:133 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-07-25 15:46:49.959 algo-1:137 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-07-25 15:46:49.959 algo-1:133 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-07-25 15:46:49.959 algo-1:145 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-07-25 15:46:49.959 algo-1:178 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-07-25 15:46:49.959 algo-1:146 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-07-25 15:46:49.959 algo-1:130 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-07-25 15:46:49.959 algo-1:143 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-07-25 15:46:49.959 algo-1:137 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-07-25 15:46:49.960 algo-1:178 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-07-25 15:46:49.960 algo-1:137 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-07-25 15:46:49.960 algo-1:130 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-07-25 15:46:49.960 algo-1:178 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-07-25 15:46:49.960 algo-1:133 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-07-25 15:46:49.960 algo-1:130 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-07-25 15:46:49.960 algo-1:137 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-07-25 15:46:49.960 algo-1:143 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-07-25 15:46:49.960 algo-1:145 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-07-25 15:46:49.960 algo-1:133 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-07-25 15:46:49.960 algo-1:146 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-07-25 15:46:49.960 algo-1:143 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-07-25 15:46:49.960 algo-1:145 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-07-25 15:46:49.960 algo-1:146 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-07-25 15:46:49.971 algo-1:141 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-07-25 15:46:49.972 algo-1:141 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-07-25 15:46:49.972 algo-1:141 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-07-25 15:46:49.973 algo-1:141 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-07-25 15:46:49.973 algo-1:141 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    return _run_code(code, main_globals, None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 87, in _run_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/mpi4py/__main__.py\", line 7, in <module>\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    main()\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/mpi4py/run.py\", line 198, in main\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    run_command_line(args)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/mpi4py/run.py\", line 47, in run_command_line\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    run_path(sys.argv[0], run_name='__main__')\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 288, in run_path\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    return _run_module_code(code, init_globals, run_name,\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 97, in _run_module_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    _run_code(code, mod_globals, init_globals,\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 87, in _run_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"train-LORA.py\", line 171, in <module>\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    trainer.train()\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 1543, in train\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    return inner_training_loop(\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 1791, in _inner_training_loop\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    tr_loss_step = self.training_step(model, inputs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 2539, in training_step\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    loss = self.compute_loss(model, inputs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 2571, in compute_loss\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    outputs = model(**inputs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1026, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    if torch.is_grad_enabled() and self.reducer._rebuild_buckets():\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:making sure all `forward` function outputs participate in calculating loss. \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Parameters which did not receive grad for rank 7: base_model.model.classifier.original_module.bias, base_model.model.classifier.original_module.weight, base_model.model.pre_classifier.original_module.bias, base_model.model.pre_classifier.original_module.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Parameter indices which did not receive grad for rank 7: 73 74 77 78\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mMPI_ABORT was invoked on rank 7 in communicator MPI COMMUNICATOR 4 DUP FROM 0\u001b[0m\n",
      "\u001b[34mwith errorcode 1.\u001b[0m\n",
      "\u001b[34mNOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.\u001b[0m\n",
      "\u001b[34mYou may or may not see output from other processes, depending on\u001b[0m\n",
      "\u001b[34mexactly when Open MPI kills them.\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:141:1435 [4] NCCL INFO [Service thread] Connection closed by localRank 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    return _run_code(code, main_globals, None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 87, in _run_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/mpi4py/__main__.py\", line 7, in <module>\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    main()\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/mpi4py/run.py\", line 198, in main\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    run_command_line(args)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/mpi4py/run.py\", line 47, in run_command_line\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    run_path(sys.argv[0], run_name='__main__')\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 288, in run_path\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    return _run_module_code(code, init_globals, run_name,\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 97, in _run_module_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    _run_code(code, mod_globals, init_globals,\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 87, in _run_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"train-LORA.py\", line 171, in <module>\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    trainer.train()\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 1543, in train\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    return inner_training_loop(\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 1791, in _inner_training_loop\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    tr_loss_step = self.training_step(model, inputs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 2539, in training_step\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    loss = self.compute_loss(model, inputs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 2571, in compute_loss\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    outputs = model(**inputs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1026, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    if torch.is_grad_enabled() and self.reducer._rebuild_buckets():\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:making sure all `forward` function outputs participate in calculating loss. \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Parameters which did not receive grad for rank 2: base_model.model.classifier.original_module.bias, base_model.model.classifier.original_module.weight, base_model.model.pre_classifier.original_module.bias, base_model.model.pre_classifier.original_module.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Parameter indices which did not receive grad for rank 2: 73 74 77 78\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:137:1436 [3] NCCL INFO [Service thread] Connection closed by localRank 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    return _run_code(code, main_globals, None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 87, in _run_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/mpi4py/__main__.py\", line 7, in <module>\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    main()\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/mpi4py/run.py\", line 198, in main\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    run_command_line(args)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/mpi4py/run.py\", line 47, in run_command_line\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    run_path(sys.argv[0], run_name='__main__')\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 288, in run_path\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    return _run_module_code(code, init_globals, run_name,\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 97, in _run_module_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    _run_code(code, mod_globals, init_globals,\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 87, in _run_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"train-LORA.py\", line 171, in <module>\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    trainer.train()\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 1543, in train\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    return inner_training_loop(\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 1791, in _inner_training_loop\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    tr_loss_step = self.training_step(model, inputs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 2539, in training_step\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    loss = self.compute_loss(model, inputs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 2571, in compute_loss\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    outputs = model(**inputs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1026, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    if torch.is_grad_enabled() and self.reducer._rebuild_buckets():\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:making sure all `forward` function outputs participate in calculating loss. \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Parameters which did not receive grad for rank 3: base_model.model.classifier.original_module.bias, base_model.model.classifier.original_module.weight, base_model.model.pre_classifier.original_module.bias, base_model.model.pre_classifier.original_module.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Parameter indices which did not receive grad for rank 3: 73 74 77 78\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    return _run_code(code, main_globals, None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 87, in _run_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/mpi4py/__main__.py\", line 7, in <module>\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    main()\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/mpi4py/run.py\", line 198, in main\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    run_command_line(args)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/mpi4py/run.py\", line 47, in run_command_line\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    run_path(sys.argv[0], run_name='__main__')\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 288, in run_path\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    return _run_module_code(code, init_globals, run_name,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 97, in _run_module_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    _run_code(code, mod_globals, init_globals,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 87, in _run_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"train-LORA.py\", line 171, in <module>\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    trainer.train()\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 1543, in train\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    return inner_training_loop(\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 1791, in _inner_training_loop\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    tr_loss_step = self.training_step(model, inputs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 2539, in training_step\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    loss = self.compute_loss(model, inputs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 2571, in compute_loss\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    outputs = model(**inputs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1026, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    if torch.is_grad_enabled() and self.reducer._rebuild_buckets():\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:making sure all `forward` function outputs participate in calculating loss. \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Parameters which did not receive grad for rank 0: base_model.model.classifier.original_module.bias, base_model.model.classifier.original_module.weight, base_model.model.pre_classifier.original_module.bias, base_model.model.pre_classifier.original_module.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Parameter indices which did not receive grad for rank 0: 73 74 77 78\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    return _run_code(code, main_globals, None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 87, in _run_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/mpi4py/__main__.py\", line 7, in <module>\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    main()\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/mpi4py/run.py\", line 198, in main\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    run_command_line(args)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/mpi4py/run.py\", line 47, in run_command_line\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    run_path(sys.argv[0], run_name='__main__')\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 288, in run_path\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    return _run_module_code(code, init_globals, run_name,\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 97, in _run_module_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    _run_code(code, mod_globals, init_globals,\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 87, in _run_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"train-LORA.py\", line 171, in <module>\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    trainer.train()\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 1543, in train\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    return inner_training_loop(\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 1791, in _inner_training_loop\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    tr_loss_step = self.training_step(model, inputs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 2539, in training_step\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    loss = self.compute_loss(model, inputs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 2571, in compute_loss\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    outputs = model(**inputs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1026, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    if torch.is_grad_enabled() and self.reducer._rebuild_buckets():\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:making sure all `forward` function outputs participate in calculating loss. \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Parameters which did not receive grad for rank 1: base_model.model.classifier.original_module.bias, base_model.model.classifier.original_module.weight, base_model.model.pre_classifier.original_module.bias, base_model.model.pre_classifier.original_module.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Parameter indices which did not receive grad for rank 1: 73 74 77 78\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:143:1434 [5] NCCL INFO [Service thread] Connection closed by localRank 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    return _run_code(code, main_globals, None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 87, in _run_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/mpi4py/__main__.py\", line 7, in <module>\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    main()\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/mpi4py/run.py\", line 198, in main\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    run_command_line(args)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/mpi4py/run.py\", line 47, in run_command_line\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    run_path(sys.argv[0], run_name='__main__')\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 288, in run_path\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    return _run_module_code(code, init_globals, run_name,\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 97, in _run_module_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    _run_code(code, mod_globals, init_globals,\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 87, in _run_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"train-LORA.py\", line 171, in <module>\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    trainer.train()\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 1543, in train\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    return inner_training_loop(\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 1791, in _inner_training_loop\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    tr_loss_step = self.training_step(model, inputs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 2539, in training_step\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    loss = self.compute_loss(model, inputs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 2571, in compute_loss\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    outputs = model(**inputs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1026, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    if torch.is_grad_enabled() and self.reducer._rebuild_buckets():\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:making sure all `forward` function outputs participate in calculating loss. \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Parameters which did not receive grad for rank 5: base_model.model.classifier.original_module.bias, base_model.model.classifier.original_module.weight, base_model.model.pre_classifier.original_module.bias, base_model.model.pre_classifier.original_module.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Parameter indices which did not receive grad for rank 5: 73 74 77 78\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    return _run_code(code, main_globals, None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 87, in _run_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/mpi4py/__main__.py\", line 7, in <module>\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    main()\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/mpi4py/run.py\", line 198, in main\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    run_command_line(args)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/mpi4py/run.py\", line 47, in run_command_line\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    run_path(sys.argv[0], run_name='__main__')\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 288, in run_path\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    return _run_module_code(code, init_globals, run_name,\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 97, in _run_module_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    _run_code(code, mod_globals, init_globals,\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/runpy.py\", line 87, in _run_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"train-LORA.py\", line 171, in <module>\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    trainer.train()\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 1543, in train\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    return inner_training_loop(\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 1791, in _inner_training_loop\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    tr_loss_step = self.training_step(model, inputs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 2539, in training_step\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    loss = self.compute_loss(model, inputs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 2571, in compute_loss\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    outputs = model(**inputs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1026, in forward\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    if torch.is_grad_enabled() and self.reducer._rebuild_buckets():\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:making sure all `forward` function outputs participate in calculating loss. \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Parameters which did not receive grad for rank 6: base_model.model.classifier.original_module.bias, base_model.model.classifier.original_module.weight, base_model.model.pre_classifier.original_module.bias, base_model.model.pre_classifier.original_module.weight\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Parameter indices which did not receive grad for rank 6: 73 74 77 78\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  1%|          | 1/98 [00:01<02:44,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[algo-1:00117] 6 more processes have sent help message help-mpi-api.txt / mpi-abort\u001b[0m\n",
      "\u001b[34m[algo-1:00117] Set MCA parameter \"orte_base_help_aggregate\" to 0 to see all help / error messages\u001b[0m\n",
      "\u001b[34m2023-07-25 15:46:51,610 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-07-25 15:46:51,611 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 1 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-07-25 15:46:51,612 sagemaker-training-toolkit ERROR    Reporting training FAILURE\u001b[0m\n",
      "\u001b[34m2023-07-25 15:46:51,612 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mExitCode 1\u001b[0m\n",
      "\u001b[34mErrorMessage \"RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by\n",
      " making sure all `forward` function outputs participate in calculating loss.\n",
      " If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).\n",
      " Parameters which did not receive grad for rank 7: base_model.model.classifier.original_module.bias, base_model.model.classifier.original_module.weight, base_model.model.pre_classifier.original_module.bias, base_model.model.pre_classifier.original_module.weight\n",
      " Parameter indices which did not receive grad for rank 7: 73 74 77 78\n",
      " algo-1:141:1435 [4] NCCL INFO [Service thread] Connection closed by localRank 7\n",
      " Traceback (most recent call last)\n",
      " File \"/opt/conda/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      " return _run_code(code, main_globals, None,\n",
      " File \"/opt/conda/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      " exec(code, run_globals)\n",
      " File \"/opt/conda/lib/python3.9/site-packages/mpi4py/__main__.py\", line 7, in <module>\n",
      " main()\n",
      " File \"/opt/conda/lib/python3.9/site-packages/mpi4py/run.py\", line 198, in main\n",
      " run_command_line(args)\n",
      " File \"/opt/conda/lib/python3.9/site-packages/mpi4py/run.py\", line 47, in run_command_line\n",
      " run_path(sys.argv[0], run_name='__main__')\n",
      " File \"/opt/conda/lib/python3.9/runpy.py\", line 288, in run_path\n",
      " return _run_module_code(code, init_globals, run_name,\n",
      " File \"/opt/conda/lib/python3.9/runpy.py\", line 97, in _run_module_code\n",
      " _run_code(code, mod_globals, init_globals,\n",
      " File \"train-LORA.py\", line 171, in <module>\n",
      " trainer.train()\n",
      " File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 1543, in train\n",
      " return inner_training_loop(\n",
      " File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 1791, in _inner_training_loop\n",
      " tr_loss_step = self.training_step(model, inputs)\n",
      " File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 2539, in training_step\n",
      " loss = self.compute_loss(model, inputs)\n",
      " File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line 2571, in compute_loss\n",
      " outputs = model(**inputs)\n",
      " File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      " return forward_call(*input, **kwargs)\n",
      " File \"/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1026, in forward\n",
      " if torch.is_grad_enabled() and self.reducer._rebuild_buckets()\n",
      " RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by\n",
      " Parameters which did not receive grad for rank 2: base_model.model.classifier.original_module.bias, base_model.model.classifier.original_module.weight, base_model.model.pre_classifier.original_module.bias, base_model.model.pre_classifier.original_module.weight\n",
      " Parameter indices which did not receive grad for rank 2: 73 74 77 78\n",
      " algo-1:137:1436 [3] NCCL INFO [Service thread] Connection closed by localRank 7\n",
      " Parameters which did not receive grad for rank 3: base_model.model.classifier.original_module.bias, base_model.model.classifier.original_module.weight, base_model.model.pre_classifier.original_module.bias, base_model.model.pre_classifier.original_module.weight\n",
      " Parameter indices which did not receive grad for rank 3: 73 74 77 78\n",
      " Parameters which did not receive grad for rank 0: base_model.model.classifier.original_module.bias, base_model.model.classifier.original_module.weight, base_model.model.pre_classifier.original_module.bias, base_model.model.pre_classifier.original_module.weight\n",
      " Parameter indices which did not receive grad for rank 0: 73 74 77 78\n",
      " Parameters which did not receive grad for rank 1: base_model.model.classifier.original_module.bias, base_model.model.classifier.original_module.weight, base_model.model.pre_classifier.original_module.bias, base_model.model.pre_classifier.original_module.weight\n",
      " Parameter indices which did not receive grad for rank 1: 73 74 77 78\n",
      " algo-1:143:1434 [5] NCCL INFO [Service thread] Connection closed by localRank 7\n",
      " Parameters which did not receive grad for rank 5: base_model.model.classifier.original_module.bias, base_model.model.classifier.original_module.weight, base_model.model.pre_classifier.original_module.bias, base_model.model.pre_classifier.original_module.weight\n",
      " Parameter indices which did not receive grad for rank 5: 73 74 77 78\n",
      " Parameters which did not receive grad for rank 6: base_model.model.classifier.original_module.bias, base_model.model.classifier.original_module.weight, base_model.model.pre_classifier.original_module.bias, base_model.model.pre_classifier.original_module.weight\n",
      " Parameter indices which did not receive grad for rank 6: 73 74 77 78\n",
      " WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /simple/absl-py/\n",
      " ERROR: Exception\n",
      " File \"/opt/conda/lib/python3.9/site-packages/pip/_vendor/urllib3/response.py\", line 438, in _error_catcher\n",
      " yield\n",
      " File \"/opt/conda/lib/python3.9/site-packages/pip/_vendor/urllib3/response.py\", line 561, in read\n",
      " data = self._fp_read(amt) if not fp_closed else b\"\"\n",
      " File \"/opt/conda/lib/python3.9/site-packages/pip/_vendor/urllib3/response.py\", line 527, in _fp_read\n",
      " return self._fp.read(amt) if amt is not None else self._fp.read()\n",
      " File \"/opt/conda/lib/python3.9/site-packages/pip/_vendor/cachecontrol/filewrapper.py\", line 90, in read\n",
      " data = self.__fp.read(amt)\n",
      " File \"/opt/conda/lib/python3.9/http/client.py\", line 463, in read\n",
      " n = self.readinto(b)\n",
      " File \"/opt/conda/lib/python3.9/http/client.py\", line 507, in readinto\n",
      " n = self.fp.readinto(b)\n",
      " File \"/opt/conda/lib/python3.9/socket.py\", line 704, in readinto\n",
      " return self._sock.recv_into(b)\n",
      " File \"/opt/conda/lib/python3.9/ssl.py\", line 1242, in recv_into\n",
      " return self.read(nbytes, buffer)\n",
      " File \"/opt/conda/lib/python3.9/ssl.py\", line 1100, in read\n",
      " return self._sslobj.read(len, buffer)\n",
      " ConnectionResetError: [Errno 104] Connection reset by peer\n",
      " \n",
      " During handling of the above exception, another exception occurred\n",
      " File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/cli/base_command.py\", line 160, in exc_logging_wrapper\n",
      " status = run_func(*args)\n",
      " File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/cli/req_command.py\", line 247, in wrapper\n",
      " return func(self, options, args)\n",
      " File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/commands/install.py\", line 415, in run\n",
      " requirement_set = resolver.resolve(\n",
      " File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 92, in resolve\n",
      " result = self._result = resolver.resolve(\n",
      " File \"/opt/conda/lib/python3.9/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 481, in resolve\n",
      " state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
      " File \"/opt/conda/lib/python3.9/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 373, in resolve\n",
      " failure_causes = self._attempt_to_pin_criterion(name)\n",
      " File \"/opt/conda/lib/python3.9/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 213, in _attempt_to_pin_criterion\n",
      " criteria = self._get_updated_criteria(candidate)\n",
      " File \"/opt/conda/lib/python3.9/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 204, in _get_updated_criteria\n",
      " self._add_to_criteria(criteria, requirement, parent=candidate)\n",
      " File \"/opt/conda/lib/python3.9/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 172, in _add_to_criteria\n",
      " if not criterion.candidates\n",
      " File \"/opt/conda/lib/python3.9/site-packages/pip/_vendor/resolvelib/structs.py\", line 151, in __bool__\n",
      " return bool(self._sequence)\n",
      " File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 155, in __bool__\n",
      " return any(self)\n",
      " File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 143, in <genexpr>\n",
      " return (c for c in iterator if id(c) not in self._incompatible_ids)\n",
      " File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 47, in _iter_built\n",
      " candidate = func()\n",
      " File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/factory.py\", line 206, in _make_candidate_from_link\n",
      " self._link_candidate_cache[link] = LinkCandidate(\n",
      " File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 297, in __init__\n",
      " super().__init__(\n",
      " File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 162, in __init__\n",
      " self.dist = self._prepare()\n",
      " File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 231, in _prepare\n",
      " dist = self._prepare_distribution()\n",
      " File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/resolution/r:esolvelib/candidates.py\", line 308, in _prepare_distribution\n",
      " return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True)\n",
      " File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/operations/prepare.py\", line 491, in prepare_linked_requirement\n",
      " return self._prepare_linked_requirement(req, parallel_builds)\n",
      " File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/operations/prepare.py\", line 536, in _prepare_linked_requirement\n",
      " local_file = unpack_url(\n",
      " File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/operations/prepare.py\", line 166, in unpack_url\n",
      " file = get_http_url(\n",
      " File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/operations/prepare.py\", line 107, in get_http_url\n",
      " from_path, content_type = download(link, temp_dir.path)\n",
      " File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/network/download.py\", line 147, in __call__\n",
      " for chunk in chunks\n",
      " File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/cli/progress_bars.py\", line 53, in _rich_progress_bar\n",
      " for chunk in iterable\n",
      " File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/network/utils.py\", line 63, in response_chunks\n",
      " for chunk in response.raw.stream(\n",
      " File \"/opt/conda/lib/python3.9/site-packages/pip/_vendor/urllib3/response.py\", line 622, in stream\n",
      " data = self.read(amt=amt, decode_content=decode_content)\n",
      " File \"/opt/conda/lib/python3.9/site-packages/pip/_vendor/urllib3/response.py\", line 587, in read\n",
      " raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\n",
      " File \"/opt/conda/lib/python3.9/contextlib.py\", line 137, in __exit__\n",
      " self.gen.throw(typ, value, traceback)\n",
      " File \"/opt/conda/lib/python3.9/site-packages/pip/_vendor/urllib3/response.py\", line 455, in _error_catcher\n",
      " raise ProtocolError(\"Connection broken: %r\" % e, e)\n",
      " pip._vendor.urllib3.exceptions.ProtocolError: (\"Connection broken: ConnectionResetError(104, 'Connection reset by peer')\", ConnectionResetError(104, 'Connection reset by peer'))\n",
      " [notice] A new release of pip is available: 23.0 -> 23.2.1\n",
      " [notice] To update, run: pip install --upgrade pip\n",
      " WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      " ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/opt/conda/lib/python3.9/site-packages/nltk/app/rdparser_app.py'\n",
      " #015Downloading ()lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]\n",
      " #015Downloading ()lve/main/config.json: 100%|| 483/483 [00:00<00:00, 63.4kB/s]\n",
      " #015Downloading ()\"model.safetensors\";:   0%|          | 0.00/268M [00:00<?, ?B/s]\n",
      " #015Downloading ()\"model.safetensors\";:   8%|         | 21.0M/268M [00:00<00:01, 130MB/s]\n",
      " #015Downloading ()\"model.safetensors\";:  16%|        | 41.9M/268M [00:00<00:01, 152MB/s]\n",
      " #015Downloading ()\"model.safetensors\";:  27%|       | 73.4M/268M [00:00<00:01, 190MB/s]\n",
      " #015Downloading ()\"model.safetensors\";:  39%|      | 105M/268M [00:00<00:00, 193MB/s]\n",
      " #015Downloading ()\"model.safetensors\";:  47%|     | 126M/268M [00:00<00:00, 187MB/s]\n",
      " #015Downloading ()\"model.safetensors\";:  59%|    | 157M/268M [00:00<00:00, 216MB/s]\n",
      " #015Downloading ()\"model.safetensors\";:  70%|   | 189M/268M [00:00<00:00, 241MB/s]\n",
      " #015Downloading ()\"model.safetensors\";:  82%| | 220M/268M [00:01<00:00, 257MB/s]\n",
      " #015Downloading ()\"model.safetensors\";:  94%|| 252M/268M [00:01<00:00, 247MB/s]\n",
      " #015Downloading ()\"model.safetensors\";: 100%|| 268M/268M [00:01<00:00, 210MB/s]\n",
      " Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n",
      " - This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      " - This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      " Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n",
      " You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " #015Downloading ()okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]\n",
      " #015Downloading ()okenizer_config.json: 100%|| 28.0/28.0 [00:00<00:00, 2.21kB/s]\n",
      " #015Downloading ()solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]\n",
      " #015Downloading ()solve/main/vocab.txt: 100%|| 232k/232k [00:00<00:00, 37.9MB/s]\n",
      " #015Downloading ()/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]\n",
      " #015Downloading ()/main/tokenizer.json: 100%|| 466k/466k [00:00<00:00, 43.4MB/s]\n",
      " Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      " Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      " Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
      " Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      " Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      " Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      " Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n",
      " Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']\n",
      " Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      " Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n",
      " Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
      " /opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      " warnings.warn(\n",
      " The following columns in the training set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      " ***** Running training *****\n",
      " Num examples = 25000\n",
      " Num Epochs = 1\n",
      " Instantaneous batch size per device = 32\n",
      " Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      " Gradient Accumulation steps = 1\n",
      " Total optimization steps = 98\n",
      " Number of trainable parameters = 1825540\n",
      " #015  0%|          | 0/98 [00:00<?, ?it/s]\n",
      " You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " --------------------------------------------------------------------------\n",
      " MPI_ABORT was invoked on rank 7 in communicator MPI COMMUNICATOR 4 DUP FROM 0\n",
      " with errorcode 1.\n",
      " NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.\n",
      " You may or may not see output from other processes, depending on\n",
      " exactly when Open MPI kills them.\n",
      " #015  1%|          | 1/98 [00:01<02:44,  1.69s/it]\n",
      " [algo-1:00117] 6 more processes have sent help message help-mpi-api.txt / mpi-abort\n",
      " [algo-1:00117] Set MCA parameter \"orte_base_help_aggregate\" to 0 to see all help / error messages\"\u001b[0m\n",
      "\u001b[34mCommand \"mpirun --host algo-1 -np 8 --allow-run-as-root --tag-output --oversubscribe -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -mca plm_rsh_num_concurrent 1 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x SMDATAPARALLEL_USE_SINGLENODE=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1 -x LD_PRELOAD=/opt/conda/lib/python3.9/site-packages/gethostname.cpython-39-x86_64-linux-gnu.so -verbose -mca distributed-backend nccl -mca ddp-backend no_c10d -x USE_SMDDP_COLLECTIVES=0 smddprun /opt/conda/bin/python3.9 -m mpi4py train-LORA.py --epochs 1 --eval_batch_size 256 --model_name distilbert-base-uncased --train_batch_size 32\"\u001b[0m\n",
      "\u001b[34m2023-07-25 15:46:51,612 sagemaker-training-toolkit ERROR    Encountered exit_code 1\u001b[0m\n",
      "\n",
      "2023-07-25 15:47:10 Uploading - Uploading generated training model\n",
      "2023-07-25 15:47:10 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job huggingface-pytorch-training-2023-07-25-15-39-26-321: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by\n making sure all `forward` function outputs participate in calculating loss.\n If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).\n Parameters which did not receive grad for rank 7: base_model.model.classifier.original_module.bias, base_model.model.classifier.original_module.weight, base_model.model.pre_classifier.original_module.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-6ab5175f39f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# starting the train job with our uploaded datasets as input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mhuggingface_estimator_DDP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtraining_input_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_input_path\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# TRAIN EXEC TIME 55\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/workflow/pipeline_context.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_StepArguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretrieve_caller_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_instance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrun_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1290\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   2452\u001b[0m         \u001b[0;31m# If logs are requested, call logs_for_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2453\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2454\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2455\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2456\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   4812\u001b[0m             \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnexpectedStatusException\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mwaiting\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mjob\u001b[0m \u001b[0mfails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4813\u001b[0m         \"\"\"\n\u001b[0;32m-> 4814\u001b[0;31m         \u001b[0m_logs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboto_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4816\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlogs_for_processing_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoll\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_logs_for_job\u001b[0;34m(boto_session, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   6681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6682\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6683\u001b[0;31m         \u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TrainingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6684\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6685\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   6737\u001b[0m             \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6738\u001b[0m             \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6739\u001b[0;31m             \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6740\u001b[0m         )\n\u001b[1;32m   6741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job huggingface-pytorch-training-2023-07-25-15-39-26-321: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by\n making sure all `forward` function outputs participate in calculating loss.\n If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).\n Parameters which did not receive grad for rank 7: base_model.model.classifier.original_module.bias, base_model.model.classifier.original_module.weight, base_model.model.pre_classifier.original_module."
     ]
    }
   ],
   "source": [
    "#####################################################################\n",
    "############################# MAIN TEST #############################\n",
    "#####################################################################\n",
    "\n",
    "#####################################################################\n",
    "##################### DATA DISTRIBUTED WITH LORA ####################\n",
    "#####################################################################\n",
    "\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={'epochs': 1,\n",
    "                 'train_batch_size': 32,\n",
    "                 'model_name':'distilbert-base-uncased',\n",
    "                 \"eval_batch_size\": 256 # InExample: 64\n",
    "                 }\n",
    "\n",
    "training_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/train'\n",
    "test_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/test'\n",
    "\n",
    "# configuration for running training on smdistributed data parallel\n",
    "distribution = {'smdistributed':{'dataparallel':{'enabled': True,\n",
    "                                                \"custom_mpi_options\": \"-verbose -mca distributed-backend nccl -mca ddp-backend no_c10d\"}}}\n",
    "#                'mpi': {\"enabled\" : True,            # Required\n",
    "#                        \"processes_per_host\" : 8,    # Required\n",
    "#                        \"custom_mpi_options\" : \"-distributed-backend 'nccl' --ddp-backend 'no_c10d'\"}}\n",
    "\n",
    "huggingface_estimator_DDP = HuggingFace(entry_point='train-LORA.py',\n",
    "                            source_dir='./',\n",
    "                            instance_type='ml.p3.16xlarge',\n",
    "                            instance_count=1,\n",
    "                            role=role,\n",
    "                            transformers_version='4.26.0', # InExample: '4.26' with Error about the name\n",
    "                            pytorch_version='1.13.1', # InExample: '1.13' with Error about the version\n",
    "                            py_version='py39',\n",
    "                            hyperparameters = hyperparameters,\n",
    "                            distribution = distribution)\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator_DDP.fit({'train': training_input_path, 'test': test_input_path})\n",
    "\n",
    "# TRAIN EXEC TIME 55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "############################## TBC... ###############################\n",
    "#####################################################################\n",
    "\n",
    "#####################################################################\n",
    "################# MODEL PARALLEL (LoRA NOT TESTED) ##################\n",
    "#####################################################################\n",
    "\n",
    "# configuration for running training on smdistributed model parallel\n",
    "mpi_options = {\"enabled\" : True,\n",
    "               \"processes_per_host\" : 8}\n",
    "\n",
    "#\"\"\" # InExemple: -- but I had to change instance type and it results in GPU MEM ERROR\n",
    "\n",
    "smp_options = {\"enabled\":True,\n",
    "               \"parameters\": {\"microbatches\": 2, # InExample: 4 but changed to address the memory error\n",
    "                              \"placement_strategy\": \"spread\",\n",
    "                              \"pipeline\": \"interleaved\",\n",
    "                              \"optimize\": \"speed\",\n",
    "                              \"partitions\": 4, # InExample: 4 but changed to address the memory error\n",
    "                              \"ddp\": True,}}\n",
    "#\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# BABSED ON SM DMP explorations\n",
    "# BUT ERROR: TypeError: SMTrainingCompilerConfigurationError() takes no keyword arguments\n",
    "# and that is nowhere used in the code?!!??!!?\n",
    "# can be another memory error but this needs investigation\n",
    "\n",
    "smp_options = {\"enabled\": True,\n",
    "               \"parameters\": {\"partitions\": 1, # Got an Error '\"partitions\" is a required parameter'!!!; controls pipeline_parallel_degree\n",
    "                               \"ddp\": True, # PyTorch Specifc -- allows mix data+model parallel\n",
    "                               \"sharded_data_parallel_degree\": 8,  # InExample: 32\n",
    "                               \"delayed_parameter_initialization\": True,\n",
    "                               \"microbatches\": 4, # DEFAULT: 1; TEST 4\n",
    "                               \"horovod\": False, # DEFAULT: False; TEST True   ---- ValueError: 'ddp' and 'horovod' cannot be simultaneously enabled.\n",
    "                               \"ddp\": True, # DEFAULT: False; TEST True   ---- ValueError: 'ddp' and 'horovod' cannot be simultaneously enabled.\n",
    "                               \"tensor_parallel_degree\": 1, # DEFAULT: 1; TEST 2, ???\n",
    "                               \"fp16\": False, # DEFAULT: False; TEST True\n",
    "                               \"fp16_params\": False, # DEFAULT: False; TEST True --- DEPReciated\n",
    "                               \"bf16\": True, # DEFAULT: False; TEST True\n",
    "                               \"offload_activations\": True, # DEFAULT: False; TEST True\n",
    "                               \"shard_optimizer_state\": False, # DEFAULT: False; TEST True\n",
    "                               \"activation_loading_horizon\": 1, # DEFAULT: 4; TEST 1, ???  \n",
    "                               \"placement_strategy\": \"cluster\" # DEFAULT: \"cluster\" (\"DPT\"); TEST \"spread\" (\"TPD\"), permutations of \"D,P,T\"\n",
    "                              },}\n",
    "\"\"\"\n",
    "\n",
    "distribution={\"smdistributed\": {\"modelparallel\": smp_options},\n",
    "              \"mpi\": mpi_options}\n",
    "\n",
    " # create the Estimator\n",
    "huggingface_estimator_DMP = HuggingFace(entry_point='train-LORA.py',\n",
    "                                    source_dir='./',\n",
    "                                    instance_type='ml.p3.16xlarge',\n",
    "                                    instance_count=1,\n",
    "                                    role=role,\n",
    "                                    transformers_version='4.26.0',\n",
    "                                    pytorch_version='1.13.1',\n",
    "                                    py_version='py39',\n",
    "                                    hyperparameters = hyperparameters,\n",
    "                                    distribution = distribution)\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator_DMP.fit({'train': training_input_path, 'test': test_input_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "c281c456f1b8161c8906f4af2c08ed2c40c50136979eaae69688b01f70e9f4a9"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
